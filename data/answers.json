{"answers":[{"Id":"299744","ParentId":"299723","CreationDate":"2008-11-18T18:51:23.953","OwnerUserId":"35296","Tags":[],"Body":"&lt;p&gt;No.  CouchDB uses an &quot;optimistic concurrency&quot; model.  In the simplest terms, this just means that you send a document version along with your update, and CouchDB rejects the change if the current document version doesn't match what you've sent.&lt;\/p&gt;\n\n&lt;p&gt;It's deceptively simple, really.  You can reframe many normal transaction based scenarios for CouchDB. You do need to sort of throw out your RDBMS domain knowledge when learning CouchDB, though.  It's helpful to approach problems from a higher level, rather than attempting to mold Couch to a SQL based world.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Keeping track of inventory&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;p&gt;The problem you outlined is primarily an inventory issue.  If you have a document describing an item, and it includes a field for &quot;quantity available&quot;, you can handle concurrency issues like this:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Retrieve the document, take note of the &lt;code&gt;_rev&lt;\/code&gt; property that CouchDB sends along&lt;\/li&gt;\n&lt;li&gt;Decrement the quantity field, if it's greater than zero&lt;\/li&gt;\n&lt;li&gt;Send the updated document back, using the &lt;code&gt;_rev&lt;\/code&gt; property&lt;\/li&gt;\n&lt;li&gt;If the &lt;code&gt;_rev&lt;\/code&gt; matches the currently stored number, be done!&lt;\/li&gt;\n&lt;li&gt;If there's a conflict (when &lt;code&gt;_rev&lt;\/code&gt; doesn't match), retrieve the newest document version&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;In this instance, there are two possible failure scenarios to think about.  If the most recent document version has a quantity of 0, you handle it just like you would in a RDBMS and alert the user that they can't actually buy what they wanted to purchase.  If the most recent document version has a quantity greater than 0, you simply repeat the operation with the updated data, and start back at the beginning.  This forces you to do a bit more work than an RDBMS would, and could get a little annoying if there are frequent, conflicting updates.&lt;\/p&gt;\n\n&lt;p&gt;Now, the answer I just gave presupposes that you're going to do things in CouchDB in much the same way that you would in an RDBMS.  I might approach this problem a bit differently:&lt;\/p&gt;\n\n&lt;p&gt;I'd start with a &quot;master product&quot; document that includes all the descriptor data (name, picture, description, price, etc).  Then I'd add an &quot;inventory ticket&quot; document for each specific instance, with fields for &lt;code&gt;product_key&lt;\/code&gt; and &lt;code&gt;claimed_by&lt;\/code&gt;.  If you're selling a model of hammer, and have 20 of them to sell, you might have documents with keys like &lt;code&gt;hammer-1&lt;\/code&gt;, &lt;code&gt;hammer-2&lt;\/code&gt;, etc, to represent each available hammer.&lt;\/p&gt;\n\n&lt;p&gt;Then, I'd create a view that gives me a list of available hammers, with a reduce function that lets me see a &quot;total&quot;.  These are completely off the cuff, but should give you an idea of what a working view would look like.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Map&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function(doc) \n{ \n    if (doc.type == 'inventory_ticket' &amp;#38;amp;&amp;#38;amp; doc.claimed_by == null ) { \n     emit(doc.product_key, { 'inventory_ticket' :doc.id, '_rev' : doc._rev }); \n    } \n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;This gives me a list of available &quot;tickets&quot;, by product key.  I could grab a group of these when someone wants to buy a hammer, then iterate through sending updates (using the &lt;code&gt;id&lt;\/code&gt; and &lt;code&gt;_rev&lt;\/code&gt;) until I successfully claim one (previously claimed tickets will result in an update error).&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Reduce&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function (keys, values, combine) {\n    return values.length;\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;This reduce function simply returns the total number of unclaimed &lt;code&gt;inventory_ticket&lt;\/code&gt; items, so you can tell how many &quot;hammers&quot; are available for purchase.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Caveats&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;p&gt;This solution represents roughly 3.5 minutes of total thinking for the particular problem you've presented.  There may be better ways of doing this!  That said, it does substantially reduce conflicting updates, and cuts down on the need to respond to a conflict with a new update.  Under this model, you won't have multiple users attempting to change data in primary product entry.  At the very worst, you'll have multiple users attempting to claim a single ticket, and if you've grabbed several of those from your view, you simply move on to the next ticket and try again.&lt;\/p&gt;\n"},{"Id":"316875","ParentId":"299723","CreationDate":"2008-11-25T10:02:12.037","OwnerUserId":"39392","Tags":[],"Body":"&lt;p&gt;Expanding on MrKurt's answer. For lots of scenarios you don't need to have stock tickets redeemed in order.  Instead of selecting the first ticket, you can select randomly from the remaining tickets. Given a large number tickets and a large number of concurrent requests, you will get much reduced contention on those tickets, versus everyone trying to get the first ticket.&lt;\/p&gt;\n"},{"Id":"337398","ParentId":"337344","CreationDate":"2008-12-03T14:57:27.837","OwnerUserId":"42439","Tags":[],"Body":"&lt;p&gt;Document based DBs are best suiting for storing, well, documents. Lotus Notes is a common implementation and Notes email is an example. For what you are describing, eCommerce, CRUD, etc., realtional DBs are better designed for storage and retrieval of data items\/elements that are indexed (as opposed to documents).&lt;\/p&gt;\n"},{"Id":"337851","ParentId":"337344","CreationDate":"2008-12-03T16:49:00.950","OwnerUserId":"39392","Tags":[],"Body":"&lt;p&gt;You need to think of how you approach the application in an Document oriented way.  If you simply try and replicate how you would model the problem in an RDBMS you will fail.  There are also different trade-offs that you might want to make.  Remember as well that CouchDB's design is assuming that you will have an active active cluster of many nodes that could fail at any time.  How is your app going to handle one of the database nodes dissapearing from under it?&lt;\/p&gt;\n\n&lt;p&gt;One way to think about it is to imagine you didn't have any computers, just paper documents.  How would you create an efficient business process using bits of paper being passed around?  How can you avoid bottlenecks?  What if something goes wrong?&lt;\/p&gt;\n\n&lt;p&gt;Another angle you should think about is eventual consistency, where you will get into a consistent state eventually, but you may be inconsistent for some period of time.  This is an anathema in RMDBS land, but is extremely common in the real world.  The canonical transaction example is of transferring money from bank accounts.  How does this actually happen in the real world?  Through a single atomic transactions or through different banks issuing credit and debit notices to each other?  What happens when you write a cheque?&lt;\/p&gt;\n\n&lt;p&gt;So lets look at your examples:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Crud of entities with some fields with unique index on it.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;If I understand this correctly in CouchDB terms, you want to have a collection of documents where some that some named value is guaranteed to be unique across all those documents?  That case isn't generally supportable because documents may be created on different replicas.&lt;\/p&gt;\n\n&lt;p&gt;So we need to look at the real world problem and see if we can model that.  Do you really need them to be unique?  Can your application handle multiple docs with the same value?  Do you need to assign a unique identifier?  Can you do that deterministically?  A common scenario where this is required is where you need a unique sequential identifier.  This is tough to solve in a replicated environment.  In fact if the unique id is need to be strictly sequential with respect to time created it's impossible if you need the id straight away.  You need to relax at least one of those constraints.&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ecommerce web app like ebay&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;I'm not sure what to add here as the last comment you made on that post was to say &quot;very useful! thanks&quot;.  Was there something missing from the approach outlined there that is still causing you a problem?  I thought MrKurt's answer was pretty full and I added a little enhancement that would reduce contention.&lt;\/p&gt;\n"},{"Id":"337861","ParentId":"337344","CreationDate":"2008-12-03T16:52:27.543","OwnerUserId":"13930","Tags":[],"Body":"&lt;p&gt;Is there a need to normalize the data?&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Yes: Use relational.&lt;\/li&gt;\n&lt;li&gt;No: Use document.&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"568675","ParentId":"299723","CreationDate":"2009-02-20T08:09:54.137","OwnerUserId":"6277","Tags":[],"Body":"&lt;p&gt;Actually, you can in a way.  Have a look at the &lt;a href=&quot;http:\/\/wiki.apache.org\/couchdb\/HTTP%5FDocument%5FAPI&quot; rel=&quot;nofollow&quot;&gt;HTTP Document API&lt;\/a&gt; and scroll down to the heading &quot;Modify Multiple Documents With a Single Request&quot;.&lt;\/p&gt;\n\n&lt;p&gt;Basically you can create\/update\/delete a bunch of documents in a single post request to *URI \/{dbname}\/_bulk_docs* and they will either all succeed or all fail.  The document does caution that this behaviour may change in the future, though.&lt;\/p&gt;\n\n&lt;p&gt;EDIT: As predicted, from version 0.9 the bulk docs no longer works this way.&lt;\/p&gt;\n"},{"Id":"667161","ParentId":"667141","CreationDate":"2009-03-20T17:31:11.260","OwnerUserId":"11208","Tags":[],"Body":"&lt;p&gt;I suggest you visit the &lt;a href=&quot;http:\/\/highscalability.com\/&quot; rel=&quot;nofollow&quot;&gt;High Scalability blog&lt;\/a&gt;, which discusses this topic almost on a daily basis and has many articles about projects that chose distributed hashes, etc. over RDMBS.&lt;\/p&gt;\n\n&lt;p&gt;The quick (but very incomplete answer) is that not all data translates well to tables in efficient ways. For example, if your data is essentially one big dictionary, there are probably much faster alternatives that plain old RDBMS. Having said that, it mostly a matter of performance, and if performance isn't a huge concern in a project, and stability, consistency and reliability, for example, are, then I don't see much point in delving into these technologies when RDBMS is a much more mature and well developed scheme, with support in all languages and platforms and a huge set of solutions to choose from.&lt;\/p&gt;\n"},{"Id":"667162","ParentId":"667141","CreationDate":"2009-03-20T17:31:15.647","OwnerUserId":"69307","Tags":[],"Body":"&lt;p&gt;Fifteen years ago I was working on a credit risk system (basically a big tree walking system). We were using Sybase on HPUX &amp;#38;amp; solaris and performnce was killing us. We hired in consultants direct from Sybase who said it couldn't be done. Then we switched to an OO database (Object store in this case) and got a about a 100x performance increase (and the code was about 100x easier to write too)&lt;\/p&gt;\n\n&lt;p&gt;But such situations are quite rare - a relational database is a good first choice.&lt;\/p&gt;\n"},{"Id":"667355","ParentId":"667141","CreationDate":"2009-03-20T18:21:21.657","OwnerUserId":"20860","Tags":[],"Body":"&lt;p&gt;The relational database paradigm makes some assumptions about usage of data.&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A relation consists of an unordered set of rows.&lt;\/li&gt;\n&lt;li&gt;All rows in a relation have the same set of columns.&lt;\/li&gt;\n&lt;li&gt;Each column has a fixed name and data type and semantic meaning on all rows.&lt;\/li&gt;\n&lt;li&gt;Rows in a relation are identified by unique values in primary key column(s).&lt;\/li&gt;\n&lt;li&gt;etc.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;These assumptions support simplicity and structure, at the cost of some flexibility.  Not all data management tasks fit into this kind of structure.  Entities with complex attributes or variable attributes do not, for instance.  If you need flexibility in areas where a relational database solution doesn't support it, you need to use a different kind of solution.&lt;\/p&gt;\n\n&lt;p&gt;There are other solutions for managing data with different requirements.  Semantic Web technology, for example, allows each entity to define its own attributes and to be self-describing, by treating metadata as attributes just like data.  This is more flexible than the structure imposed by a relational database, but that flexibility comes with a cost of its own.&lt;\/p&gt;\n\n&lt;p&gt;Overall, you should use the right tool for each job.&lt;\/p&gt;\n\n&lt;p&gt;See also my other answer to &quot;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/282783\/the-next-gen-databases\/282813#282813&quot;&gt;The Next-gen databases&lt;\/a&gt;.&quot;&lt;\/p&gt;\n"},{"Id":"667673","ParentId":"667141","CreationDate":"2009-03-20T19:41:21.667","OwnerUserId":"726","Tags":[],"Body":"&lt;p&gt;In my experience, you shouldn't use a relational database when any one of these criteria are true:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;your data is structured as a hierarchy or a graph (network),&lt;\/li&gt;\n&lt;li&gt;the typical access pattern emphasizes reading over writing, or&lt;\/li&gt;\n&lt;li&gt;there\u2019s no requirement for ad-hoc queries.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Hierarchies and graphs do not translate well to relational tables. Even with the assistance of proprietary extensions like Oracle's &lt;code&gt;CONNECT BY&lt;\/code&gt;, chasing down trees is a mighty pain using SQL.&lt;\/p&gt;\n\n&lt;p&gt;Relational databases add a lot of overhead for simple read access. Transactional and referential integrity are powerful, but overkill for some applications. So for read-mostly applications, a file metaphor is good enough.&lt;\/p&gt;\n\n&lt;p&gt;Finally, you simply don\u2019t need a relational database with its full-blown query language if there are no unexpected queries anticipated. If there are no suits asking questions like &quot;how many 5%-discounted blue widgets did we sell in on the east coast grouped by salesperson?&quot;, and there never will be, then you, sir, can live free of DB.&lt;\/p&gt;\n"},{"Id":"667704","ParentId":"667141","CreationDate":"2009-03-20T19:50:48.290","OwnerUserId":"68869","Tags":[],"Body":"&lt;p&gt;\nAbout 7-8 years ago I worked on a web site that grew in popularity beyond our initial expectations and it got us in trouble performance-wise. Since we were all relatively inexperienced in web based projects it posed a significant strain on us about what to do beyond usual database separation onto separate server, load balancing etc.\n&lt;\/p&gt;\n\n&lt;p&gt;One day I've thought of something pretty simple. Since site was based on users, their profiles were stored in a database table the usual way someone would do it - user id, lots of info variables and stuff like that - which would show up as a users profile page which other users could look up. I've flushed all that data into a simple html file, already prepared as a users profile page and got a significant boost - basically a cache. I even made a system that when user edited their profile info, it would parse original html file, put it up for edit, and then flush out html back to the file system - got even more boost.\n&lt;p&gt;\nI made something simillar with messages users sent to each other. Basically wherever I could make a system bypass a database altogether, avoiding a INSERT or UPDATE, I got a significant boost. It may sound like a common sense, but it was an enlightening moment. It is not an avoidance of relational setup per se, but it is an avoidance of the database altogether - KISS. \n&lt;p&gt;&lt;\/p&gt;\n"},{"Id":"667878","ParentId":"667141","CreationDate":"2009-03-20T20:37:07.587","OwnerUserId":"27978","Tags":[],"Body":"&lt;p&gt;When you schema varies a lot you will have a hard time with relational databases. This is where XML databases or key-value pair databases work best. or you could use IBM DB2 and have both relational data and XML data managed by a single database engine. Get it free - check &lt;a href=&quot;http:\/\/FreeDB2.com&quot; rel=&quot;nofollow&quot;&gt;http:\/\/FreeDB2.com&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"668500","ParentId":"667141","CreationDate":"2009-03-21T00:57:56.070","OwnerUserId":"49716","Tags":[],"Body":"&lt;p&gt;A very good reason not to use a relational DBMS is the case when you can't properly design a relational schema, that means you are unaware of Relational Model:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Relational algebra&lt;\/li&gt;\n&lt;li&gt;Functional dependencies theory &lt;\/li&gt;\n&lt;li&gt;Normalization theory and normal forms&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"823509","ParentId":"299723","CreationDate":"2009-05-05T05:47:50.337","OwnerUserId":"","Tags":[],"Body":"&lt;p&gt;How do you do the classic &quot;bank account&quot; example of a database transaction?  I.e. you want to atomically withdraw $100 from Alice's account and deposit it into Bob's.  There are millions of accounts so you can't really expect Alice's and Bob's accounts to be the same document.&lt;\/p&gt;\n"},{"Id":"1118594","ParentId":"337344","CreationDate":"2009-07-13T09:59:34.833","OwnerUserId":"","Tags":[],"Body":"&lt;p&gt;when is there a need for normalized data?&lt;\/p&gt;\n"},{"Id":"1145740","ParentId":"1145726","CreationDate":"2009-07-17T21:18:02.017","OwnerUserId":"3043","Tags":[],"Body":"&lt;p&gt;It's like Jacuzzi: both a brand and a generic name.  It's not just a specific technology, but rather a specific &lt;em&gt;type&lt;\/em&gt; of technology, in this case referring to large-scale (often sparse) &quot;databases&quot; like Google's BigTable or CouchDB.&lt;\/p&gt;\n"},{"Id":"1145751","ParentId":"1145726","CreationDate":"2009-07-17T21:21:16.177","OwnerUserId":"122228","Tags":[],"Body":"&lt;p&gt;I assume you are refering to &lt;a href=&quot;http:\/\/blog.oskarsson.nu\/2009\/06\/nosql-debrief.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt; meaning of NoSQL.&lt;\/p&gt;\n\n&lt;p&gt;If I recall correctly, it refers to types of databases that don't neccessarily follow the relational form. Document databases come to mind, databases without a specific structure, and which don't use SQL as a specific query language.&lt;\/p&gt;\n\n&lt;p&gt;If I am recalling correctly, its generally better in web applications that rely on performance of the database, and don't need more advanced features of Relation Database Engines. For example, a Key-&gt;Value store providing a simple query by id interface might be 10-100x faster then the corresponding SQL server implementation, with a lower developer maintenance cost. &lt;\/p&gt;\n\n&lt;p&gt;One example is this &lt;a href=&quot;http:\/\/www.vldb.org\/conf\/2007\/papers\/industrial\/p1150-stonebraker.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;\/a&gt; for an &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Online%5Ftransaction%5Fprocessing&quot; rel=&quot;nofollow&quot;&gt;OLTP&lt;\/a&gt; Tuple Store, which sacrificed transactions for single threaded processing (no concurrency problem because no concurrency allowed), and kept all data in memory; achieving 10-100x better performance as compared to a similar RDBMS driven system. Basically, it's moving away from the 'One Size Fits All' view of SQL and database systems. &lt;\/p&gt;\n"},{"Id":"1145871","ParentId":"1145726","CreationDate":"2009-07-17T21:57:18.193","OwnerUserId":"47773","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/www.strozzi.it\/cgi-bin\/CSA\/tw7\/I\/en%5FUS\/nosql\/Home%20Page&quot; rel=&quot;nofollow&quot;&gt;NoSQL&lt;\/a&gt; the actual program appears to be a relational database implemented in awk using flat files on the backend.  Though they profess, &quot;NoSQL essentially has no arbitrary limits, and can work where other products can't. For example there is no limit on data field size, the number of columns, or file size&quot; , I don't think it is the large scale database of the future.  &lt;\/p&gt;\n\n&lt;p&gt;As Joel says, massively scalable databases like &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/BigTable&quot; rel=&quot;nofollow&quot;&gt;BigTable&lt;\/a&gt; or &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Hbase&quot; rel=&quot;nofollow&quot;&gt;HBase&lt;\/a&gt;, are much more interesting.  GQL is the query language associated with BigTable and App Engine.  It's largely SQL tweaked to avoid features Google considers bottle-necks (like joins).  However, I haven't heard this referred to as &quot;NoSQL&quot; before.&lt;\/p&gt;\n"},{"Id":"1164587","ParentId":"1145726","CreationDate":"2009-07-22T11:17:58.127","OwnerUserId":"142745","Tags":[],"Body":"&lt;p&gt;As the author of NoSQL I'm pleased to learn that Matthew gave me the due credit by posting a link to NoSQL's Wiki site. I'm really disappointed by learning that mr. Johan Oskarsson, who's behind that newborn &quot;nosql movement&quot;, has carefully avoided to mention the original implementation of NoSQL in his posts, events and web pages, in spite of the fact the if one googles for nosql the thruth pops up right away. By doing so, mr. Oskarsson also denies to give due credit to the many who have contributed with creating and evolving NoSQL over the last 15 years or so, and who are dutifully listed on NoSQL's home page. As if that was not enough, some others, who obviusly have never heard of Google, have gone to the point to closely mimic the NoSQL logo, as you may see in &lt;a href=&quot;http:\/\/www.cwi.it\/notizia\/18526\/2009-07-21\/Cresce-il-movimento-anti-database.html&quot; rel=&quot;nofollow&quot;&gt;this Italian translation&lt;\/a&gt; of a recent Computerworld article about said &quot;nosql movement&quot;. Those folks too, of course, did not give any credits either. And I do not blame you for saying about NoSQL &quot;I don't think it is the large scale database of the future.&quot;, because it never indended to be. Thank-you Matthew, I mean it.&lt;\/p&gt;\n"},{"Id":"1164798","ParentId":"1145726","CreationDate":"2009-07-22T12:01:27.667","OwnerUserId":"16883","Tags":[],"Body":"&lt;blockquote&gt;\n  &lt;ol&gt;\n  &lt;li&gt;What exactly is it?&lt;\/li&gt;\n  &lt;\/ol&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;On one hand, a &lt;a href=&quot;http:\/\/www.strozzi.it\/cgi-bin\/CSA\/tw7\/I\/en%5FUS\/nosql\/Home%20Page&quot; rel=&quot;nofollow&quot;&gt;specific system&lt;\/a&gt;, but it has also become a generic word for a &lt;a href=&quot;http:\/\/www.eflorenzano.com\/blog\/post\/my-thoughts-nosql\/&quot; rel=&quot;nofollow&quot;&gt;variety of new data storage backends&lt;\/a&gt; that do not follow the relational DB model.&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;ol&gt;\n  &lt;li&gt;How does it work?&lt;\/li&gt;\n  &lt;\/ol&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;Each of the systems labelled with the generic name works differently, but the basic idea is to offer better scalability and performance by using DB models that don't support all the functionality of a generic RDBMS, but still enough functionality to be useful. In a way it's like MySQL, which at one time lacked support for transactions but, exactly &lt;em&gt;because&lt;\/em&gt; of that, managed to outperform other DB systems. If you could write your app in a way that didn't require transactions, it was great.&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;ol&gt;\n  &lt;li&gt;Why would it be better than using a SQL Database? And how much better is it?&lt;\/li&gt;\n  &lt;\/ol&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;It would be better when your site needs to scale so massively that the best RDBMS running on the best hardware you can afford and optimized as much as possible simply can't keep up with the load. How much better it is depends on the specific use case (lots of update activity combined with lots of joins is very hard on &quot;traditional&quot; RDBMSs) - could well be a factor of 1000 in extreme cases.&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;ol&gt;\n  &lt;li&gt;Is the technology too new to start implementing yet or is it worth taking a look into?&lt;\/li&gt;\n  &lt;\/ol&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;Depends mainly on what you're trying to achieve. It's certainly mature enough to use. But few applications really need to scale that massively. For most, a traditional RDBMS is sufficient. However, with internet usage becoming more ubiquitous all the time, it's quite likely that applications that do will become more common (though probably not dominant).&lt;\/p&gt;\n"},{"Id":"1165148","ParentId":"1145726","CreationDate":"2009-07-22T13:08:55.227","OwnerUserId":"142745","Tags":[],"Body":"&lt;p&gt;Since someone said that my previous post was off-topic, I'll try to compensate :-) NoSQL is not, and never was, intended to be a replacement for more mainstream SQL databases, but a couple of words are in order to get things in the right perspective. At the very hart of the &lt;a href=&quot;http:\/\/www.strozzi.it\/cgi-bin\/CSA\/tw7\/I\/en%5FUS\/NoSQL\/Philosophy%20of%20NoSQL&quot; rel=&quot;nofollow&quot;&gt;NoSQL philosophy&lt;\/a&gt; lies the consideration that, possibly for commercial and portability reasons, SQL engines tend to disregard the tremendous power of the UNIX operating system and its derivatives. With a filesystem-based database, you can take immediate advantage of the ever increasing capabilities and power of the underlying operating system, which have been steadily increasing for many years now in accordance with Moore's law. With this approach, many operating-system commands become automatically also &quot;database operators&quot; (think of  &quot;ls&quot; &quot;sort&quot;, &quot;find&quot; and the other countless UNIX shell utilities). With this in mind, and a bit of creativity, you can indeed devise a filesystem-based database that is able to overcome the limitations of many common SQL engines, at least for specific usage patterns, and this is the whole point behind NoSQL's philosophy the way I see it. I run hundreds of web sites and they all use NoSQL to a greater or lesser extent. In fact, they do not host huge amounts of data, but even if some of them did I could probably think of a creative use of NoSQL and the filesystem, to overcome any bottlenecks. Something that would likely be more difficult with traditional SQL &quot;jails&quot;. I urge you to google for &quot;unix&quot;, &quot;manis&quot; and &quot;shaffer&quot; to understand what I mean.&lt;\/p&gt;\n"},{"Id":"1182873","ParentId":"1165185","CreationDate":"2009-07-25T19:40:22.670","OwnerUserId":"54990","Tags":[],"Body":"&lt;p&gt;It all depends on your scaling requirments. RBDMS require locks to work and so can only really be scaled &quot;up&quot;. NoSQL-style DBs such as Googles bigtable and CouchDB are massively scalable and very cheap, but can get very complicated to write an app on top of as developers have to deal with all kinds of data consistency\/fault tolerance issues in thier application layer. &lt;\/p&gt;\n\n&lt;p&gt;I would say for a small application you're probably better off with a SQL-based relational database. Whilst in theory much more expensive, being realistic at a small scale that price trades off as a much simpler system to work with.&lt;\/p&gt;\n\n&lt;p&gt;If however you're start up is a muti-tenant solution which needs to deal with a lot of writes, I'd look carefully at alternatives.&lt;\/p&gt;\n"},{"Id":"1189998","ParentId":"1189911","CreationDate":"2009-07-27T19:05:09.473","OwnerUserId":"121191","Tags":[],"Body":"&lt;p&gt;I'm answering this with CouchDB in the back of my mind, but I would presume most would be true for other DBs also. We looked at using CouchDB, but finally decided against it since our data access is not known beforehand and scalability is not the issue.&lt;\/p&gt;\n\n&lt;p&gt;Harder:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Takes rethinking on conceptual level so it's 'harder' since it is just different. Since you have to know your data access patterns in advance, no automatic translation can be applied. You would need to add the access pattern at least.&lt;\/li&gt;\n&lt;li&gt;Consistency is not handled by the database but must be dealt with in the application. Less guarantees means easier migration, fail-over and better scalability at the cost of a more complicated application. An application has to deal with conflicts and inconsistencies.&lt;\/li&gt;\n&lt;li&gt;Links which cross documents (or key\/value) have to be dealt with on application level also.&lt;\/li&gt;\n&lt;li&gt;SQL type of databases have IDEs which are much more mature. You get a lot of support libraries (although the layering of those libraries make things much more complex than needed for SQL).&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Easier:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Faster if you know your data access patterns.&lt;\/li&gt;\n&lt;li&gt;Migration \/ Fail-over is easier for the database since no promises are made to you as an application programmer. Although you get eventual consistency. Probably. Finally. Some time.&lt;\/li&gt;\n&lt;li&gt;One key \/ value is much easier to understand than one row from a table. All the (tree) relations are already in, and complete objects can be recognized.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;The modeling should be about the same but you have to be careful about what you put in one document: UML can also be used for both OO modeling as well as DB modeling, which are two different beasts already.&lt;\/p&gt;\n\n&lt;p&gt;I would have liked to see a good open OO database nicely integrated with C# \/ Silverlight. Just to make the choice even more difficult. :)&lt;\/p&gt;\n"},{"Id":"1190022","ParentId":"1189911","CreationDate":"2009-07-27T19:11:08.167","OwnerUserId":"14149","Tags":[],"Body":"&lt;p&gt;Flat files have long been considered arcane and impractical for a data set of any size. However, faster computers with more memory make it possible to load a file into memory and sort it in real time, at least for reasonably small n and local, single-user applications.&lt;\/p&gt;\n\n&lt;p&gt;For example, you can usually read a file of 10,000 records AND sort it on a field in less than half a second, an acceptable response time.&lt;\/p&gt;\n\n&lt;p&gt;Of course, there are reasons to use a database instead of a flat file -- relational operations, data integrity, multiuser capability, remote acccess, larger capacity, standardization, etc., but increased computer speed and memory capacity have made in-memory manipulation of data more practical in some cases.&lt;\/p&gt;\n"},{"Id":"1192822","ParentId":"1189911","CreationDate":"2009-07-28T08:57:49.517","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;I think you have to consider that the non-relational DBMS differ a lot regarding their data model and therefore the conceptual data design will also differ a lot. In the thread &lt;a href=&quot;http:\/\/groups.google.com\/group\/nosql-discussion\/browse%5Fthread\/thread\/bbe3aa69071fd7b9\/b5bb363c32f598c9&quot; rel=&quot;nofollow&quot;&gt;Data Design in Non-Relational Databases&lt;\/a&gt; of the &lt;a href=&quot;http:\/\/groups.google.com\/group\/nosql-discussion\/&quot; rel=&quot;nofollow&quot;&gt;NOSQL Google group&lt;\/a&gt; the different paradigms are categorized like this:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Bigtable-like systems (HBase,\nHypertable, etc)&lt;\/li&gt;\n&lt;li&gt;Key-value stores (Tokyo, Voldemort,\netc)&lt;\/li&gt;\n&lt;li&gt;Document databases (CouchDB,\nMongoDB, etc)&lt;\/li&gt;\n&lt;li&gt;Graph databases (AllegroGraph,\nNeo4j, Sesame, etc)&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;I'm mostly into &lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1000162\/have-anyone-used-graph-based-databases-http-neo4j-org&quot;&gt;graph databases&lt;\/a&gt;, and the elegance of data design using this paradigm was what brought me there, tired of the shortcomings of &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Relational%5Fdatabase%5Fmanagement%5Fsystem&quot; rel=&quot;nofollow&quot;&gt;RDBMS&lt;\/a&gt;. I have put a few examples of data design using a graph database on this &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/Domain%5FModeling%5FGallery&quot; rel=&quot;nofollow&quot;&gt;wiki page&lt;\/a&gt; and there's an &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/IMDB%5FThe%5FDomain&quot; rel=&quot;nofollow&quot;&gt;example of how to model&lt;\/a&gt; the basic &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Internet%5FMovie%5FDatabase&quot; rel=&quot;nofollow&quot;&gt;IMDB&lt;\/a&gt; movie\/actor\/role data too.&lt;\/p&gt;\n\n&lt;p&gt;The presentation slides (pdf) &lt;a href=&quot;http:\/\/markorodriguez.com\/Lectures%5Ffiles\/risk-symposium2009.pdf&quot; rel=&quot;nofollow&quot;&gt;Graph Databases and the Future of Large-Scale Knowledge Management&lt;\/a&gt; by &lt;a href=&quot;http:\/\/markorodriguez.com\/&quot; rel=&quot;nofollow&quot;&gt;Marko Rodriguez&lt;\/a&gt; contains a very nice introduction to data design using a graph database as well.&lt;\/p&gt;\n\n&lt;p&gt;&lt;em&gt;Answering the specific questions from a graphdb point of view:&lt;\/em&gt;&lt;\/p&gt;\n\n&lt;p&gt;Alternate design: adding relationships between many different kinds of entities without any worries or a need to predefine which entities can get connected.&lt;\/p&gt;\n\n&lt;p&gt;Bridging the gap: I tend to do this different for every case, based on the domain itself, as I don't want a &quot;table-oriented graph&quot; and the like. However, &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/SQL%5FImporter&quot; rel=&quot;nofollow&quot;&gt;here's&lt;\/a&gt; some information on automatic translation from RDBMS to graphdb.&lt;\/p&gt;\n\n&lt;p&gt;Explicit data models: I do these all the time (whiteboard style), and then use the model as it is in the DB as well.&lt;\/p&gt;\n\n&lt;p&gt;Miss from RDBMS world: easy ways to create reports.&lt;\/p&gt;\n"},{"Id":"1203360","ParentId":"1203010","CreationDate":"2009-07-29T22:12:01.220","OwnerUserId":"2958","Tags":[],"Body":"&lt;p&gt;I think &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; is beginning to look like the front runner performance wise for schemaless data stores.  &lt;\/p&gt;\n\n&lt;p&gt;We're currently in the processes of evaluating this for storing binary objects that can range from 10Kb to 50Mb and I've been very impressed with it's performance even on modest hardware.&lt;\/p&gt;\n"},{"Id":"1203457","ParentId":"1203010","CreationDate":"2009-07-29T22:32:19.830","OwnerUserId":"30506","Tags":[],"Body":"&lt;p&gt;If it is primarily read performance you are worried about why not just put a &lt;a href=&quot;http:\/\/varnish.projects.linpro.no\/&quot; rel=&quot;nofollow&quot;&gt;varnish&lt;\/a&gt; proxy in front of couchdb? I use a couple of custom configurations in varnish to tell it not to actually query couchdb for cached objects despite couchdb specifying must-validate, then have a script with an active HTTP GET on _changes that uses the data from _changes in order to explicitly purge changed entries from varnish.&lt;\/p&gt;\n\n&lt;p&gt;As a plus varnish lets you do URL rewriting, which I need. Most of the other solutions for it involve running something like apache or ngnix just to rewrite URLs for couchdb.&lt;\/p&gt;\n"},{"Id":"1215297","ParentId":"299723","CreationDate":"2009-07-31T23:01:59.983","OwnerUserId":"9069","Tags":[],"Body":"&lt;p&gt;A design pattern for restfull transactions is to create a &quot;tension&quot; in the system. For the popular example use case of a bank account transaction you must ensure to update the total for both involved accounts:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a transaction document &quot;transfer USD 10 from account 11223 to account 88733&quot;. This creates the tension in the system.&lt;\/li&gt;\n&lt;li&gt;To resolve any tension scan for all transaction documents and\n&lt;ul&gt;\n&lt;li&gt;If the source account is not updated yet update the source account (-10 USD)&lt;\/li&gt;\n&lt;li&gt;If the source account was updated but the transaction document does not show this then update the transaction document (e.g. set flag &quot;sourcedone&quot; in the document)&lt;\/li&gt;\n&lt;li&gt;If the target account is not updated yet update the target account (+10 USD)&lt;\/li&gt;\n&lt;li&gt;If the target  account was updated but the transaction document does not show this then update the transaction document &lt;\/li&gt;\n&lt;li&gt;If both accouts have been updated you can delete the transaction document or keep it for auditing.&lt;\/li&gt;\n&lt;\/ul&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;The scanning for tension should be done in a backend process for all &quot;tension documents&quot; to keep the times of tension in the system short. In the above example there will be a short time anticipated inconsistence when the first account has been updated but the second is not updated yet. This must be taken into account the same way you'll deal with eventual consistency if your Couchdb is distributed.&lt;\/p&gt;\n\n&lt;p&gt;Another possible implementation avoids the need for transactions completely: just store the tension documents and evaluate the state of your system by evaluating every involved tension document. In the example above this would mean that the total for a account is only determined as the sum values in the transaction documents where this account is involved. In Couchdb you can modles this very nicely as a map\/reduce view.&lt;\/p&gt;\n"},{"Id":"1245361","ParentId":"1245338","CreationDate":"2009-08-07T15:17:44.580","OwnerUserId":"6568","Tags":[],"Body":"&lt;p&gt;Put simply, it means not using a relational database for data storage.&lt;\/p&gt;\n\n&lt;p&gt;Here's a relevant article:  &lt;a href=&quot;http:\/\/www.computerworld.com\/s\/article\/9135086\/No_to_SQL_Anti_database_movement_gains_steam_&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.computerworld.com\/s\/article\/9135086\/No_to_SQL_Anti_database_movement_gains_steam_&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1245362","ParentId":"1245338","CreationDate":"2009-08-07T15:17:48.947","OwnerUserId":"2469","Tags":[],"Body":"&lt;p&gt;From the &lt;a href=&quot;http:\/\/www.strozzi.it\/cgi-bin\/CSA\/tw7\/I\/en%5FUS\/nosql\/Home%20Page&quot; rel=&quot;nofollow&quot;&gt;NoSQL Homepage&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;NoSQL is a fast, portable, relational database management system without arbitrary limits, (other than memory and processor speed) that runs under, and interacts with, the UNIX  1 Operating System. It uses the &quot;Operator-Stream Paradigm&quot; described in &quot;Unix Review&quot;, March, 1991, page 24, entitled &quot;A 4GL Language&quot;. There are a number of &quot;operators&quot; that each perform a unique function on the data. The &quot;stream&quot; is supplied by the UNIX Input\/Output redirection mechanism. Therefore each operator processes some data and then passes it along to the next operator via the UNIX pipe function. This is very efficient as UNIX pipes are implemented in memory. NoSQL is compliant with the &quot;Relational Model&quot;.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;I would also see this answer on &lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1145726\/what-is-nosql-how-does-it-work-and-what-benefits-does-it-profide\/1145751#1145751&quot;&gt;Stackoverflow&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"1245379","ParentId":"1245338","CreationDate":"2009-08-07T15:19:58.493","OwnerUserId":"25538","Tags":[],"Body":"&lt;p&gt;If you've ever worked with a database, you've probably worked with a &lt;em&gt;relational&lt;\/em&gt; database.  Examples would be an Access database, SQL Server, or MySQL.  When you think about tables in these kinds of databases, you generally think of a grid, like in Excel.  You have to name each column of your database table, and you have to specify whether all the values in that column are integers, strings, etc.  Finally, when you want to look up information in that table, you have to use a language called SQL.&lt;\/p&gt;\n\n&lt;p&gt;A new trend is forming around non-relational databases, that is, databases that do not fall into a neat grid.  You don't have to specify which things are integers and strings and booleans, etc.  These types of databases are more flexible, but they don't use SQL, because they are not structured that way.&lt;\/p&gt;\n\n&lt;p&gt;Put simply, that is why they are &quot;NoSQL&quot; databases.&lt;\/p&gt;\n\n&lt;p&gt;The advantage of using a NoSQL database is that you don't have to know exactly what your data will look like ahead of time.  Perhaps you have a Contacts table, but you don't know what kind of information you'll want to store about each contact.  In a relational database, you need to make columns like &quot;Name&quot; and &quot;Address&quot;.  If you find out later on that you need a phone number, you have to add a column for that.  There's no need for this kind of planning\/structuring in a NoSQL database.  There are also potential scaling advantages, but that is a bit controversial, so I won't make any claims there.&lt;\/p&gt;\n\n&lt;p&gt;Disadvantages of NoSQL databases is really the lack of SQL.  SQL is simple and ubiquitous.  SQL allows you to slice and dice your data easier to get aggregate results, whereas it's a bit more complicated in NoSQL databases (you'll probably use things like MapReduce, for which there is a bit of a learning curve). &lt;\/p&gt;\n"},{"Id":"1245392","ParentId":"1245338","CreationDate":"2009-08-07T15:22:43.900","OwnerUserId":"33052","Tags":[],"Body":"&lt;p&gt;NoSql is the new database philosophy which talks about all the shortcomings of the relational database design, particularly the problems they have in scaling up for today's demanding web environments.&lt;\/p&gt;\n\n&lt;p&gt;NoSql is quickly evolving into a movement with new tools, software and formats coming up as alternative to SQL.&lt;\/p&gt;\n\n&lt;p&gt;RDBMS is as ubiquitous as OOP and while both of these design methodologies solve some problems wonderfully, they don't solve all.&lt;\/p&gt;\n\n&lt;p&gt;So think of NoSql as the functional programmin of the database world.&lt;\/p&gt;\n\n&lt;p&gt;Was this simple enough?&lt;\/p&gt;\n"},{"Id":"1248165","ParentId":"1245338","CreationDate":"2009-08-08T06:43:49.370","OwnerUserId":"49478","Tags":[],"Body":"&lt;p&gt;NoSQL is the idea that SQL-type databases don't satisfy the demands\/requirements of a heavily-used database that requires transactions be reliable and failsafe (or close to it). This ties into the ideas of ACID and CAP, both things worth looking into but not something to lose sleep over unless you run a really popular site that is transaction-heavy (ie Amazon or Ebay). To get a great start on these subjects, I suggest:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.eflorenzano.com\/blog\/post\/my-thoughts-nosql\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.eflorenzano.com\/blog\/post\/my-thoughts-nosql\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;and&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.julianbrowne.com\/article\/viewer\/brewers-cap-theorem&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.julianbrowne.com\/article\/viewer\/brewers-cap-theorem&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1342883","ParentId":"1342741","CreationDate":"2009-08-27T18:21:30.437","OwnerUserId":"143972","Tags":[],"Body":"&lt;p&gt;I am also planning on moving away from SQL. I have been looking at &lt;a href=&quot;http:\/\/couchdb.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;\/a&gt;, which looks promising. Looking at your requirements, I think all can be done with CouchDB views, and the list api.&lt;\/p&gt;\n"},{"Id":"1343331","ParentId":"1342741","CreationDate":"2009-08-27T19:31:45.503","OwnerUserId":"115305","Tags":[],"Body":"&lt;p&gt;For a project I once needed a simple database that was fast at doing lookups and which would do lots of lookups and just an occasional write. I just ended up writing my own file format.&lt;\/p&gt;\n\n&lt;p&gt;While you could do this too, it is pretty complex, especially if you need to support it from a web server. With a web server, you would at least need to protect every write to the file and make sure it can be read from multiple threads. The design of this file format is something you should work out as good as possible with plenty of testing and experiments. One minor bug could prove fatal for a web project in this style, but if you get it working, it can work real well and extremely fast.&lt;\/p&gt;\n\n&lt;p&gt;But for 99.999% of all situations, you don't want such a custom solution. It's easier to just upgrade the hardware, move to Oracle, SQL Server or InterBase, use a dedicated database server, use faster hard disks, install more memory, upgrade to a 64-bit system. Those are the more generic tricks to improve performance with the least effort.&lt;\/p&gt;\n"},{"Id":"1343565","ParentId":"1342741","CreationDate":"2009-08-27T20:25:08.657","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;I'd really, really, suggest stay with MySQL (or a RDBMS) until you fully understand the situation.&lt;\/p&gt;\n\n&lt;p&gt;I have no idea how much performance or much data you plan on using, but 30M rows is not very many. &lt;\/p&gt;\n\n&lt;p&gt;If you need to optimise certain range scans, you can do this with (for example) InnoDB by choosing a (implicitly clustered) primary key judiciously, and\/or denormalising where necessary.&lt;\/p&gt;\n\n&lt;p&gt;But like most things, make it work first, then fix performance problems you detect in your performance test lab on production-grade hardware.&lt;\/p&gt;\n\n&lt;p&gt;&lt;hr \/&gt;&lt;\/p&gt;\n\n&lt;p&gt;EDIT:Some other points:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;key\/value database such as Cassandra, Voldermort etc, do not generally support secondary indexes&lt;\/li&gt;\n&lt;li&gt;Therefore, you cannot do a CREATE INDEX&lt;\/li&gt;\n&lt;li&gt;Most of them also don't do range scans (even on the main index) because they're using hashing to implement partitioning (which they mostly do).&lt;\/li&gt;\n&lt;li&gt;Therefore they also don't do range expiry (DELETE FROM tbl WHERE ts &amp;#38;lt; NOW() - INTERVAL 30 DAYS)&lt;\/li&gt;\n&lt;li&gt;Your application must do ALL of this itself or manage without it; secondary indexes are really the killer&lt;\/li&gt;\n&lt;li&gt;ALTER TABLE ... ADD INDEX takes quite a long time in e.g. MySQL with a large table, but at least you don't have to write much code to do it. In a &quot;nosql&quot; database, it will also take a long time BUT also you have to write heaps and heaps of code to maintain the new secondary index, expire it correctly, AND modify your queries to use it.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;In short... you can't use a key\/value database as a shortcut to avoid ALTER TABLE.&lt;\/p&gt;\n"},{"Id":"1345085","ParentId":"1245338","CreationDate":"2009-08-28T04:55:33.297","OwnerUserId":"15962","Tags":[],"Body":"&lt;p&gt;Something everyone considering a &quot;nosql&quot; approach should consider:&lt;\/p&gt;\n\n&lt;p&gt;(I shan't risk putting the image into this post as it contains a curse word, and I don't want offensive flags. So clicker beware -- there's an f-word in there.  Only click if you have a sense of humor.)&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/browsertoolkit.com\/fault-tolerance.png&quot; rel=&quot;nofollow&quot;&gt;http:\/\/browsertoolkit.com\/fault-tolerance.png&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1387734","ParentId":"1342741","CreationDate":"2009-09-07T05:53:08.153","OwnerUserId":"20860","Tags":[],"Body":"&lt;p&gt;I'd recommend learning about &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Message%5Fqueue&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;message queue&lt;\/strong&gt;&lt;\/a&gt; technology.  There are several open-source options available, and also robust commercial products that would serve up the volume you describe as a tiny snack.&lt;\/p&gt;\n"},{"Id":"1387872","ParentId":"1342741","CreationDate":"2009-09-07T06:43:18.427","OwnerUserId":"38207","Tags":[],"Body":"&lt;p&gt;It seems to me that what you want to do -- Query a large set of data in several different ways and order the results -- is exactly and precisely what RDBMeS were designed for.&lt;\/p&gt;\n\n&lt;p&gt;I doubt you would find any other datastore that would do this as well as a modern commercial DBMS (Oracle, SQLServer, DB2 etc.) or any opn source tool that would accomplish\nthis any better than MySql.&lt;\/p&gt;\n\n&lt;p&gt;You could have a look at Googles BigTable, which is really a relational database but\nit can present an 'object'y personality to your program. Its exceptionaly good for free format text \nsearches, and complex predicates. As the whole thing (at least the version you can download) is implemented in Python I doubt it would beat MySql in a query marathon.&lt;\/p&gt;\n"},{"Id":"1408093","ParentId":"1245338","CreationDate":"2009-09-10T22:39:12.707","OwnerUserId":"20126","Tags":[],"Body":"&lt;p&gt;Found this &lt;a href=&quot;http:\/\/www.25hoursaday.com\/weblog\/2009\/09\/10\/BuildingScalableDatabasesDenormalizationTheNoSQLMovementAndDigg.aspx&quot; rel=&quot;nofollow&quot;&gt;nice article about no-sql&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;and this as well: \n&lt;a href=&quot;http:\/\/www.elasticsearch.com\/\/blog\/2010\/02\/25\/nosql_yessearch.html&quot; rel=&quot;nofollow&quot;&gt;NoSQL, Yes Search&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1429406","ParentId":"1342741","CreationDate":"2009-09-15T20:21:08.323","OwnerUserId":"133221","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/couchdb.apache.org&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;\/a&gt; is schema-free, and it's fairly simple to retrieve a huge amount of data quickly, because you are working only with indexes.  You are not &quot;querying&quot; the database each time, you are retrieving only matching keys (which are pre-sorted making it even faster).  &lt;\/p&gt;\n\n&lt;p&gt;&quot;Views&quot; are re-indexed everytime new data is entered into the database, but this takes place transparently to the user, so while there might be potential delay in generating an updated view, there will virtually never be any delay in retrieving results.  &lt;\/p&gt;\n\n&lt;p&gt;I've just started to explore building an &quot;activity stream&quot; solution using CouchDB, and because the paradigm is different, my thinking about the process had to change from the SQL thinking.  &lt;\/p&gt;\n\n&lt;p&gt;Rather than figure out how to query the data I want and then process it on the page, I instead generate a view that keys all documents by date, so I can easily create multiple groups of data, just by using the appropriate date key, essentially running several queries simultaneously, but with no degradation in performance.&lt;\/p&gt;\n\n&lt;p&gt;This is ideal for activity streams, and I can isolate everything by date, or along with date isolation I can further filter results of a particular subtype, etc - by creating a view as needed, and because the view itself is just using javascript and all data in CouchDB is JSON, virtually everything can be done client-side to render your page.&lt;\/p&gt;\n"},{"Id":"1457656","ParentId":"1436076","CreationDate":"2009-09-22T01:01:52.763","OwnerUserId":"41887","Tags":[],"Body":"&lt;p&gt;This type of querying is not available through the 0.20.0 API. I'm not sure if there are any plans for it (I doubt it would appear anytime soon). You'll find some roadmap details on the HBase website that &lt;em&gt;might&lt;\/em&gt; answer that question.&lt;\/p&gt;\n\n&lt;p&gt;You'll need to compute the answer in your own application (although I'd love to be proved wrong).&lt;\/p&gt;\n"},{"Id":"1506162","ParentId":"1436076","CreationDate":"2009-10-01T20:08:08.833","OwnerUserId":"27587","Tags":[],"Body":"&lt;p&gt;The query as described is better suited to a relational database.  You can answer the query quickly, however, by precomputing the result.  For example, you might have a table where the key is the number of classes in common, and the cells are individual students that have key-many classes in common.&lt;\/p&gt;\n\n&lt;p&gt;You could use a variant on this to answer questions like &quot;which students are in class X and class Y&quot;: use the classes as pieces of the key (in alphabetical ordering, or something at least consistent), and again, each column is a student.&lt;\/p&gt;\n"},{"Id":"1512978","ParentId":"1502735","CreationDate":"2009-10-03T06:26:31.557","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;For me, the main thing is a decision whether to use the OrderedPartitioner or RandomPartitioner.&lt;\/p&gt;\n\n&lt;p&gt;If you use the RandomPartitioner, range scans are not possible. This means that you must know the exact key for any activity, INCLUDING CLEANING UP OLD DATA.&lt;\/p&gt;\n\n&lt;p&gt;So if you've got a lot of churn, unless you have some magic way of knowing exactly which keys you've inserted stuff for, using the random partitioner you can easily &quot;lose&quot; stuff, which causes a disc space leak and will eventually consume all storage.&lt;\/p&gt;\n\n&lt;p&gt;On the other hand, you can ask the ordered partitioner &quot;what keys do I have in Column Family X between A and B&quot; ? - and it'll tell you. You can then clean them up.&lt;\/p&gt;\n\n&lt;p&gt;However, there is a downside as well. As Cassandra doesn't do automatic load balancing, if you use the ordered partitioner, in all likelihood all your data will end up in just one or two nodes and none in the others, which means you'll waste resources.&lt;\/p&gt;\n\n&lt;p&gt;I don't have any easy answer for this, except you can get &quot;best of both worlds&quot; in some cases by putting a short hash value (of something you can enumerate easily from other data sources) on the beginning of your keys - for example a 16-bit hex hash of the user ID - which will give you 4 hex digits, followed by whatever the key is you really wanted to use.&lt;\/p&gt;\n\n&lt;p&gt;Then if you had a list of recently-deleted users, you can just hash their IDs and range scan to clean up anything related to them.&lt;\/p&gt;\n\n&lt;p&gt;The next tricky bit is secondary indexes - Cassandra doesn't have any - so if you need to look up X by Y, you need to insert the data under both keys, or have a pointer. Likewise, these pointers may need to be cleaned up when the thing they point to doesn't exist, but there's no easy way of querying stuff on this basis, so your app needs to Just Remember.&lt;\/p&gt;\n\n&lt;p&gt;And application bugs may leave orphaned keys that you've forgotten about, and you'll have no way of easily detecting them, unless you write some garbage collector which periodically scans every single key in the db (this is going to take a while - but you can do it in chunks) to check for ones which aren't needed any more.&lt;\/p&gt;\n\n&lt;p&gt;None of this is based on real usage, just what I've figured out during research. We don't use Cassandra in production.&lt;\/p&gt;\n"},{"Id":"1515470","ParentId":"1502735","CreationDate":"2009-10-04T02:47:41.340","OwnerUserId":"178678","Tags":[],"Body":"&lt;p&gt;Another tutorial is: here &lt;a href=&quot;http:\/\/blog.evanweaver.com\/articles\/2009\/07\/06\/up-and-running-with-cassandra\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blog.evanweaver.com\/articles\/2009\/07\/06\/up-and-running-with-cassandra\/&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"1528860","ParentId":"1528827","CreationDate":"2009-10-07T00:20:09.730","OwnerUserId":"99967","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/code.google.com\/p\/app-engine-site-creator\/&quot; rel=&quot;nofollow&quot;&gt;App Engine Site Creator&lt;\/a&gt; &quot;is designed to be a highly extensible and light weight content management system. It features a user-friendly content editing interface, a high degree of flexibility and customization, a file sharing mechanism, full support for page hierarchies, and fine-grained mechanisms for user management and access controls. It is built to run on Google App Engine and to scale well with minimal engineering maintenance.&quot;&lt;\/p&gt;\n\n&lt;p&gt;I haven't used it, but I think it at least claims to be what you want.&lt;\/p&gt;\n"},{"Id":"1529134","ParentId":"1528827","CreationDate":"2009-10-07T02:01:17.873","OwnerUserId":"125525","Tags":[],"Body":"&lt;p&gt;What you will find is that whilst the database itself is the reason for a slow performing site, you need to think about the site itself as a whole.&lt;\/p&gt;\n\n&lt;p&gt;CMS systems use a database to store the content of pages simply so that they are easily editable. In high traffic scenarios, there is absolutely no appreciable change in content from one user to the next.\nAs such, most CMS systems also provide caching mechanisms to overcome the load required to interact with a database. A typical flowchart of this is in action is:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1. Is the page already cached in memory\/disk?\n2. If already cached, goto step 5.\n3. If not, access the database and format the page.\n4. Store the page to memory\/disk.\n5. retrieve that page from memory\/disk.\n6. serve the page.\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Obviously, things get a little tricky if you want to show custom login details on the page. However, by using a judicial balance of reducing database load, caching all\/some parts of the page, the effect of being slashdotted\/digged can be reduced significantly.&lt;\/p&gt;\n\n&lt;p&gt;Don't forget that you can also specify cache header (Cache-Control) information in your returned pages so that the same user returning to the page can reuse previously sent information. See this &lt;a href=&quot;http:\/\/www.web-caching.com\/mnot%5Ftutorial\/how.html&quot; rel=&quot;nofollow&quot;&gt;link&lt;\/a&gt; for some information.&lt;\/p&gt;\n\n&lt;p&gt;So, to answer your question. The best way to reduce database issues in high traffic scenarios, it's best not to use the database at all :)&lt;\/p&gt;\n"},{"Id":"1543990","ParentId":"1543965","CreationDate":"2009-10-09T14:05:31.860","OwnerUserId":"71883","Tags":[],"Body":"&lt;p&gt;The community based opensource.net driver for mongodb works quite nice and is really fast. It can be found on the mongodb homepage&lt;\/p&gt;\n"},{"Id":"1544012","ParentId":"1543965","CreationDate":"2009-10-09T14:08:18.433","OwnerUserId":"37020","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/msdn.microsoft.com\/en-us\/data\/cc655792.aspx&quot; rel=&quot;nofollow&quot;&gt;Microsoft &quot;Velocity&quot;&lt;\/a&gt; does this (also a link to &lt;a href=&quot;http:\/\/blogs.msdn.com\/velocity\/&quot; rel=&quot;nofollow&quot;&gt;their blog&lt;\/a&gt;), but is still in community technology preview (i.e. &quot;beta&quot;).&lt;\/p&gt;\n\n&lt;p&gt;Other projects, like memcached, have .NET APIs.&lt;\/p&gt;\n"},{"Id":"1544086","ParentId":"1543965","CreationDate":"2009-10-09T14:22:26.927","OwnerUserId":"97572","Tags":[],"Body":"&lt;p&gt;Community supported C# driver for &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Drivers&quot; rel=&quot;nofollow&quot;&gt;mongodb&lt;\/a&gt; avaiable in &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp&quot; rel=&quot;nofollow&quot;&gt;github&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;And another promising key value store is &lt;a href=&quot;http:\/\/www.hypertable.org\/&quot; rel=&quot;nofollow&quot;&gt;hyphertable&lt;\/a&gt;. You can access this through c# thrift client. You can get it &lt;a href=&quot;http:\/\/wiki.apache.org\/thrift\/ThriftUsageC%23&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;I dont have any working experience with thrift clients. So you can clarify it in hyphertable forum..&lt;\/p&gt;\n\n&lt;p&gt;Cheers&lt;\/p&gt;\n\n&lt;p&gt;Ramesh Vel&lt;\/p&gt;\n"},{"Id":"1545529","ParentId":"1543965","CreationDate":"2009-10-09T19:02:12.090","OwnerUserId":"71883","Tags":[],"Body":"&lt;p&gt;there is a memcachedb which is persistent, and also the tokyo cabinet which is persistent (and supposedly very fast). Both are compatible with the memcache protocol sot you can use their drivers for .net&lt;\/p&gt;\n"},{"Id":"1595672","ParentId":"1595562","CreationDate":"2009-10-20T15:56:52.920","OwnerUserId":"102937","Tags":[],"Body":"&lt;p&gt;Project Voldermort is part of the &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Nosql&quot; rel=&quot;nofollow&quot;&gt;NoSQL&lt;\/a&gt; movement.  Trends in computer architectures are pressing databases in a direction that requires horizontal scalability. NOSQL attempts to address this requirement.&lt;\/p&gt;\n\n&lt;p&gt;Among the claimed benefits of such Key\/Value stores is the ability to blow through enormous amounts of data without the overhead of a traditional RDBMS.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.computerworld.com\/s\/article\/9135086\/No%5Fto%5FSQL%5FAnti%5Fdatabase%5Fmovement%5Fgains%5Fsteam%5F&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.computerworld.com\/s\/article\/9135086\/No_to_SQL_Anti_database_movement_gains_steam_&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1604444","ParentId":"1604025","CreationDate":"2009-10-22T00:43:12.217","OwnerUserId":"111332","Tags":[],"Body":"&lt;p&gt;MongoDB doesn't use magic strings, but uses query documents to represent queries. There is also an &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp&quot; rel=&quot;nofollow&quot;&gt;open source C# driver&lt;\/a&gt; available. I'm not sure of the specifics of the C# driver, but it should be relatively easy to add a validation layer on top of it if one doesn't exist already. There are similar projects on top of the drivers in Python and Ruby, for example.&lt;\/p&gt;\n"},{"Id":"1604801","ParentId":"1604025","CreationDate":"2009-10-22T03:02:50.387","OwnerUserId":"179972","Tags":[],"Body":"&lt;p&gt;You can connect to text files using ADO.NET and read\/write them using SQL syntax and Commands issued through ADO.NET. &lt;a href=&quot;http:\/\/www.c-sharpcorner.com\/UploadFile\/mgold\/ConnectODBCText11262005070206AM\/ConnectODBCText.aspx&quot; rel=&quot;nofollow&quot;&gt;There's an example in this article&lt;\/a&gt;. Your data will be stored in human-readable format in comma- or tab-delimited record format. Of course it won't be fast with large data sets.\nI'm unsure if you're trying to get away from both SQL databases and SQL syntax. The text file solutions is queryable by SQL.&lt;\/p&gt;\n\n&lt;p&gt;You can also do the same with Excel spreadsheets by treating them like SQL data sources (even though they're not) through ADO.NET access.&lt;\/p&gt;\n"},{"Id":"1655701","ParentId":"1604025","CreationDate":"2009-10-31T21:40:14.833","OwnerUserId":"149270","Tags":[],"Body":"&lt;p&gt;Not sure if it is what you are looking for but you could try &lt;a href=&quot;http:\/\/www.db4o.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.db4o.com\/&lt;\/a&gt; I've never used it myself but it may help you&lt;\/p&gt;\n"},{"Id":"1684143","ParentId":"1502735","CreationDate":"2009-11-05T22:36:52.260","OwnerUserId":"204234","Tags":[],"Body":"&lt;blockquote&gt;\n  &lt;p&gt;Are there any deal breaks for you?\n  Not necessarily deal breakers but something to be aware of &lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;A client connects to a nearest node, which address it should know beforehand, all communications with all other Cassandra nodes proxied through it.\na. read\/write traffic is not evenly distributed among nodes - some nodes proxy more data than they host themselves\nb. Should the node go down, the client is helpless, can\u2019t read, can\u2019t write anywhere in the cluster.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Although Cassandra claims that \u201cwrites never fail\u201d they do fail, at least at the moment of speaking they do. Should the target data node become sluggish, request times out and write fails. There are many reason for a node to become unresponsive: garbage collector kicks in, compaction process, whatever\u2026\nIn all such cases all write\/read request fail. In a conventional database these requests would have become proportionally slow, but in Cassandra they just fail.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;There is multi-get but there is no multi-delete and one can\u2019t truncate ColumnFamily either&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Should a new, empty data node enter the cluster, portion of data from one neighbor nodes on the key-ring will be transfered only. This leads to uneven data distribution and uneven load. You can fix it by always doubling number of nodes.One should also keep track on tokens manually and select them wisely.&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"1690853","ParentId":"1604025","CreationDate":"2009-11-06T22:16:13.557","OwnerUserId":"2351","Tags":[],"Body":"&lt;p&gt;I'm the principle author behind the .Net driver for Mongo.  There isn't currently a ORM like mapper for it yet.  Instead it works on simple documents that are the equivalent of a dictionary.  It wouldn't be hard to use reflection to iterate over the fields in a document and assign them to properties on an object.  I've written a simple thing like that for LDAP results in the past.  You don't have to worry about sql injection with Mongo as there really isn't a query language that gets parsed.  All drivers talk to Mongo in its native tongue.  There is some potential if you dynamically generate javascript and send it to the DB but the need for that should mostly be rare.  If you have any questions about using the driver feel free to post them to the Google Group or send a message through GitHub.&lt;\/p&gt;\n"},{"Id":"1708336","ParentId":"1700827","CreationDate":"2009-11-10T14:22:54.787","OwnerUserId":"111332","Tags":[],"Body":"&lt;p&gt;There's no great way to represent this query yet (as of 1.1.2) - if you ask on the list or file a feature request we can try to get something cooked up.&lt;\/p&gt;\n\n&lt;p&gt;For now the best bet is probably to use an $in query to do half of the work:&lt;\/p&gt;\n\n&lt;p&gt;&lt;code&gt;db.test.find({keys: {$in: Keys1}})&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;You can do this in combination with a $where which can do the Keys2 part (but won't take advantage of an index - that's why it is good to do as much as possible with the regular query syntax). This would look something like this:&lt;\/p&gt;\n\n&lt;p&gt;&lt;code&gt;db.test.find({keys: {$in: Keys1}, $where: &quot;for (i in this.keys) { for (j in Keys2) { if (this.keys[i] == Keys2[j]) return true;}} return false;&quot;})&lt;\/code&gt;&lt;\/p&gt;\n"},{"Id":"1737014","ParentId":"1736840","CreationDate":"2009-11-15T08:46:42.873","OwnerUserId":"146325","Tags":[],"Body":"&lt;p&gt;I presume you are meaning products such as Couch DB or Tokyo Cabinet (rather than ECM products like Documentum).  I think the attraction for many developers is familiarity.  &lt;\/p&gt;\n\n&lt;p&gt;Firstly, the conceptual model (in most cases) is key-value pairs, like a configuration file.  As most frameworks seem to require a lot of configuration-wrangling, front-end\/middle tier developers are comfortable with that way of working working.  Secondly, these tools offer interfaces in developer-friendly languages like Java, Python, etc.&lt;\/p&gt;\n\n&lt;p&gt;Whereas, traditional RDBMS products require thinking in a different fashion - relationally.  And they require learning not just a weird language, SQL, but a new way of programming: set-based rather than procedural.  If you rehearse the arguments for putting business logic in the middle tier rather stored procedures in the database, well a lot of them apply to No SQL as well.   &lt;\/p&gt;\n"},{"Id":"1737029","ParentId":"1736840","CreationDate":"2009-11-15T08:55:37.587","OwnerUserId":"13447","Tags":[],"Body":"&lt;p&gt;I enjoyed listening to the &lt;a href=&quot;http:\/\/twit.tv\/floss36&quot; rel=&quot;nofollow&quot;&gt;floss weekly episode&lt;\/a&gt; about CouchDB. Lots of reasoning and ideas there. &lt;\/p&gt;\n\n&lt;p&gt;Prior to listening, most of the stuff I read about this topic triggered not much insight (for me). Listening to people talking and reasoning about why&amp;#38;amp;where you want to use document-oriented DBs helped me a lot to really get the concepts, reasoning, pros and cons. Now all the articles and statements (IMHO) suddenly make a lot more sense. &lt;\/p&gt;\n\n&lt;p&gt;Your mileage may vary, but this helped me a lot.&lt;\/p&gt;\n"},{"Id":"1744506","ParentId":"1740882","CreationDate":"2009-11-16T20:00:43.123","OwnerUserId":"188094","Tags":[],"Body":"&lt;p&gt;Try using low-level datastore API, with the item ID as a key and &quot;friends&quot; IDs as property names (property value is true or false in this case). I think that lookup with a key filter  must be quite fast.&lt;\/p&gt;\n"},{"Id":"1777166","ParentId":"1777103","CreationDate":"2009-11-21T23:40:45.567","OwnerUserId":"62368","Tags":[],"Body":"&lt;p&gt;I guess you mean non-&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Relational%5Fdatabase%5Fmanagement%5Fsystem&quot; rel=&quot;nofollow&quot;&gt;RDBMS&lt;\/a&gt;? If so take a look at &lt;a href=&quot;http:\/\/www.db4o.com\/about\/productinformation\/&quot; rel=&quot;nofollow&quot;&gt;db4o&lt;\/a&gt;. It's object-based, easy, works with LINQ and it's open source.&lt;\/p&gt;\n"},{"Id":"1777181","ParentId":"1777103","CreationDate":"2009-11-21T23:45:17.167","OwnerUserId":"200872","Tags":[],"Body":"&lt;p&gt;You might want to check Ayende's Rhino.DHT:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/ayende\/rhino-dht&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ayende\/rhino-dht&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/ayende.com\/Blog\/category\/555.aspx&quot; rel=&quot;nofollow&quot;&gt;http:\/\/ayende.com\/Blog\/category\/555.aspx&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;or the rather new MS project - Velocity:\n&lt;a href=&quot;http:\/\/www.google.com\/search?rlz=1C1CHMB%5FplPL350PL350&amp;#38;amp;sourceid=chrome&amp;#38;amp;ie=UTF-8&amp;#38;amp;q=microsoft+velocity&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.google.com\/search?rlz=1C1CHMB_plPL350PL350&amp;#38;amp;sourceid=chrome&amp;#38;amp;ie=UTF-8&amp;#38;amp;q=microsoft+velocity&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1777263","ParentId":"1777103","CreationDate":"2009-11-22T00:13:45.800","OwnerUserId":"12968","Tags":[],"Body":"&lt;p&gt;You don't state what your requirements are (i.e. has to run on Windows), so I'll throw out the 2 that I've used successfully.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; is a document database that has prebuilt binaries for 32bit and 64bit Windows.  That's always a nice thing to see.&lt;\/p&gt;\n\n&lt;p&gt;Client access can be done with &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp\/tree\/master&quot; rel=&quot;nofollow&quot;&gt;this driver&lt;\/a&gt;.  It isn't an official client from the MongoDB team itself, but I've used it.  And in my usage, it has supported what I need.  There is some &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp\/tree\/master\/MongoDB.Linq\/&quot; rel=&quot;nofollow&quot;&gt;LINQ stuff in the repo&lt;\/a&gt;, but I haven't tried it.&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\/\/ from the wiki\nusing MongoDB.Driver; \nMongo db = new Mongo(); \ndb.Connect(); \/\/Connect to localhost on the default port. \nDocument query = new Document(); \nquery[&quot;field1&quot;] = 10; \nDocument result = db[&quot;tests&quot;][&quot;reads&quot;].FindOne(query); \ndb.Disconnect();\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;I was able to run both client and server on Windows with no problems.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/couchdb.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;\/a&gt; is an option as well.  There are some native .NET clients, but all of CouchDB is done with REST.  So HttpWebRequest\/Response will serve you well here.  A &lt;a href=&quot;http:\/\/abdullin.com\/journal\/2009\/7\/24\/couchdb-in-the-cloud-persisting-from-net-code.html&quot; rel=&quot;nofollow&quot;&gt;blog post by Rinat Abdullin&lt;\/a&gt; shows how some of the pieces fit together.  There is also &lt;a href=&quot;http:\/\/code.google.com\/p\/couchbrowse\/&quot; rel=&quot;nofollow&quot;&gt;CouchBrowse&lt;\/a&gt;.  I've never used a native client.  GET\/PUT\/POST have worked very well for me.&lt;\/p&gt;\n\n&lt;p&gt;I got CouchDB to work on Windows (it's written in Erlang), but my performance testing showed that Linux was faster.  My guess is maybe in how Erlang itself is implemented?  I dunno.  But it runs on both Windows and Linux.  And I was able to call the Linux instance from Windows easily (it's just REST).&lt;\/p&gt;\n\n&lt;p&gt;This next one I've never tried, but I've got a friend who is a committer on the &lt;a href=&quot;http:\/\/hadoop.apache.org\/hbase\/&quot; rel=&quot;nofollow&quot;&gt;HBase project&lt;\/a&gt;.  And he thinks that the &lt;a href=&quot;http:\/\/wiki.apache.org\/hadoop\/Hbase\/ThriftApi&quot; rel=&quot;nofollow&quot;&gt;Thrift interface to HBase&lt;\/a&gt; should be usable from .NET (since &lt;a href=&quot;http:\/\/incubator.apache.org\/thrift\/&quot; rel=&quot;nofollow&quot;&gt;Thrift&lt;\/a&gt; will generate C#).  The big thing here is the fact that Hadoop\/HBase are focused more on *nix environments.  But there is no reason you couldn't run HBase on a Linux cluster and connect to it from .NET on Windows for production.  For development, you can run HBase on Windows using Cygwin.  A good set of instructions on how to do this is &lt;a href=&quot;http:\/\/hadoop.apache.org\/hbase\/docs\/r0.20.2\/cygwin.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;There are others (Valdemort, Cassandra, etc.) but I have no real experience with them so I won't pretend to say how they integrate with C#\/.NET.  The big thing to look at is what their API looks like - if it has a Thrift interface, REST, etc. you should be able to connect to them with no problems.  You might not be able to run the NoSQL Service on Windows OS as efficiently as Linux, but maybe that isn't a big deal.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT&lt;\/strong&gt; Changed that there are some native CouchDB clients.  I'm not familiar with them as I always use raw HTTP and my own little wrapper classes.&lt;\/p&gt;\n"},{"Id":"1778782","ParentId":"1778763","CreationDate":"2009-11-22T14:00:20.190","OwnerUserId":"67585","Tags":[],"Body":"&lt;p&gt;The &lt;a href=&quot;http:\/\/martinfowler.com\/eaaCatalog\/repository.html&quot; rel=&quot;nofollow&quot;&gt;Repository-pattern&lt;\/a&gt; is a well-known and widespread pattern to map you DAL to you domain-layer.&lt;\/p&gt;\n"},{"Id":"1778885","ParentId":"1778763","CreationDate":"2009-11-22T14:44:40.440","OwnerUserId":"170034","Tags":[],"Body":"&lt;p&gt;Hello,&lt;\/p&gt;\n\n&lt;p&gt;In addition to the Repository pattern proposed by cwap, you should also look at the Data Mapper pattern. (&lt;a href=&quot;http:\/\/martinfowler.com\/eaaCatalog\/dataMapper.html&quot; rel=&quot;nofollow&quot;&gt;Data Mapper&lt;\/a&gt;). From my understanding, both work together. Repository relies on the Data Mapper when it comes to write or read the object to the database (or other persistance media). It is the data mapper that deals with the specific persistance technology. The Repository on the other hand can remain unchanged even if the repository changes.&lt;\/p&gt;\n"},{"Id":"1786655","ParentId":"1543965","CreationDate":"2009-11-23T22:58:11.177","OwnerUserId":"12748","Tags":[],"Body":"&lt;p&gt;CouchDB is well regarded and &lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1050152\/use-couchdb-with-net&quot;&gt;accessible via .Net&lt;\/a&gt; albeit not that pleasant to install on windows still.&lt;\/p&gt;\n\n&lt;p&gt;Thrift api compatible servers like &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/&quot; rel=&quot;nofollow&quot;&gt;cassandra&lt;\/a&gt; will talk &lt;a href=&quot;http:\/\/svn.apache.org\/viewvc\/incubator\/thrift\/trunk\/lib\/csharp\/&quot; rel=&quot;nofollow&quot;&gt;.Net&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/1978th.net\/tokyocabinet\/&quot; rel=&quot;nofollow&quot;&gt;Tokyo Cabinet&lt;\/a&gt; can be simply accessed by the (apparently) full &lt;a href=&quot;http:\/\/tokyotyrant.codeplex.com\/&quot; rel=&quot;nofollow&quot;&gt;.Net port of Tokyo Tyrant&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; has &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Drivers&quot; rel=&quot;nofollow&quot;&gt;several .Net api options&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I would suggest that indicating whether sharding (or other horizontal scaling capabilities) are more or less important that some level of consistency in your persistent store since all of the above trade off the consistency for low latency and good scalability in some way or another.&lt;\/p&gt;\n"},{"Id":"1799987","ParentId":"1799958","CreationDate":"2009-11-25T21:13:28.923","OwnerUserId":"51233","Tags":[],"Body":"&lt;p&gt;Looks like the riak source has several bash start scripts. You would have to convert those to a windows batch script equivalent. That could be a fairly interesting chore given how limited batch scripts are. Those start-*.sh files show how to start it up though so I'd start there.&lt;\/p&gt;\n\n&lt;p&gt;The &lt;a href=&quot;http:\/\/hg.basho.com\/riak\/src\/tip\/README&quot; rel=&quot;nofollow&quot;&gt;http:\/\/hg.basho.com\/riak\/src\/tip\/README&lt;\/a&gt; Readme file has futher info on what each script does.&lt;\/p&gt;\n"},{"Id":"1813745","ParentId":"1813612","CreationDate":"2009-11-28T20:47:07.223","OwnerUserId":"2140","Tags":[],"Body":"&lt;p&gt;If you go with CouchDB, you can use &lt;a href=&quot;http:\/\/github.com\/couchapp\/couchapp&quot; rel=&quot;nofollow&quot;&gt;CouchApp&lt;\/a&gt; which is a set of scripts for deploying an application directly to a CouchDB database. In essence, you skip the middleware and use CouchDB's views, lists, and show functions along with clientside JavaScript to implement the whole app. If your app works in this architecture, it's surprisingly refreshing, simple and cool. &lt;\/p&gt;\n"},{"Id":"1815932","ParentId":"1815731","CreationDate":"2009-11-29T16:13:44.370","OwnerUserId":"199201","Tags":[],"Body":"&lt;p&gt;Ok, you haven given a normalized data model as you would do in an SQL setup.&lt;\/p&gt;\n\n&lt;p&gt;In my understanding you don't do this in MongoDB. You could store references, but you do not for performance reasons in the general case.&lt;\/p&gt;\n\n&lt;p&gt;I'm not an expert in the NoSQL area in no way, but why don't you simply follow your needs and store the user (ids) that have voted for a story in the stories collection and the story (ids) a user has voted for in the users collection?&lt;\/p&gt;\n"},{"Id":"1815943","ParentId":"1815731","CreationDate":"2009-11-29T16:18:33.623","OwnerUserId":"111332","Tags":[],"Body":"&lt;p&gt;I would suggest storing votes as a list of story &lt;code&gt;_id&lt;\/code&gt;s in each user. That way you can find out what stories a user has voted for just by looking at the list. To get the users who have voted for a story you can do something like:&lt;\/p&gt;\n\n&lt;p&gt;&lt;code&gt;db.users.find({stories: story_id})&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;where &lt;code&gt;story_id&lt;\/code&gt; is the &lt;code&gt;_id&lt;\/code&gt; of the story in question. If you create an index on the &lt;code&gt;stories&lt;\/code&gt; field both of those queries will be fast.&lt;\/p&gt;\n"},{"Id":"1816191","ParentId":"1815731","CreationDate":"2009-11-29T17:50:12.903","OwnerUserId":"32797","Tags":[],"Body":"&lt;p&gt;In CouchDB this is very simple. One view emits:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function(doc) {\n if(doc.type == &quot;vote&quot;) {\n   emit(doc.story_id, doc.user_id);\n }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Another view emits:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function(doc) {\n if(doc.type == &quot;vote&quot;) {\n   emit(doc.user_id, doc.story_id);\n }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Both are queries extremely fast since there is no join. If you do need user data or story data, CouchDB supports multi-document fetch. Also quite fast and is one way to do a &quot;join&quot;.&lt;\/p&gt;\n"},{"Id":"1823472","ParentId":"1822444","CreationDate":"2009-12-01T01:21:49.077","OwnerUserId":"32797","Tags":[],"Body":"&lt;p&gt;Your approach is fine. Using CouchDB doesn't mean you'll just abandon relational modeling. You will need need to run two queries but that's because this is a &quot;join&quot;. SQL queries with joins are also slow but the SQL syntax lets you express the query in one statement.&lt;\/p&gt;\n\n&lt;p&gt;In my few months of experience with CouchDB this is what I've discovered:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;No schema, so designing the application models is fast and flexible&lt;\/li&gt;\n&lt;li&gt;CRUD is there, so developing your application is fast and flexible&lt;\/li&gt;\n&lt;li&gt;Goodbye SQL injection&lt;\/li&gt;\n&lt;li&gt;What would be a SQL join takes a little bit more work in CouchDB&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Depending on your needs I've found that couchdb-lucene is also useful for building more complex queries.&lt;\/p&gt;\n"},{"Id":"1823549","ParentId":"1823536","CreationDate":"2009-12-01T01:53:15.083","OwnerUserId":"79914","Tags":[],"Body":"&lt;p&gt;SQL-subsets like GQL obviously still concern themselves with it -- but pure non-SQL databases like CouchDB, Voldemort, etc should put &amp;#38;amp; get data without concern for SQL-injection-style attacks.&lt;\/p&gt;\n\n&lt;p&gt;That however does not excuse you from doing content validation, because while it might not break the database, it may break your application and allow things like XSS (if it is a web app).&lt;\/p&gt;\n"},{"Id":"1823567","ParentId":"1823536","CreationDate":"2009-12-01T01:58:30.593","OwnerUserId":"213421","Tags":[],"Body":"&lt;p&gt;Anytime data that is from or manipulated by user input is used to control the execution of code, there needs to be sanitization.  I've seen cases where code used user input to execute a command without sanitizing the input.  It hadn't been exploited, but if it had been it would have been a horrible attack vector.&lt;\/p&gt;\n"},{"Id":"1823568","ParentId":"1823536","CreationDate":"2009-12-01T01:58:38.083","OwnerUserId":"55164","Tags":[],"Body":"&lt;p&gt;SQl Injection is only a subset of a type of security flaw in which any uncontrolled input gets evaluated.&lt;\/p&gt;\n\n&lt;p&gt;techincally, you could &quot;inject&quot; javascript, among others.&lt;\/p&gt;\n"},{"Id":"1823603","ParentId":"1823536","CreationDate":"2009-12-01T02:11:43.767","OwnerUserId":"18936","Tags":[],"Body":"&lt;p&gt;\u201cInjection\u201d holes are to do with text context mismatches. Every time you put a text string into another context of string you need to do encoding to fit the changed context. It seems seductively simple to blindly stuff strings together, but the difficulty of string processing is deceptive.&lt;\/p&gt;\n\n&lt;p&gt;Databases with a purely object-based interface are immune to injection vulnerabilities, just like parameterised queries are in SQL. There is nothing an attacker can put in his string to break out of the string literal context in which you've put him.&lt;\/p&gt;\n\n&lt;p&gt;But GQL specifically is not one of these. It's a string query language, and if you go concatenating untrusted unescaped material into a query like &lt;code&gt;&quot;WHERE title='%s'&quot; % title&lt;\/code&gt;, you're just as vulnerable as you were with full-on SQL. Maybe the limited capabilities of GQL make it more difficult to exploit that to completely compromise the application, but certainly not impossible in general, and in the very best case your application is still wrong and will fall over when people try to legitimately use apostrophes.&lt;\/p&gt;\n\n&lt;p&gt;GQL has a parameter binding interface. Use it. Resist the allure of string hacking.&lt;\/p&gt;\n"},{"Id":"1823804","ParentId":"1822444","CreationDate":"2009-12-01T03:25:00.453","OwnerUserId":"53529","Tags":[],"Body":"&lt;p&gt;I cross-posted this question to the &lt;a href=&quot;http:\/\/couchdb.apache.org\/community\/lists.html&quot; rel=&quot;nofollow&quot;&gt;couchdb users mailing list&lt;\/a&gt; and Nathan Stott &lt;a href=&quot;http:\/\/markmail.org\/message\/vsvwyz4rccc33jox&quot; rel=&quot;nofollow&quot;&gt;pointed me to&lt;\/a&gt; a &lt;a href=&quot;http:\/\/www.cmlenz.net\/archives\/2007\/10\/couchdb-joins&quot; rel=&quot;nofollow&quot;&gt;very helpful blog post&lt;\/a&gt; by Christopher Lenz&lt;\/p&gt;\n"},{"Id":"1823934","ParentId":"1823924","CreationDate":"2009-12-01T04:05:37.437","OwnerUserId":"15124","Tags":[],"Body":"&lt;p&gt;A great article about NOSQL patterns is found here:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/horicky.blogspot.com\/2009\/11\/nosql-patterns.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/horicky.blogspot.com\/2009\/11\/nosql-patterns.html&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;covers&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;API model&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Machines layout&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Data partitioning (Consistent\nHashing)&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Data replication&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Membership Changes&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Client Consistency&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Master Slave (or Single Master) Model&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Multi-Master (or No Master) Model&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Quorum Based 2PC&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Vector Clock&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;State Transfer Model&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Operation Transfer Model&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Map Reduce Execution&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Handling Deletes&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Storage Implementation&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"1841979","ParentId":"1841883","CreationDate":"2009-12-03T18:40:03.713","OwnerUserId":"2114","Tags":[],"Body":"&lt;p&gt;The section about View Indexes in the &lt;a href=&quot;http:\/\/couchdb.apache.org\/docs\/overview.html&quot; rel=&quot;nofollow&quot;&gt;Technical Overview&lt;\/a&gt; is a great guide to this.&lt;\/p&gt;\n\n&lt;blockquote&gt;The view builder uses the database sequence ID to determine if the view group is fully up-to-date with the database. If not, the view engine examines the all database documents (in packed sequential order) changed since the last refresh. Documents are read in the order they occur in the disk file, reducing the frequency and cost of disk head seeks.&lt;\/blockquote&gt;\n\n&lt;blockquote&gt;As documents are examined, their previous row values are removed from the view indexes, if they exist. If the document is selected by a view function, the function results are inserted into the view as a new row.&lt;\/blockquote&gt;\n\n&lt;p&gt;CouchDB first checks to see if &lt;em&gt;anything&lt;\/em&gt; has changed in the entire database using a sequence id (that gets updated whenever there's a change to any document in the database). If something has changed it goes looking for those documents and runs the map function on them.&lt;\/p&gt;\n\n&lt;p&gt;There really shouldn't be any need to rebuild\/regenerate your views since it will incrementally refresh as you modify your documents (note that it won't update the view until you use it though). With hat said one way (and I'm sure there's a better way) would be to remove the design document describing the view and insert it again seeing as a design document is no different (almost) from a normal document.&lt;\/p&gt;\n"},{"Id":"1852372","ParentId":"1849204","CreationDate":"2009-12-05T14:44:35.613","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Anecdotally: yes, Twitter, Digg, Ooyala, SimpleGeo, Mahalo, and others are using or moving to Cassandra for a primary data store (&lt;a href=&quot;http:\/\/n2.nabble.com\/Cassandra-users-survey-td4040068.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/n2.nabble.com\/Cassandra-users-survey-td4040068.html&lt;\/a&gt;).&lt;\/p&gt;\n\n&lt;p&gt;Technically: yes; besides supporting replication (including to multiple datacenters), each Cassandra node has an fsync'd commit log to make sure writes are durable; from there writes are turned into SSTables which are immutable until compaction (which combines multiple SSTables to GC old versions).  Snapshotting is supported at any time, including automatic snapshot-before-compaction.&lt;\/p&gt;\n"},{"Id":"1855610","ParentId":"1853735","CreationDate":"2009-12-06T15:01:39.590","OwnerUserId":"164255","Tags":[],"Body":"&lt;p&gt;I think there is no perfect solution - depends on what operations are more important for your app. I believe Silicon Alley Insider stores comments nested with MongoDB for example. That does make the query you mention harder.&lt;\/p&gt;\n\n&lt;p&gt;One option is store at top-level in the post a list of all commenters in an array. Think of that as denormalized data. Then one can easily find all posts which involve a certain commenter. Then to drill down, you use map\/reduce or db.eval() to get the nested post info within.&lt;\/p&gt;\n\n&lt;p&gt;One other note - if you are dealing with a single document, db.eval() is probably lighter-weight than map\/reduce. $where is also an option but can be slow so I like the additional 'list of commenters' mentioned above - not it is also easy to index that array too (see 'Multikey' in the docs).&lt;\/p&gt;\n\n&lt;p&gt;See also:\n&lt;a href=&quot;http:\/\/groups.google.com\/group\/mongodb-user\/browse%5Fthread\/thread\/df8250573c91f75a\/e880d9c57e343b52?lnk=gst&amp;#38;amp;q=trees#e880d9c57e343b52&quot; rel=&quot;nofollow&quot;&gt;http:\/\/groups.google.com\/group\/mongodb-user\/browse%5Fthread\/thread\/df8250573c91f75a\/e880d9c57e343b52?lnk=gst&amp;#38;amp;q=trees#e880d9c57e343b52&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1879118","ParentId":"1839218","CreationDate":"2009-12-10T06:53:02.203","OwnerUserId":"170692","Tags":[],"Body":"&lt;p&gt;Hi,&lt;\/p&gt;\n\n&lt;p&gt;SimpleDB can only scale by sharding, has 10 GB data size limit per table, and query performance is parallel to record count (eg: poor if you have 1 million records). And google's datastore is slower than simpledb. Cassandra is much more scalable, high traffic sites began to use it, there is nothing better for free if you need high write rates with massive data. &lt;a href=&quot;http:\/\/n2.nabble.com\/Cassandra-users-survey-td4040068.html&quot; rel=&quot;nofollow&quot;&gt;cassandra survey&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;If your read\/write ratio is something like %90 for read and %10 for write, then terracotta  or infinispan with postgres is a better fit. There some free clustering options for postgresql but none of them matured (mostly prototypes).&lt;\/p&gt;\n\n&lt;p&gt;Another option is sharding. Hiberntae and NHibernate has sharding support. You can use them with postgres or mysql but you loose joins.&lt;\/p&gt;\n\n&lt;p&gt;Regards&lt;\/p&gt;\n"},{"Id":"1886658","ParentId":"1886650","CreationDate":"2009-12-11T08:40:39.697","OwnerUserId":"47680","Tags":[],"Body":"&lt;p&gt;Umm, &lt;a href=&quot;http:\/\/memcached.org\/&quot; rel=&quot;nofollow&quot;&gt;memcached&lt;\/a&gt;?&lt;\/p&gt;\n"},{"Id":"1886659","ParentId":"1886650","CreationDate":"2009-12-11T08:41:49.507","OwnerUserId":"71883","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/www.oracle.com\/database\/berkeley-db\/index.html&quot; rel=&quot;nofollow&quot;&gt;BerkeleyDB&lt;\/a&gt; is backed by Oracle&lt;\/p&gt;\n\n&lt;p&gt;Using the native C interface one can reach close to 1 million read requests per second.&lt;\/p&gt;\n\n&lt;p&gt;By the way, when you say thousands requests per minute, any 'normal' DB should be able to handle that easily too.&lt;\/p&gt;\n"},{"Id":"1886679","ParentId":"1886650","CreationDate":"2009-12-11T08:47:30.947","OwnerUserId":"124894","Tags":[],"Body":"&lt;p&gt;I think the NoSQL systems are an excellent choice if I you 'only' care about speed and service time (and not or less about stuff like consistency and transactions). Facebook uses Cassandra. &lt;\/p&gt;\n\n&lt;p&gt;&quot;Cassandra is used in Facebook as an email search system containing 25TB and over 100m mailboxes.&quot; &lt;a href=&quot;http:\/\/highscalability.com\/product-facebooks-cassandra-massive-distributed-store&quot; rel=&quot;nofollow&quot;&gt;http:\/\/highscalability.com\/product-facebooks-cassandra-massive-distributed-store&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I think CouchDb isn't really speedy, maybe you can use MongoDB: &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Production+Deployments&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Production+Deployments&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1886912","ParentId":"1813612","CreationDate":"2009-12-11T09:39:46.500","OwnerUserId":"2108","Tags":[],"Body":"&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Pick whichever middleware you are most comfortable with. &lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/couchapp\/couchapp&quot; rel=&quot;nofollow&quot;&gt;CouchApp&lt;\/a&gt; is very experimental at the moment. The main issue is being able to add security to your app without having a standard HTTP pop-up box. This is obviously a big issue for standard web apps.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Try and avoid parsing each DB request in the middleware and rebuilding the query for couchdb. You can make your middleware act like a proxy so most requests are forwarded on without modification. You can also add a security layer in the middlelayer on top of all requests that need authentication.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Pick a middleware\/framework with good URL routing capabilities. For example you could route all requests that go to mydomain.com\/db\/ to couchdb.&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"1888750","ParentId":"1886650","CreationDate":"2009-12-11T15:20:50.903","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Cassandra handles thousands of requests (including write-mostly workloads) per &lt;em&gt;second&lt;\/em&gt;, per machine, and its scaling-by-adding-machines has been there since day 1.&lt;\/p&gt;\n\n&lt;p&gt;Here is a thread about Cassandra use in production and in-production-soon at dozens of companies: &lt;a href=&quot;http:\/\/n2.nabble.com\/Cassandra-users-survey-td4040068.html#a4040068&quot; rel=&quot;nofollow&quot;&gt;http:\/\/n2.nabble.com\/Cassandra-users-survey-td4040068.html#a4040068&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;We're also adding more docs all the time, like &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/Operations&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/cassandra\/Operations&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"1893485","ParentId":"1886650","CreationDate":"2009-12-12T13:47:39.340","OwnerUserId":"142017","Tags":[],"Body":"&lt;p&gt;Also worth consideration is using a traditional RDBMS like MySQL to store schema-less.  This method gives you the stability of a proven database server like MySQL with the flexibility a NoSQL solution.&lt;\/p&gt;\n\n&lt;p&gt;Check out &lt;a href=&quot;http:\/\/bret.appspot.com\/entry\/how-friendfeed-uses-mysql&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt; blog posting on how &lt;a href=&quot;http:\/\/friendfeed.com\/&quot; rel=&quot;nofollow&quot;&gt;FriendFeed&lt;\/a&gt; does this.&lt;\/p&gt;\n"},{"Id":"1900810","ParentId":"1899843","CreationDate":"2009-12-14T13:03:31.877","OwnerUserId":"19636","Tags":[],"Body":"&lt;p&gt;The relationships in the Google App Engine are only keys to entities that are automatically de-referenced when accessed in code.  And are only values when used to filter against.  Its a function of the DB Api rather than anything explicit, so the access to the ReferenceProperty will simply perform a query against the referenced model to get access to the object.&lt;\/p&gt;\n\n&lt;p&gt;If you look at something like MongoDB, the relationships are stored in-object (from what I remeber), but they can also be stored however you want in the sense that you would create an API that would search the joined table for your item in the relationship in a similar manner to who the App Engine works.&lt;\/p&gt;\n\n&lt;p&gt;Paul.&lt;\/p&gt;\n"},{"Id":"1902758","ParentId":"1899843","CreationDate":"2009-12-14T18:51:18.727","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;It belongs to the core features of &lt;strong&gt;graph databases&lt;\/strong&gt; to provide support for relationships between entities. Typically, you model your entities as nodes and the relationships as relationships\/edges in the graph. Unlike &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Relational%5Fdatabase%5Fmanagement%5Fsystem&quot; rel=&quot;nofollow&quot;&gt;RDBMS&lt;\/a&gt; you don't have to define relationships in advance -- just add them to the graph as needed (schema-free). I created a &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/Domain%5FModeling%5FGallery&quot; rel=&quot;nofollow&quot;&gt;domain modeling gallery&lt;\/a&gt; giving a few examples of how this can look in practice. The examples use the Neo4j graphdb, a project I'm involved in. The &lt;a href=&quot;http:\/\/neo4j.org\/community\/list\/&quot; rel=&quot;nofollow&quot;&gt;mailing list&lt;\/a&gt; of this project use to prove very helpful for graph modeling questions.&lt;\/p&gt;\n\n&lt;p&gt;The document-oriented database &lt;a href=&quot;http:\/\/riak.basho.com\/&quot; rel=&quot;nofollow&quot;&gt;Riak&lt;\/a&gt; has support for links between documents.&lt;\/p&gt;\n\n&lt;p&gt;You can add support for relationships on top of any database engine (like key\/value), but it doesn't come whithout work. It all comes down to your use case. If you provide more details it's easier to come up with a useful answer.&lt;\/p&gt;\n\n&lt;p&gt;Oops, now I saw that the title says &quot;nosql store&quot; and then your actual question narrows this down to &quot;nosql key value store&quot;. As key\/value stores have no semantics for defining relationships between entities I'll still post my answer.&lt;\/p&gt;\n"},{"Id":"1909186","ParentId":"1909110","CreationDate":"2009-12-15T17:52:29.953","OwnerUserId":"3408","Tags":[],"Body":"&lt;p&gt;Probably because it's well promoted and quite good at what it does.&lt;\/p&gt;\n"},{"Id":"1909193","ParentId":"1909110","CreationDate":"2009-12-15T17:53:14.527","OwnerUserId":"17413","Tags":[],"Body":"&lt;p&gt;Here is a pretty good introduction&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.linux-mag.com\/cache\/7597\/1.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.linux-mag.com\/cache\/7597\/1.html&lt;\/a&gt;&lt;br&gt;&lt;\/p&gt;\n\n&lt;p&gt;Snip from article:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;You\u2019ll often see the word \u201crelax\u201d\n  associated with CouchDB. That\u2019s\n  because CouchDB tries to solve a lot\n  of the \u201chard problems\u201d associated with\n  building a scalable distributed\n  document-oriented database. It does a\n  lot of heavy lifting for you so that\n  you can focus on building your\n  application without worrying too much\n  about administration or weird corner\n  cases.&lt;\/p&gt;\n  \n  &lt;p&gt;CouchDB also sports a very simple and\n  easy to understand RESTful API. This\n  should make for a very low barrier to\n  entry and stress-free development. As\n  we progress through the process of\n  using CouchDB, I think you\u2019ll start to\n  realize that this motto is not just\n  \u201cmarketing speak.\u201d&lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"1909257","ParentId":"1909110","CreationDate":"2009-12-15T18:04:47.073","OwnerUserId":"146325","Tags":[],"Body":"&lt;p&gt;As the late, great Eric Morecombe once observed, &quot;Sofa, so good&quot;.&lt;\/p&gt;\n"},{"Id":"1909291","ParentId":"1909110","CreationDate":"2009-12-15T18:10:21.183","OwnerUserId":"11181","Tags":[],"Body":"&lt;blockquote&gt;\n  &lt;ol&gt;\n  &lt;li&gt;It's &lt;strong&gt;well-suited&lt;\/strong&gt; to a good portion of web app development today where scalability and online\/offline sysc are important (additionally, the strength of relational database's powerful data set analysis is often less important).&lt;\/li&gt;\n  &lt;li&gt;Arguably trivial &lt;strong&gt;replication&lt;\/strong&gt; built-in (replication is an afterthought in the lineage of most RDBMS ecosystems)&lt;\/li&gt;\n  &lt;li&gt;It's emerging as an &lt;strong&gt;essential part of the stack&lt;\/strong&gt; upon which desktop\/cloud sync services in the open source arena are being built (see &lt;a href=&quot;https:\/\/one.ubuntu.com\/&quot; rel=&quot;nofollow&quot;&gt;Ubuntu One&lt;\/a&gt;).  &lt;\/li&gt;\n  &lt;\/ol&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;Because of #3, there's a decent marketing\/awareness campaign behind it right now.&lt;\/p&gt;\n"},{"Id":"1922422","ParentId":"1502735","CreationDate":"2009-12-17T15:16:51.967","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;This was too long to add as a comment, so to clear up some misconceptions from the list-of-problems reply:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Any client may connect to any node; if the first node you pick (or you connect to via a load balancer) goes down, simply connect to another. Additionally, a &quot;fat client&quot; api is available where the client can direct the writes itself; an example is on &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/ClientExamples&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/cassandra\/ClientExamples&lt;\/a&gt;&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Timing out when a server is unresponsive rather than hanging indefinitely is a feature that most people who have dealt with overloaded rdbms systems has wished for.  The Cassandra RPC timeout is configurable; if you wish, you are free to set it to several days and deal with hanging indefinitely instead. :)&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;It is true that there is no multidelete or truncation support yet, but there are patches for both of these in review.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;There is obviously a tradeoff in keeping load balanced across cluster nodes: the more perfectly balanced you try to keep things, the more data movement you will do, which is not free.  By default, new nodes in a Cassandra cluster will move to the optimal position in the token ring to minimize uneven-ness.  In practice, this has been shown to work well, and the larger your cluster is, the less true it is that doubling is optimal.  This is covered more in &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/Operations&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/cassandra\/Operations&lt;\/a&gt;&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"1928136","ParentId":"1924871","CreationDate":"2009-12-18T13:14:25.587","OwnerUserId":"167866","Tags":[],"Body":"&lt;p&gt;Hi, &lt;\/p&gt;\n\n&lt;p&gt;Check out &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt;. It is a graph database (schema-free) that can be used like a document or key\/value store. &lt;\/p&gt;\n\n&lt;p&gt;Neo4j has been in production for many years in environments like you describe. Unlike many other NOSQL databases Neo4j actually flushes data to disk and uses a transaction log to recover from an inconsistent state. It also has real transactions (full ACID) that can span multiple operations and treat them as a single unit (which also seems to be a feature that is frequently left out in many other NOSQL stores).&lt;\/p&gt;\n\n&lt;p&gt;-Johan&lt;\/p&gt;\n\n&lt;p&gt;(Disclaimer: I am part of the Neo4j team)&lt;\/p&gt;\n"},{"Id":"1930294","ParentId":"1930243","CreationDate":"2009-12-18T19:42:20.903","OwnerUserId":"20860","Tags":[],"Body":"&lt;p&gt;Consider Martin Fowler's &lt;a href=&quot;http:\/\/www.martinfowler.com\/eaaCatalog\/serializedLOB.html&quot; rel=&quot;nofollow&quot;&gt;Serialized LOB&lt;\/a&gt; pattern:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE Documents (\n  documentid SERIAL PRIMARY KEY,\n  -- fixed relational attributes ...\n  document TEXT -- contains XML, YAML, whatever\n);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;You can put any semi-structured data with dynamic attributes into the &lt;code&gt;document&lt;\/code&gt; column.  You just can't easily use SQL predicates to search or sort by fields in that blob.  But you couldn't anyway -- variable attributes is a non-relational concept, and it's awkward to support them in SQL no matter what.&lt;\/p&gt;\n\n&lt;p&gt;You can use a hybrid approach, storing some fixed attributes in conventional columns, and all the variable attribute stuff in the blob.&lt;\/p&gt;\n\n&lt;p&gt;This points out why document-oriented databases exist.  They are designed to solve a problem that the relational paradigm has chosen not to support.  But document-oriented databases don't do some of the cool things that relational databases do, like referential integrity and even data type coherency.&lt;\/p&gt;\n"},{"Id":"1930754","ParentId":"1930243","CreationDate":"2009-12-18T21:21:39.797","OwnerUserId":"47303","Tags":[],"Body":"&lt;p&gt;A simple MySQL example:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE Docs (\n  id INT,\n  attr VARCHAR(255),\n  value BLOB,\n  PRIMARY KEY (id, attr),\n  KEY attr_index (attr)\n)\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Once you have that you can add any attribute to a document and stuff anything in the value, and you can use self joins on the document table to do complex queries like:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM Docs AS d1, docs AS d2 WHERE d1.attr = &quot;foo&quot; AND d2.attr = &quot;bar&quot;\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Which returns documents with both foo and bar attributes.&lt;\/p&gt;\n"},{"Id":"1933173","ParentId":"1813612","CreationDate":"2009-12-19T15:23:07.817","OwnerUserId":"230894","Tags":[],"Body":"&lt;p&gt;I've been tinkering with a few. Ultimately, I'd like to move my controller layer of MVC to the jQuery\/javascript frontend and use pure JSON\/REST to talk with the backend. Though the backend will need strong security and, for my application some ability to do workflow, queries, and rules. &lt;\/p&gt;\n\n&lt;p&gt;You also might want to look at:&lt;\/p&gt;\n\n&lt;p&gt;1) Couldkit, which runs on Tokyo Cabinet. Supports JSONQuery and OAuth. Runs on Ruby\/Rack may have enough functionality. Loks like a strong REST implementation.\n2) Persevere, which is Java based and strongly supported in Dojo. It is REST-ish but also has some RPC type calls. Seems very powerful overall, with server-side java scripting, etc.&lt;\/p&gt;\n\n&lt;p&gt;I wouldn't mind hearing how you're coming along.&lt;\/p&gt;\n\n&lt;p&gt;Cheers,\nAlex&lt;\/p&gt;\n"},{"Id":"1934092","ParentId":"1924871","CreationDate":"2009-12-19T20:48:35.833","OwnerUserId":"121199","Tags":[],"Body":"&lt;p&gt;CouchDB has the reliability you need:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;The CouchDB file layout and commitment system features all Atomic Consistent Isolated Durable (ACID) properties. On-disk, CouchDB never overwrites committed data or associated structures, ensuring the database file is always in a consistent state. &lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;Look at the ACID Properties section &lt;a href=&quot;http:\/\/couchdb.apache.org\/docs\/overview.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt; for more info.&lt;\/p&gt;\n\n&lt;p&gt;With CouchDB you also get easy backup and replication. &lt;\/p&gt;\n\n&lt;p&gt;I've no code in production using CouchDB yet, but so far I'm very happy with the tests and the development process with CouchDB.&lt;\/p&gt;\n"},{"Id":"1946581","ParentId":"1543965","CreationDate":"2009-12-22T14:09:33.873","OwnerUserId":"34107","Tags":[],"Body":"&lt;p&gt;The link to Tokyo Cabinet for .Net is for the client driver.\nThere is a port of the Tokyo Cabinet og Tyrant it can be downloaded from this site:\n&lt;a href=&quot;http:\/\/spench.net\/drupal\/software\/tokyocabinet&quot; rel=&quot;nofollow&quot;&gt;http:\/\/spench.net\/drupal\/software\/tokyocabinet&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I have not testet this port.&lt;\/p&gt;\n"},{"Id":"1951383","ParentId":"1528827","CreationDate":"2009-12-23T08:25:14.557","OwnerUserId":"220599","Tags":[],"Body":"&lt;p&gt;When you say NoSQL, I'm assuming you mean solutions such as CouchDB, MongoDB, Cassandra, etc.  I personally don't know if there are any CMS solutions that support these, but that doesn't mean there aren't.&lt;\/p&gt;\n\n&lt;p&gt;However, there are &lt;a href=&quot;http:\/\/wiki.apache.org\/jackrabbit\/JcrLinks&quot; rel=&quot;nofollow&quot;&gt;plenty of CMS systems&lt;\/a&gt; using &lt;a href=&quot;http:\/\/jackrabbit.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;Apache Jackrabbit&lt;\/a&gt; (an implementation of JCR - Java Content Repository), which is not a relational datastore.  As indicated on their site:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;A content repository is a hierarchical content store with \n  support for structured and unstructured content, full text \n  search, versioning, transactions, observation, and more.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;There are &lt;a href=&quot;http:\/\/wiki.apache.org\/jackrabbit\/JcrLinks&quot; rel=&quot;nofollow&quot;&gt;many CMS solutions&lt;\/a&gt; that use JCR\/Jackrabbit as the datastore.  I personally use &lt;a href=&quot;http:\/\/code.google.com\/p\/brix-cms\/&quot; rel=&quot;nofollow&quot;&gt;Brix-CMS&lt;\/a&gt; with my &lt;a href=&quot;http:\/\/wicket.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;Apache Wicket&lt;\/a&gt; projects.  There is also the very capable &lt;a href=&quot;http:\/\/www.onehippo.org\/&quot; rel=&quot;nofollow&quot;&gt;Hippo CMS&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Perhaps this isn't the type of solution you are looking for (especially if you aren't a Java developer), but in many ways JCR fits the needs of a CMS better than most NoSQL solutions.  Since you refer to GAE, I guess there's a 50% chance you would consider Java.&lt;\/p&gt;\n\n&lt;p&gt;I have not used GAE myself, but I've read that other have wicket applications running within it.  You would need to check and see if Hippo or Brix or some other JCR implementation would run within it.&lt;\/p&gt;\n\n&lt;p&gt;Good luck!&lt;\/p&gt;\n"},{"Id":"1954759","ParentId":"1950863","CreationDate":"2009-12-23T19:13:41.273","OwnerUserId":"234031","Tags":[],"Body":"&lt;p&gt;First off, please don't create your own _id, let CouchDB create a uuid for you. It's much better, I promise you :)&lt;\/p&gt;\n\n&lt;p&gt;The short answer is, you can't get the idea document with anything other than an additional query. Although the query is quite fast since you have the _id in your vote document.&lt;\/p&gt;\n\n&lt;p&gt;If you wanted to return the the full documents for all of the votes, in order to grab comments or something, you could definitely do that. Just run the view query with ?include_docs=true&lt;\/p&gt;\n"},{"Id":"1954804","ParentId":"1951061","CreationDate":"2009-12-23T19:20:49.340","OwnerUserId":"234031","Tags":[],"Body":"&lt;p&gt;Concurrent connections aren't a problem, erlang and CouchDB are built for concurrent performance.&lt;\/p&gt;\n\n&lt;p&gt;Are you thinking that you'll have to be generating new map functions dynamically, cause it kind of sounds like it?&lt;\/p&gt;\n\n&lt;p&gt;Whenever you add a new view map function you're going to hit a big bottleneck in the initial view generation.&lt;\/p&gt;\n\n&lt;p&gt;If you use erlang views they generate much faster than javascript views because they don't hit the JSON serialization step, this can significantly speed up the view generation performance.&lt;\/p&gt;\n\n&lt;p&gt;Once the view is generated it will be quite fast even with the size you're talking about.&lt;\/p&gt;\n"},{"Id":"1959820","ParentId":"1959818","CreationDate":"2009-12-24T21:33:58.533","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;I really don't see how this is a major goal of any scalable database. You could certainly use time to make some swanky visual tool, but&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;These data sets are usually so big that ad-hoc queries run against a production system will return too much data to be useful&lt;\/li&gt;\n&lt;li&gt;Developers have enough real problems to think about - like working out a compromise between availability, consistency and partition-tolerance&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"1959835","ParentId":"1959818","CreationDate":"2009-12-24T21:41:52.693","OwnerUserId":"171461","Tags":[],"Body":"&lt;p&gt;couchDB does: visit the &lt;code&gt;http:\/\/localhost:5984\/_utils\/&lt;\/code&gt; once installed.&lt;\/p&gt;\n"},{"Id":"1960570","ParentId":"1958365","CreationDate":"2009-12-25T05:42:10.423","OwnerUserId":"217275","Tags":[],"Body":"&lt;p&gt;Did you install mongo_ext? \nI think the performance is more related to the driver than the mapper itself. When looking at the mongo log, I can see without the extension, that the transer seems to have some lags.&lt;\/p&gt;\n\n&lt;p&gt;Also do as they recommend on the monogdb site, select only the fields you need.&lt;\/p&gt;\n"},{"Id":"1960576","ParentId":"1958365","CreationDate":"2009-12-25T05:45:12.877","OwnerUserId":"142240","Tags":[],"Body":"&lt;p&gt;&lt;code&gt;sudo gem install mongo_ext&lt;\/code&gt; is key to getting performance. &lt;\/p&gt;\n\n&lt;p&gt;MongoDB blows away CouchDB in terms of raw speed \u2013\u00a0though CDB doe shave its own set of advantages. &lt;\/p&gt;\n\n&lt;p&gt;Benchmark: &lt;a href=&quot;http:\/\/www.snailinaturtleneck.com\/blog\/?p=74&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.snailinaturtleneck.com\/blog\/?p=74&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1961090","ParentId":"1961013","CreationDate":"2009-12-25T12:18:32.237","OwnerUserId":"14343","Tags":[],"Body":"&lt;p&gt;One of the supposed benefits of these databases is that they are schemaless, and therefore don't need schema migration tools.  Instead, you write your data handling code to deal with the variety of data stored in the db.&lt;\/p&gt;\n"},{"Id":"1962068","ParentId":"1959818","CreationDate":"2009-12-25T22:12:51.343","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;When using a &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Graph%5Fdatabase&quot; rel=&quot;nofollow&quot;&gt;graph database&lt;\/a&gt; visualization of the data comes in very handy during both development and debugging. For the &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt; open source graph database there is &lt;a href=&quot;http:\/\/neoclipse.org\/&quot; rel=&quot;nofollow&quot;&gt;Neoclipse&lt;\/a&gt;, which can be used both to model the domain during development and to peek into an existing database; and &lt;a href=&quot;http:\/\/github.com\/obruening\/neoviz&quot; rel=&quot;nofollow&quot;&gt;neoviz&lt;\/a&gt;, a Rails based visualization tool for Neo4j.&lt;\/p&gt;\n"},{"Id":"1964138","ParentId":"1950863","CreationDate":"2009-12-26T19:27:00.243","OwnerUserId":"49407","Tags":[],"Body":"&lt;p&gt;I don't agree with server-side ID generation. Client-Side ID generation protects you from duplicate inserts. sha(&quot;%f@%s-%f&quot; % (time(), hostname(), random()) for example generates very reasonable IDs.&lt;\/p&gt;\n"},{"Id":"1966375","ParentId":"1961013","CreationDate":"2009-12-27T16:50:07.403","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;If your data are sufficiently big, you will probably find that you cannot &lt;em&gt;EVER&lt;\/em&gt; migrate the data, or that it is not beneficial to do so. This means that when you do a schema change, the code needs to continue to be backwards compatible with the old formats forever.&lt;\/p&gt;\n\n&lt;p&gt;Of course if your data &quot;age&quot; and eventually expire anyway, this can do schema migration for you - simply change the format for newly added data, then wait for all data in the old format to expire - you can then retire the backward-compatibility code.&lt;\/p&gt;\n"},{"Id":"1968598","ParentId":"1968589","CreationDate":"2009-12-28T08:57:00.867","OwnerUserId":"13051","Tags":[],"Body":"&lt;p&gt;You should first study the existing social networks out there (Facebook, Myspace, etc). There is a fair amount of information available about how they are implemented.&lt;\/p&gt;\n"},{"Id":"1968612","ParentId":"1968589","CreationDate":"2009-12-28T09:03:24.443","OwnerUserId":"34088","Tags":[],"Body":"&lt;p&gt;The key to success for social networks is not the technology on which it is based but the problems they solve for the users. If the users like it, you're doomed for success even if your technology is crap.&lt;\/p&gt;\n\n&lt;p&gt;[EDIT] How is it implemented? Check any SQL-based user role system. In this case, every user is also a role which can be added as &quot;allowed to access&quot; to any object. Depending on how many objects you have and how fine grained the control should be, that can mean that you have a table with three columns: &lt;code&gt;OBJECT, USER, ACCESS_TYPE&lt;\/code&gt; where &lt;code&gt;ACCESS_TYPE&lt;\/code&gt; can be one of &lt;code&gt;OWNER&lt;\/code&gt;, &lt;code&gt;READ&lt;\/code&gt; (friend), &lt;code&gt;WRITE&lt;\/code&gt; (close friend).&lt;\/p&gt;\n\n&lt;p&gt;This table will become pretty large but a few 100 million rows is not uncommon for todays databases anymore.&lt;\/p&gt;\n"},{"Id":"1968696","ParentId":"1968589","CreationDate":"2009-12-28T09:38:18.990","OwnerUserId":"164394","Tags":[],"Body":"&lt;p&gt;First thing to get out the way is the database, an SQL one would just look like a normalised sql database. What else could it look like? A nosql database would look like a bunch of name value pair files.&lt;\/p&gt;\n\n&lt;p&gt;Three approaches to building a social web site after and only after you do a shed load of research on existing popular and unpopular ones to ascertain their architecture and the markets that that they are aimed at, and the particular services that they offer to these markets.&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Roll your own from scratch (and or use a framework). Like Facebook, Beebo, Myspace et al. This is obviously the longest route to getting there but it does mean you have something to sell when you do. Both the platform and the membership and the USP are yours to sell to Rupert Murdoch or whomever.&lt;\/li&gt;\n&lt;li&gt;Use a CMS that lends itself to social site and use the basic functionality, plus the plug-ins plus your own inspiration to hit your target market. In this area Drupal is often used (i have used it successfully as well) but Joomla, Xaraya and many other both free and paid for can be used. Yep more research. Less to sell here when Rupert gives you a bell as the base tool is probably GPL'd&lt;\/li&gt;\n&lt;li&gt;Use one of the provided system where you sign up and then use the tools to build your own but all the goodies are provided, These are known as white label sites. &lt;a href=&quot;http:\/\/www.web-strategist.com\/blog\/2007\/02\/12\/list-of-white-label-social-networking-platforms\/&quot; rel=&quot;nofollow&quot;&gt;Start looking here&lt;\/a&gt;. Here you have little to sell on if someone wants to take you over.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;How is &quot;content visibility&quot; handled. Initially of course the site builder makes a decision on who can see content. Owners only, friends, registered users, the general public? etc. But this decision must fir in with the aims and policies of the site. The best way then to handle this is through Role Based Access RBAC &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Role-based%5Faccess%5Fcontrol&quot; rel=&quot;nofollow&quot;&gt;see here for details&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;When you say you &quot;need to design \/ architect \/ develop&quot; is this because of an overwhelming inner urge or because someone is paying you? &lt;\/p&gt;\n\n&lt;p&gt;Either way remember the social web space is very very crowded. If you are just building another YouTube, or FaceBook then you are unlikely to be able to generate the critical mass of number required to make such a site commercially successful.&lt;\/p&gt;\n\n&lt;p&gt;If it is for a niche market not already catered for, e.g. &quot;The Peckham and Brockley Exotic Bird Fanciers Club&quot; then you know what you market is and what features will be required so any of the above options you deem the easiest and cheapest can be used but that is up to you to analyse and execute.&lt;\/p&gt;\n\n&lt;p&gt;You may of course have an idea for a social site that is mainstream and is not covered by the other, i.e. you have spotted the mythological &quot;gap in the market&quot;. In this case go for it but prepare to be disappointed. Or not.&lt;\/p&gt;\n"},{"Id":"1968863","ParentId":"1968589","CreationDate":"2009-12-28T10:34:57.813","OwnerUserId":"89556","Tags":[],"Body":"&lt;p&gt;Your design should be maintenable. This is what I have in my project.&lt;\/p&gt;\n\n&lt;p&gt;1.) &lt;strong&gt;Application.Infrastructure&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Base classes for all businessobjects, busines object collection, data-access classes and my custom attributes and utilities as extension methods, Generic validation framework. This determines overall behavior organization of my final .net application.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;2.) &lt;strong&gt;Application.DataModel&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Typed Dataset for the Database.&lt;\/li&gt;\n&lt;li&gt;TableAdapters extended to incorporate Transactions and other features I may need.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;3.) &lt;strong&gt;Application.DataAccess&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data access classes.&lt;\/li&gt;\n&lt;li&gt;Actual place where Database actions are queried using underlying Typed Dataset.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;4.) &lt;strong&gt;Application.DomainObjects&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Business objects and Business object collections.&lt;\/li&gt;\n&lt;li&gt;Enums.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;5.) &lt;strong&gt;Application.BusinessLayer&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Provides manager classes accessible from Presentation layer.&lt;\/li&gt;\n&lt;li&gt;HttpHandlers.&lt;\/li&gt;\n&lt;li&gt;My own Page base class.&lt;\/li&gt;\n&lt;li&gt;More things go here..&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;6.) &lt;strong&gt;Application.WebClient&lt;\/strong&gt; or &lt;strong&gt;Application.WindowsClient&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My presentation layer&lt;\/li&gt;\n&lt;li&gt;Takes references from Application.BusinessLayer and Application.BusinessObjects.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Application.BusinessObjects are used across the application and they travel across all layers whenever neeeded [except Application.DataModel and Application.Infrastructure]&lt;\/p&gt;\n\n&lt;p&gt;All my queries are defined only Application.DataModel.&lt;\/p&gt;\n\n&lt;p&gt;Application.DataAccess returns or takes Business objects as part of any data-access operation. Business objects are created with the help of reflection attributes. Each business object is marked with an attribute mapping to target table in database and properties within the business object are marked with attributes mapping to target coloumn in respective data-base table.&lt;\/p&gt;\n\n&lt;p&gt;My validation framework lets me validate each field with the help of designated ValidationAttribute.&lt;\/p&gt;\n\n&lt;p&gt;My framrwork heavily uses Attributes to automate most of the tedious tasks like mapping and validation. I can also new feature as new aspect in the framework.&lt;\/p&gt;\n\n&lt;p&gt;A sample business object would look like this in my application.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;User.cs&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[TableMapping(&quot;Users&quot;)]\npublic class User : EntityBase\n{\n    #region Constructor(s)\n    public AppUser()\n    {\n        BookCollection = new BookCollection();\n    }\n    #endregion\n\n    #region Properties\n\n    #region Default Properties - Direct Field Mapping using DataFieldMappingAttribute\n\n    private System.Int32 _UserId;\n\n    private System.String _FirstName;\n    private System.String _LastName;\n    private System.String _UserName;\n    private System.Boolean _IsActive;\n\n    [DataFieldMapping(&quot;UserID&quot;)]\n    [DataObjectFieldAttribute(true, true, false)]\n    [NotNullOrEmpty(Message = &quot;UserID From Users Table Is Required.&quot;)]\n    public override int Id\n    {\n        get\n        {\n            return _UserId;\n        }\n        set\n        {\n            _UserId = value;\n        }\n    }\n\n    [DataFieldMapping(&quot;UserName&quot;)]\n    [Searchable]\n    [NotNullOrEmpty(Message = &quot;Username Is Required.&quot;)]\n    public string UserName\n    {\n        get\n        {\n            return _UserName;\n        }\n        set\n        {\n            _UserName = value;\n        }\n    }\n\n    [DataFieldMapping(&quot;FirstName&quot;)]\n    [Searchable]\n    public string FirstName\n    {\n        get\n        {\n            return _FirstName;\n        }\n        set\n        {\n            _FirstName = value;\n        }\n    }\n\n    [DataFieldMapping(&quot;LastName&quot;)]\n    [Searchable]\n    public string LastName\n    {\n        get\n        {\n            return _LastName;\n        }\n        set\n        {\n            _LastName = value;\n        }\n    }\n\n    [DataFieldMapping(&quot;IsActive&quot;)]\n    public bool IsActive\n    {\n        get\n        {\n            return _IsActive;\n        }\n        set\n        {\n            _IsActive = value;\n        }\n    }\n\n    #region One-To-Many Mappings\n    public BookCollection Books { get; set; }\n\n    #endregion\n\n    #region Derived Properties\n    public string FullName { get { return this.FirstName + &quot; &quot; + this.LastName; } }\n\n    #endregion\n\n    #endregion\n\n    public override bool Validate()\n    {\n        bool baseValid = base.Validate();\n        bool localValid = Books.Validate();\n        return baseValid &amp;#38;amp;&amp;#38;amp; localValid;\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;BookCollection.cs&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\/\/\/ &amp;#38;lt;summary&amp;#38;gt;\n\/\/\/ The BookCollection class is designed to work with lists of instances of Book.\n\/\/\/ &amp;#38;lt;\/summary&amp;#38;gt;\npublic class BookCollection : EntityCollectionBase&amp;#38;lt;Book&amp;#38;gt;\n{\n    \/\/\/ &amp;#38;lt;summary&amp;#38;gt;\n    \/\/\/ Initializes a new instance of the BookCollection class.\n    \/\/\/ &amp;#38;lt;\/summary&amp;#38;gt;\n    public BookCollection()\n    {\n    }\n\n    \/\/\/ &amp;#38;lt;summary&amp;#38;gt;\n    \/\/\/ Initializes a new instance of the BookCollection class.\n    \/\/\/ &amp;#38;lt;\/summary&amp;#38;gt;\n    public BookCollection (IList&amp;#38;lt;Book&amp;#38;gt; initialList)\n        : base(initialList)\n    {\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"1971012","ParentId":"1968589","CreationDate":"2009-12-28T19:07:35.920","OwnerUserId":"53404","Tags":[],"Body":"&lt;p&gt;Hi there,\na Graph Database like &lt;a href=&quot;http:\/\/www.neo4j.org&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.neo4j.org&lt;\/a&gt; is a choice to look at. It lends itself very well to both the social network (e.g. &lt;a href=&quot;http:\/\/blog.neo4j.org\/2009\/09\/social-networks-in-database-using-graph.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blog.neo4j.org\/2009\/09\/social-networks-in-database-using-graph.html&lt;\/a&gt;) and the ACL-based security (e.g. &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/ACL&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.neo4j.org\/content\/ACL&lt;\/a&gt; ).&lt;\/p&gt;\n"},{"Id":"1971103","ParentId":"1968589","CreationDate":"2009-12-28T19:27:02.420","OwnerUserId":"44886","Tags":[],"Body":"&lt;p&gt;As Aaroon pointed out, you should first ask youself what problem you want to solve. &lt;\/p&gt;\n\n&lt;p&gt;What content do you want people to share? Should it really be visible only to friends? It is much easier and scalable if you make content publicly visible, because what content is displayed is not dependent on who is watching the page and you can cache it easily. Publicly available user-generated content attracts new users. &lt;\/p&gt;\n\n&lt;p&gt;If you want to restrict access and give to the user the opportunity to attach groups of friends to a resource I would go with a simple group-based access control. Let each resource have a group of users which can edit the resource and a group of users who can see it. &lt;\/p&gt;\n\n&lt;p&gt;That way each resource has two single value attributes, and each user belons to a finite number of group. You can attach the view-group and edit-group attributes to a document stored in a NOSQL database, a search engine like Lucene\/Sphinx or a row in a SQL database. When querying for content available for the user, pass all groups the user belongs to (in SQL you would use &lt;code&gt;IN&lt;\/code&gt; clause, in Sphinx &lt;code&gt;setFilter('view-group', array(2,3,4))&lt;\/code&gt;. The database would return only content available for the user. Because you are attaching only 2 integer values (view-group and edit-group) to a document, you can store them in memory which makes the search fast and scalable.&lt;\/p&gt;\n"},{"Id":"1976277","ParentId":"1528827","CreationDate":"2009-12-29T18:59:46.827","OwnerUserId":"105938","Tags":[],"Body":"&lt;p&gt;There's a MongoDB-based CMS called &lt;a href=&quot;http:\/\/get.harmonyapp.com\/&quot; rel=&quot;nofollow&quot;&gt;Harmony&lt;\/a&gt; currently in private beta. It's being developed in part by John Nunemaker, the guy behind &lt;a href=&quot;http:\/\/github.com\/jnunemaker\/mongomapper&quot; rel=&quot;nofollow&quot;&gt;MongoMapper&lt;\/a&gt;. There's &lt;a href=&quot;http:\/\/railstips.org\/2009\/12\/18\/why-i-think-mongo-is-to-databases-what-rails-was-to-frameworks&quot; rel=&quot;nofollow&quot;&gt;a blog post up on RailsTips&lt;\/a&gt; where he talks about the advantages of using MongoDB for a problem like this. I don't know anything about how it'll be hosted, etc. since I'm not in the private beta, but it's certainly a step in the NOSQL direction, and looks quite interesting.&lt;\/p&gt;\n"},{"Id":"1978889","ParentId":"1595562","CreationDate":"2009-12-30T07:23:31.577","OwnerUserId":"45224","Tags":[],"Body":"&lt;p&gt;One approach to improving the speed of your database is to denormalize. Take this MySQL example:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE `users` (\n    `user_id` INT NOT NULL AUTO_INCREMENT,\n    \u2026 -- Additional user data\n    PRIMARY KEY (`user_id`)\n);\n\n\nCREATE TABLE `roles` (\n    `role_id` INT NOT NULL AUTO_INCREMENT,\n    `name` VARCHAR(64),\n    PRIMARY KEY (`role_id`)\n);\n\n\nCREATE TABLE `users_roles` (\n    `user_id` INT NOT NULL,\n    `role_id` INT NOT NULL,\n    PRIMARY KEY (`user_id`, `role_id`)\n);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Neat, tidy, normalized. But if you want to get users and their roles, the query is complex:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT u.*, r.*\n  FROM `users` u\n  LEFT JOIN `user_roles` ur ON u.`user_id` = ur.`user_id`\n  JOIN `roles` r ON ur.`role_id` = r.`role_id`;\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;If you denormalized this, it might look something like:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE `users` (\n    `user_id` INT NOT NULL AUTO_INCREMENT,\n    `role` VARCHAR(64),\n    \u2026 -- Additional user data\n    PRIMARY KEY (`user_id`)\n);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;And the equivalent query would be:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM `users`;\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;This improves some of the performance characteristics of your queries:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Because the result you want is already in a table, you don't have to perform read-side calculations. e.g. if you wanted to see the number of users with a given role, you'd need a &lt;code&gt;GROUP BY&lt;\/code&gt; and &lt;code&gt;COUNT&lt;\/code&gt;. If it were denormalized, you would store it in a different table devoted to holding roles and counts of users who have that role.&lt;\/li&gt;\n&lt;li&gt;The data you want is in the same place, and hopefully in the same place on disk. Rather than requiring many random seeks, you can do one to a few sequential reads.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;NoSQL DBs are highly optimized for these cases, where you want to access a mostly-static sequential dataset. At that point, it's just moving bytes from disk to the network. Less work, less overhead, more speed. Despite how simple this sounds, it's possible to model your data and application so it feels natural.&lt;\/p&gt;\n\n&lt;p&gt;The trade-off for this performance is write load, disk space, and some app complexity.  Denormalizing your data means more copies, which means more disk space and write load. Essentially, you have one dataset per query.  Because you shift the burden of those computations to write-time instead of read-time, you really need some sort of asynchronous mechanism to do that, hence some app complexity.&lt;\/p&gt;\n\n&lt;p&gt;And because you have to store more copies, you have to perform more writes. This is why you can't practically replicate this kind of architecture with a SQL database \u2013 it's extremely difficult to scale writes.&lt;\/p&gt;\n\n&lt;p&gt;In my experience, the trade-off is well worth it for a large-scale application. If you'd like to read a bit more about a practical application of Cassandra, &lt;a href=&quot;http:\/\/about.digg.com\/blog\/looking-future-cassandra&quot; rel=&quot;nofollow&quot;&gt;I wrote this piece&lt;\/a&gt; a few months ago, and you might find it helpful.&lt;\/p&gt;\n"},{"Id":"1985381","ParentId":"1985352","CreationDate":"2009-12-31T13:23:34.130","OwnerUserId":"27637","Tags":[],"Body":"&lt;p&gt;You might find some helpful information here:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1189911\/non-relational-database-design&quot;&gt;Non-Relational Database Design&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1886650\/are-there-any-stable-and-production-quality-nosql-datastores&quot;&gt;Are there any stable and production quality nosql datastores ?&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1777103\/what-nosql-solutions-are-out-there-for-net&quot;&gt;What NoSQL solutions are out there for .NET&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"1985382","ParentId":"1985352","CreationDate":"2009-12-31T13:23:46.437","OwnerUserId":"235058","Tags":[],"Body":"&lt;p&gt;I think this is not the right question. The question &quot;which NoSQL DBMS is most X ...&quot; is not of th same form  as &quot;which RDBMS is most X on ...&quot; &lt;\/p&gt;\n\n&lt;p&gt;Why? Well, NoSQL is not really clearly defined. There are a few recurring characteristics (distributed, large volume, map\/reduce processing, scaleable, redundant, not-relational, schemaless) but none of these are &lt;em&gt;defining characteristics&lt;\/em&gt;. To put it bluntly, NoSQL is a bucket of techniques and products, each with their own special underlying design goals and applicability. &lt;\/p&gt;\n\n&lt;p&gt;With traditional RDBMS-es the similarities are much clearer.&lt;\/p&gt;\n\n&lt;p&gt;So I guess you should ask yourself, what do you want to achieve? Then look for a product\/technique to match it.&lt;\/p&gt;\n"},{"Id":"1985854","ParentId":"1950863","CreationDate":"2009-12-31T15:36:25.470","OwnerUserId":"149230","Tags":[],"Body":"&lt;p&gt;Map:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function(doc) {\n    switch (doc.type) {\n        case &quot;idea&quot;:   emit(doc._id,     [&quot;idea&quot;,  doc]);         break;\n        case &quot;rating&quot;: emit(doc.idea_id, [&quot;rating&quot;, doc.rating]); break;\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Reduce:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function(keys, values, rereduce) {\n    var i, l, ret = {idea:null, rating:0};\n\n    for (i = 0, l = values.length; i &amp;#38;lt; l; ++i) {\n        switch (values[i][0]) {\n            case &quot;idea&quot;:   ret.idea = values[i][1];    break;\n            case &quot;rating&quot;: ret.rating += values[i][1]; break;\n        }\n    }\n\n    return ret;\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Another option would be to &lt;a href=&quot;http:\/\/chrischandler.name\/couchdb\/view-collation-for-join-like-behavior-in-couchdb\/&quot; rel=&quot;nofollow&quot;&gt;use view collation&lt;\/a&gt; to do the trick.&lt;\/p&gt;\n"},{"Id":"1991613","ParentId":"1950863","CreationDate":"2010-01-02T13:38:18.963","OwnerUserId":"242298","Tags":[],"Body":"&lt;p&gt;Seconding @mdornseif. UUIDs can be generated client side to avoid network errors. In case no UUID generation facility is at hand, CouchDB has a \/_uuids endpoint to request one or more for later use.&lt;\/p&gt;\n"},{"Id":"1993349","ParentId":"1985352","CreationDate":"2010-01-02T23:53:38.130","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;You absolutely must gather your requirements and evaluate as many options as you can.&lt;\/p&gt;\n\n&lt;p&gt;As you're likely to be deploying a massive system that will need to be maintained for years, the cost of a poor choice is very high; try to get your entire development team's input on it, as it will matter (it's them who will need to maintain it, after all).&lt;\/p&gt;\n\n&lt;p&gt;Operations is as important as development; get your operations team's input too if at all possible. As a &quot;nosql&quot; based system is likely to involve many physical machines, it may require a lot of effort to maintain by ops engineers if problems are frequent. It is important that your ops team understand how such a system works.&lt;\/p&gt;\n\n&lt;p&gt;Maturity of drivers (aka client library or whatever) clearly might be a factor, but I rather suspect that its actual features are more of an issue. None of these systems behave the same, and all offer different feature sets with incompatible interfaces which are not similar to those offered by a traditional RDBMS.&lt;\/p&gt;\n"},{"Id":"1995238","ParentId":"1995216","CreationDate":"2010-01-03T15:13:19.310","OwnerUserId":"179233","Tags":[],"Body":"&lt;p&gt;You would have to do multiple selects, and join the data manually in your application.  See &lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/782913\/googles-bigtable-vs-a-relational-database&quot;&gt;this SO post&lt;\/a&gt; for more information.  From that post:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;Bigtable datasets can be queried from services like AppEngine using a language called GQL (&quot;gee-kwal&quot;) which is a based on a subset of SQL. Conspicuously missing from GQL is any sort of JOIN command. Because of the distributed nature of a Bigtable database, performing a join between two tables would be terribly inefficient. Instead, the programmer has to implement such logic in his application, or design his application so as to not need it.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"1995372","ParentId":"1995216","CreationDate":"2010-01-03T15:57:44.453","OwnerUserId":"71134","Tags":[],"Body":"&lt;p&gt;Kaleb's right. You write custom code with a NoSQL solution if your data doesn't fit well into a key-value store. Map-reduce\/async processing and custom view caches are common. Brian Aker gave a very funny (and satirical and biased) presentation at the Nov 2009 OpenSQLCamp &lt;a href=&quot;http:\/\/www.youtube.com\/watch?v=LhnGarRsKnA&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.youtube.com\/watch?v=LhnGarRsKnA&lt;\/a&gt;. Skip in 40 seconds to hear about joins.&lt;\/p&gt;\n"},{"Id":"1996579","ParentId":"1995216","CreationDate":"2010-01-03T22:26:07.100","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;When you have extremely large data, you probably want to avoid joins. This is because the overhead of an individual key lookup is relatively large (the service needs to figure out which node(s) to query, and query them in parallel and wait for responses). By overhead, I mean latency, not throughput limitation.&lt;\/p&gt;\n\n&lt;p&gt;This makes joins suck really badly as you'd need to do a lot of foreign key lookups, which would end up going to many,many different nodes (in many cases). So you'd want to avoid this as a pattern.&lt;\/p&gt;\n\n&lt;p&gt;If it doesn't happen very often, you could probably take the hit, but if you're going to want to do a lot of them, it may be worth &quot;denormalising&quot; the data.&lt;\/p&gt;\n\n&lt;p&gt;The kind of stuff which gets stored in NoSQL stores is typically pretty &quot;abnormal&quot; in the first place. It is not uncommon to duplicate the same data in all sorts of different places to make lookups easier.&lt;\/p&gt;\n\n&lt;p&gt;Additionally most nosql don't (really) support secondary indexes either, which means you have to duplicate stuff if you want to query by any other criterion.&lt;\/p&gt;\n\n&lt;p&gt;If you're storing data such as employees and departments, you're really better off with a conventional database.&lt;\/p&gt;\n"},{"Id":"1997078","ParentId":"1997069","CreationDate":"2010-01-04T01:25:45.537","OwnerUserId":"239696","Tags":[],"Body":"&lt;p&gt;It only uses its &lt;a href=&quot;http:\/\/s3.amazonaws.com\/AllThingsDistributed\/sosp\/amazon-dynamo-sosp2007.pdf&quot; rel=&quot;nofollow&quot;&gt;Dynamo&lt;\/a&gt; key-value store for its shopping cart and select other applications.&lt;\/p&gt;\n"},{"Id":"1997085","ParentId":"1997069","CreationDate":"2010-01-04T01:29:17.883","OwnerUserId":"222908","Tags":[],"Body":"&lt;p&gt;You may want to start your research by checking out these Stack Overflow articles:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/1189911\/non-relational-database-design&quot;&gt;Non-Relational Database Design&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/103727\/how-to-think-in-data-stores-instead-of-databases&quot;&gt;How to think in data stores instead of databases?&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/176131\/pros-of-databases-like-bigtable-simpledb&quot;&gt;Pro\u2019s of databases like BigTable, SimpleDB&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/445827\/gae-how-to-live-with-no-joins\/446471#446471&quot;&gt;GAE - How to live with no joins?&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/648270\/whats-the-point-of-using-amazon-simpledb&quot;&gt;What\u2019s the point of using Amazon SimpleDB?&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/348939\/eventual-consistency&quot;&gt;Eventual Consistency&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/930966\/app-engine-datastore-does-not-support-operator-or&quot;&gt;App Engine datastore does not support operator OR&lt;\/a&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;The Amazon datastore is offered to the public as &lt;a href=&quot;http:\/\/aws.amazon.com\/simpledb\/&quot; rel=&quot;nofollow&quot;&gt;SimpleDB&lt;\/a&gt;, which is part of the &lt;a href=&quot;http:\/\/aws.amazon.com\/&quot; rel=&quot;nofollow&quot;&gt;Amazon Web Services&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Also note that Google offers a similar &quot;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Bigtable&quot; rel=&quot;nofollow&quot;&gt;sparse, distributed multi-dimensional sorted map&lt;\/a&gt;&quot; datastore for the &lt;a href=&quot;http:\/\/code.google.com\/appengine\/&quot; rel=&quot;nofollow&quot;&gt;Google App Engine&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2012941","ParentId":"2012900","CreationDate":"2010-01-06T12:44:51.340","OwnerUserId":"50394","Tags":[],"Body":"&lt;p&gt;Most programming languages come with tools to serialize and unserialize their native data structures. Use those to store the data in text files.&lt;\/p&gt;\n\n&lt;p&gt;This way you'll have data that is compliant and supported with your programming language without the need of slow &lt;em&gt;parsers&lt;\/em&gt;.&lt;\/p&gt;\n\n&lt;p&gt;For example in PHP you could use the &lt;code&gt;serialize()&lt;\/code&gt; function to turn any data structure into a &lt;strong&gt;string&lt;\/strong&gt; (&lt;code&gt;unserialize()&lt;\/code&gt; does the opposite).&lt;\/p&gt;\n\n&lt;p&gt;The querying becomes as simple as accessing native data structures, no need for the complication\/slowness of SQL.&lt;\/p&gt;\n"},{"Id":"2012949","ParentId":"2012900","CreationDate":"2010-01-06T12:46:50.677","OwnerUserId":"52201","Tags":[],"Body":"&lt;p&gt;Xml, With Linq-To-Xml?&lt;\/p&gt;\n\n&lt;p&gt;Human Readable and can be partially updated or read in all at once.&lt;\/p&gt;\n"},{"Id":"2013012","ParentId":"2012900","CreationDate":"2010-01-06T12:59:11.623","OwnerUserId":"122012","Tags":[],"Body":"&lt;p&gt;In my youth, there existed some kind of of awk-based database engine. It was just a set of shell-scripts. But I don't remember its name and I don't know if it was SQL-compliant or free.&lt;\/p&gt;\n\n&lt;p&gt;But the stored data was totally text based.&lt;\/p&gt;\n\n&lt;p&gt;I did not find it there, but also this link might be of use:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.faqs.org\/faqs\/databases\/free-databases\/&quot; rel=&quot;nofollow&quot;&gt;Catalog of free databases&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2013057","ParentId":"2012900","CreationDate":"2010-01-06T13:06:28.477","OwnerUserId":"244358","Tags":[],"Body":"&lt;p&gt;This might not be exactly what you work looking for, but I think something like CouchDB would be an excellent choice for this kind of application. It is queried via json but I don't think the stored data is directly editable.&lt;\/p&gt;\n"},{"Id":"2013806","ParentId":"2012900","CreationDate":"2010-01-06T15:05:14.723","OwnerUserId":"15459","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Flat_file_database&quot; rel=&quot;nofollow&quot; title=&quot;Flat file database - Wikipedia, the free encyclopedia&quot;&gt;Flat file database&lt;\/a&gt; shows that the idea isn't too stretched...&lt;\/p&gt;\n\n&lt;p&gt;I wonder why your requirements excludes &quot;loading all data into memory&quot;. Do you think you will have several GB of &quot;some data&quot;? Even some MB of data shouldn't be too much to load and maintain in memory, after all a browser consumes much more memory with the Dom of a large Web page...&lt;\/p&gt;\n\n&lt;p&gt;I suppose you can use &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Memory-mapped_file&quot; rel=&quot;nofollow&quot; title=&quot;Memory-mapped file - Wikipedia, the free encyclopedia&quot;&gt;memory-mapped file&lt;\/a&gt; to reduce memory usage, but maintaining an accurate index might be more challenging.&lt;br&gt;\nI wonder if Thunderbird, which stores the mailboxes in plain text files (mbox format), uses such technique.&lt;\/p&gt;\n"},{"Id":"2013831","ParentId":"2012900","CreationDate":"2010-01-06T15:08:37.197","OwnerUserId":"145357","Tags":[],"Body":"&lt;p&gt;MongoDB has a lot in common with CouchDB, but simpler.&lt;\/p&gt;\n\n&lt;p&gt;You can try my library (be careful) &lt;a href=&quot;http:\/\/code.google.com\/p\/mongodloid\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/code.google.com\/p\/mongodloid\/&lt;\/a&gt; to make your life even easier&lt;\/p&gt;\n"},{"Id":"2016810","ParentId":"1968589","CreationDate":"2010-01-06T22:29:33.553","OwnerUserId":"239472","Tags":[],"Body":"&lt;p&gt;In the end it looks like Elgg or Dolphin might meet our requirements.  These appear to be PHP frameworks for rolling your own social network.  I looked at the Facebook platform but nowhere did it clearly explain just what it is - it appears to be the facebook code but perhaps it is only the code for an addon API or something.&lt;\/p&gt;\n"},{"Id":"2023504","ParentId":"1905688","CreationDate":"2010-01-07T20:43:30.440","OwnerUserId":"234655","Tags":[],"Body":"&lt;p&gt;The main reason is scale (Facebook, MySpace)&lt;\/p&gt;\n\n&lt;p&gt;Check out these articles:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/highscalability.com\/product-facebooks-cassandra-massive-distributed-store&quot; rel=&quot;nofollow&quot;&gt;Facebook's Cassandra - A Massive Distributed Store&lt;\/a&gt; &lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/qizmt.myspace.com\/&quot; rel=&quot;nofollow&quot;&gt;MySpace Qizmt - MySpace's Mapreduce Framework&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;As you are already using Lucene, this may be of interest:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/katta.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;Katta - Lucene in the cloud&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2028554","ParentId":"2012900","CreationDate":"2010-01-08T15:28:48.377","OwnerUserId":"124894","Tags":[],"Body":"&lt;p&gt;Berkeley XML DB? &lt;a href=&quot;http:\/\/www.oracle.com\/database\/berkeley-db\/xml\/index.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.oracle.com\/database\/berkeley-db\/xml\/index.html&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2032155","ParentId":"1815731","CreationDate":"2010-01-09T02:44:27.660","OwnerUserId":"20003","Tags":[],"Body":"&lt;ul&gt;\n&lt;li&gt;don't worry if your queries are efficient until it starts to matter&lt;\/li&gt;\n&lt;li&gt;according to below quote, you're doing it wrong&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;The way I have been going about the\n  mind switch is to forget about the\n  database alltogether. In the\n  relational db world you always have to\n  worry about data normalization and\n  your table structure. Ditch it all.\n  Just layout your web page. Lay them\n  all out. Now look at them. Your\n  already 2\/3 there. If you forget the\n  notion that database size matters and\n  data shouldn't be duplicated than your\n  3\/4 there and you didnt even have to\n  write any code! Let your views dictate\n  your Models. You don't have to take\n  your objects and make them 2\n  dimensional anymore as in the\n  relational world. You can store\n  objects with shape now.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/103727\/how-to-think-in-data-stores-instead-of-databases&quot;&gt;how-to-think-in-data-stores-instead-of-databases&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2040509","ParentId":"1528827","CreationDate":"2010-01-11T07:49:44.310","OwnerUserId":"247865","Tags":[],"Body":"&lt;p&gt;Avinu Beyond the Cloud uses MongoDB and the Vork framework\n&lt;a href=&quot;http:\/\/www.Avinu.org&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.Avinu.org&lt;\/a&gt; &lt;\/p&gt;\n"},{"Id":"2042965","ParentId":"1997069","CreationDate":"2010-01-11T15:59:36.090","OwnerUserId":"3211","Tags":[],"Body":"&lt;p&gt;The Amazon.com architecture is very interesting. They moved to a service oriented architecture, if you look at all the different content areas on their site, each one is served by a different service. So there is a 'wish list' service and a 'Related to Items You've Viewed' service, and Bestsellers service, Shopping cart service, etc. &lt;\/p&gt;\n\n&lt;p&gt;Each of the services has its own set of requirements and features. The requirements include things like response time and availability. Internally each service is implemented using whatever database best suits the needs. The key value store is good for a shopping cart, because you never need to do:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select * from book where book_id = n\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;on a shopping cart.&lt;\/p&gt;\n\n&lt;p&gt;One of the important things to realize is the enormous role that availability plays at Amazon scale. Consider that Amazon 2008 revenue was $19.166 billion. The total retail revenue from from the Amazon.com site may be more than $1000 per second during the day (it may be double that, for all I know, during peak hours. It could be 5 times that during peak holiday shopping). Think of the cost if the shopping cart service goes down for 3 minutes during peak usage. It is clear that the loss would be a large dollar value in abandon carts.&lt;\/p&gt;\n\n&lt;p&gt;Using a key-value store doesn't mean embracing rampant data duplication, it means redesigning applications so the necessary data doesn't need sit all in one monolithic database.&lt;\/p&gt;\n\n&lt;p&gt;Amazon is really more of a platform for applications than anything else. Here is a &lt;a href=&quot;http:\/\/www.infoq.com\/presentations\/vogels-amazon-platform&quot; rel=&quot;nofollow&quot;&gt;video of Amazon's CTO&lt;\/a&gt; talking about just that.&lt;\/p&gt;\n"},{"Id":"2045418","ParentId":"2041622","CreationDate":"2010-01-11T22:31:06.347","OwnerUserId":"53404","Tags":[],"Body":"&lt;p&gt;Hi there,\ngraphs databases like &lt;a href=&quot;http:\/\/www.neo4j.org&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt; are a very good fit, especially as you can add different indexing schemes dynamically as you go. Typical stuff you can do on your base data is of course 1D indexing (e.g. Timline or B-Trees) or funkier stuff like Hilbert Curves etc, see &lt;a href=&quot;http:\/\/blog.notdot.net\/2009\/11\/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves&quot; rel=&quot;nofollow&quot;&gt;Nick's blog&lt;\/a&gt;. Also, for some live demonstration, look at the AWE open source GIS desktop tool &lt;a href=&quot;http:\/\/www.youtube.com\/craigtaverner&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;, the underlying indexed graph being visible around time 07:00 .&lt;\/p&gt;\n"},{"Id":"2056754","ParentId":"2041622","CreationDate":"2010-01-13T13:04:41.253","OwnerUserId":"159235","Tags":[],"Body":"&lt;p&gt;I've been storing spatial data with ZODB. There's some inherent performance advantage in accessing local file data (spatialite) or unix socket (PostGIS) compared to TCP or HTTP requests (CouchDB etc), surely, but having an spatial index makes the biggest difference. I'm using the same R-trees mentioned in the MongoDB article, but there are plenty of good options. The JTS topology suite has various spatial indexes for Java.&lt;\/p&gt;\n"},{"Id":"2059509","ParentId":"1813612","CreationDate":"2010-01-13T19:17:04.107","OwnerUserId":"250136","Tags":[],"Body":"&lt;p&gt;Also if you like the idea of JSON\/REST and sticking to JavaScript client to server, the newer generation of Persevere's core, Pintura is pure JS JSON\/REST framework that is designed specifically to work well with NoSQL DBs.&lt;\/p&gt;\n"},{"Id":"2081090","ParentId":"2081080","CreationDate":"2010-01-17T13:27:56.490","OwnerUserId":"47550","Tags":[],"Body":"&lt;p&gt;Have you considered simply serializing an ADO DataSet for your data store?&lt;\/p&gt;\n"},{"Id":"2081415","ParentId":"2081080","CreationDate":"2010-01-17T15:20:07.670","OwnerUserId":"128709","Tags":[],"Body":"&lt;p&gt;Personally I would go for SQLite with NHibernate (and Fluent NHibernate). NHibernate can generate the database schema automatically for your classes, so you just need to specify what classes you want to persist, and that's quite easy with Fluent NHibernate. Furthermore, you can search for specific objects and you don't need to load all data to memory.&lt;\/p&gt;\n\n&lt;p&gt;Best Regards&lt;br&gt;\nOliver Hanappi&lt;\/p&gt;\n"},{"Id":"2081578","ParentId":"2081148","CreationDate":"2010-01-17T16:05:18.750","OwnerUserId":"21239","Tags":[],"Body":"&lt;p&gt;I don't claim to have a definitive answer to all of this (it's a rather open-ended question which you should try to break into smaller parts and it depends on your actual requirements, in fact I'm tempted to vote to close it) but I will comment on a few things:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I would forget about modelling this on a RDBMS. &lt;a href=&quot;http:\/\/www.kimbly.com\/blog\/000239.html&quot; rel=&quot;nofollow&quot;&gt;Faceted search just doesn't work in a relational schema&lt;\/a&gt;.&lt;\/li&gt;\n&lt;li&gt;IMO this is not the right place for code generation. You should design your code so it doesn't change with data changes (I'm not talking about &lt;em&gt;schema&lt;\/em&gt; changes).&lt;\/li&gt;\n&lt;li&gt;Storing metadata \/ attributes on an Excel spreadsheet seems like a very bad idea. I'd build a UI to edit this, which would be stored on Solr \/ MongoDB \/ CouchDB \/ whatever you choose to manage this.&lt;\/li&gt;\n&lt;li&gt;Solr &lt;strong&gt;does not&lt;\/strong&gt; &quot;just mirror relational DB&quot;. In fact, Solr is completely independent of relational databases. One of the most common cases &lt;em&gt;is&lt;\/em&gt; dumping data from a RDBMS to Solr (denormalizing data in the process), but Solr is flexible enough to work without any relational data source.&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/wiki.apache.org\/solr\/HierarchicalFaceting&quot; rel=&quot;nofollow&quot;&gt;Hierarchical faceting in Solr&lt;\/a&gt; is still an open issue in research. Currently there are two separate approaches being researched (&lt;a href=&quot;http:\/\/issues.apache.org\/jira\/browse\/SOLR-64&quot; rel=&quot;nofollow&quot;&gt;SOLR-64&lt;\/a&gt;, &lt;a href=&quot;http:\/\/issues.apache.org\/jira\/browse\/SOLR-792&quot; rel=&quot;nofollow&quot;&gt;SOLR-792&lt;\/a&gt;)&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"2082588","ParentId":"2081080","CreationDate":"2010-01-17T20:53:37.507","OwnerUserId":"43901","Tags":[],"Body":"&lt;p&gt;You can use document store MongoDB, it has a .Net driver and it doesn't have a schema. However it is &lt;strong&gt;not&lt;\/strong&gt; embedded, MongoDB runs as a separate process. See &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/samus\/mongodb-csharp&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2120804","ParentId":"2113866","CreationDate":"2010-01-22T21:52:15.253","OwnerUserId":"105938","Tags":[],"Body":"&lt;p&gt;From the article, it sounds like Fossil isn't a database any more than git is a database. Yes, it's a thing that contains data, and yes, it's backed by a database, but it seems pretty far from a database itself. So the first part of of your question basically relies on a faulty assumption. There is a database called &lt;a href=&quot;http:\/\/friendlyorm.com\/&quot; rel=&quot;nofollow&quot;&gt;Friendly&lt;\/a&gt; which uses MySQL to store schema-less models, but it seems like an awkward bandaid sort of solution at best.&lt;\/p&gt;\n\n&lt;p&gt;I'm certainly not familiar with all of the NoSQL options out there, but, to my knowledge, none of the well-though-of ones use SQL for anything. MongoDB and CouchDB, the two I'm most familiar with, both use Javascript as part of their query interface, though in very different ways. MongoDB has queries more like what you'd expect from a relational database: you can write an arbitrary query for all documents that match a certain set of attributes. However, unlike a relational database, there's no such thing as a join (you'll only ever get a list of distinct documents back, not compound documents) and you can write arbitrary Javascript code to select documents. CouchDB, on the other hand, does not allow arbitrary queries. Instead, you create views (which are essentially simpler key-value stores) using map\/reduce functions written in Javascript and then query those views from a start key to and end key.&lt;\/p&gt;\n\n&lt;p&gt;In both cases, the type of information being transmitted to the server to perform the query isn't well-suited for the type of problem that SQL is good at solving. The trade-off to SQL being so high-level (to use the logic of the author of the paper) is that it's only suitable for a very narrow set of problems.&lt;\/p&gt;\n"},{"Id":"2123450","ParentId":"1799958","CreationDate":"2010-01-23T14:25:01.733","OwnerUserId":"190822","Tags":[],"Body":"&lt;p&gt;Riak can not be run on Windows, only on Linux and Mac.&lt;\/p&gt;\n"},{"Id":"2140125","ParentId":"2138454","CreationDate":"2010-01-26T14:50:46.987","OwnerUserId":"215860","Tags":[],"Body":"&lt;p&gt;There's currently no way to filter on embedded docs in the way you're describing. Using the dot notation allows you to match on an embedded doc, but the entire document, parent and all, will still be returned. It's also possible to select which fields will be returned, but that doesn't really help your case, either.&lt;\/p&gt;\n\n&lt;p&gt;We have a &quot;virtual collections&quot; case, which would implement the desired functionality; feel free to vote on it:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/jira.mongodb.org\/browse\/SERVER-142&quot; rel=&quot;nofollow&quot;&gt;http:\/\/jira.mongodb.org\/browse\/SERVER-142&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;In the meantime, you should probably treat comments as their own collection. In general, if you need to work with a given data set on its own, make it a collection. If it's better conceived of as part of some other set, it's better to embed.&lt;\/p&gt;\n"},{"Id":"2140200","ParentId":"2140011","CreationDate":"2010-01-26T15:02:13.440","OwnerUserId":"220825","Tags":[],"Body":"&lt;p&gt;For crazy performance contraints you can't beat Tokyo Cabinet: &lt;a href=&quot;http:\/\/1978th.net\/tokyocabinet\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/1978th.net\/tokyocabinet\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;There is also a server component called Tokyo Tyrant which looks really cool.&lt;\/p&gt;\n"},{"Id":"2140228","ParentId":"2140011","CreationDate":"2010-01-26T15:06:53.720","OwnerUserId":"206367","Tags":[],"Body":"&lt;p&gt;What about Sqlite? The site is &lt;a href=&quot;http:\/\/sqlite.org\/&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;. The front end to edit\/manage the sqlite database is &lt;a href=&quot;http:\/\/sqliteman.com\/&quot; rel=&quot;nofollow&quot;&gt;sqliteman&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Hope this helps,\nBest regards,\nTom.&lt;\/p&gt;\n"},{"Id":"2144279","ParentId":"337344","CreationDate":"2010-01-27T02:53:11.387","OwnerUserId":"259740","Tags":[],"Body":"&lt;p&gt;Another advantage of document-oriented databases is the ease of usage and programming so that untrained business users, for example, can create applications and design their own databases. Information can be added without worrying about the &quot;record size&quot; and so programmers simply need to build an interface to allow the information to be entered easily.&lt;\/p&gt;\n"},{"Id":"2144344","ParentId":"337344","CreationDate":"2010-01-27T03:13:25.130","OwnerUserId":"46571","Tags":[],"Body":"&lt;p&gt;A possibility is to have a main relational database that stores definitions of items that can be retrieved by their IDs, and a document database for the descriptions and\/or specifications of those items. For example, you could have a relational database with a Products table with the following fields:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ProductID&lt;\/li&gt;\n&lt;li&gt;Description&lt;\/li&gt;\n&lt;li&gt;UnitPrice&lt;\/li&gt;\n&lt;li&gt;LotSize&lt;\/li&gt;\n&lt;li&gt;Specifications&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;And that Specifications field would actually contain a reference to a document with the technical specifications of the product. This way, you have the best of both worlds.&lt;\/p&gt;\n"},{"Id":"2144765","ParentId":"2140011","CreationDate":"2010-01-27T05:26:40.147","OwnerUserId":"234031","Tags":[],"Body":"&lt;p&gt;Does your project want to support some form of offline data? If so you should probably go with CouchDB since the replication model is designed to support offline data changes and sync.&lt;\/p&gt;\n"},{"Id":"2151268","ParentId":"2140011","CreationDate":"2010-01-27T23:49:45.580","OwnerUserId":"164255","Tags":[],"Body":"&lt;p&gt;MongoDB works great with C - there is both a C driver and a C++ driver.  The database uses the C++ driver itself for functions like replication (MongoDB is written in C++).&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Drivers&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Drivers&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2162858","ParentId":"1799958","CreationDate":"2010-01-29T15:19:57.323","OwnerUserId":"261916","Tags":[],"Body":"&lt;p&gt;It does run, altough I havent managed to run it as a service yet.&lt;\/p&gt;\n\n&lt;p&gt;Install CYGwin, install latest erlang, get source code, compile in cygwin&lt;\/p&gt;\n\n&lt;p&gt;then the fun part, adjust according to your paths and place into a batch&lt;\/p&gt;\n\n&lt;p&gt;c:\\riak\\rel\\riak\\erts-5.7.4\\bin\\erl -boot c:\\riak\\rel\\riak\\releases\\0.8\\riak -embedded -config c:\\riak\\rel\\riak\\etc\\app.config -args_file c:\\riak\\rel\\riak\\etc\\vm.args -- console&lt;\/p&gt;\n\n&lt;p&gt;Regards&lt;\/p&gt;\n"},{"Id":"2171599","ParentId":"2167481","CreationDate":"2010-01-31T12:11:31.197","OwnerUserId":"247469","Tags":[],"Body":"&lt;p&gt;I found this : &lt;a href=&quot;http:\/\/www.emmet-gray.com\/Articles\/ESE.htm&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.emmet-gray.com\/Articles\/ESE.htm&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;There is a sample vb.net app that displays the tables + columns for an edb file.   I'm using Win7 x64, and I couldnt open the database given in the sample path 'C:\\WINDOWS\\SoftwareDistribution\\DataStore\\DataStore.edb' - I received a &quot;The database page size does not match the engine&quot; error.&lt;\/p&gt;\n\n&lt;p&gt;However, I created a new edb file using the c# managed api, and was able to use the browser with this file.&lt;\/p&gt;\n\n&lt;p&gt;Hope it helps someone...&lt;\/p&gt;\n"},{"Id":"2172261","ParentId":"2170152","CreationDate":"2010-01-31T15:45:46.347","OwnerUserId":"145366","Tags":[],"Body":"&lt;p&gt;&quot;NoSQL&quot; should be more about building the datastore to follow your application requirements, not about building the app to follow a certain structure -- that's more like a traditional &lt;em&gt;SQL approach&lt;\/em&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Don't abandon a relational database &quot;just because&quot;; only do it if your app really needs to.&lt;\/p&gt;\n"},{"Id":"2173089","ParentId":"2173082","CreationDate":"2010-01-31T19:59:04.813","OwnerUserId":"101258","Tags":[],"Body":"&lt;p&gt;From wikipedia:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;NoSQL is an umbrella term for a loosely defined class of non-relational data stores that break with a long history of relational databases and ACID guarantees. Data stores that fall under this term may not require fixed table schemas, and usually avoid join operations. The term was first popularised in early 2009.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;The motivation for such an architecture was high scalability, to support sites such as Facebook, advertising.com, etc...&lt;\/p&gt;\n"},{"Id":"2173090","ParentId":"2173082","CreationDate":"2010-01-31T19:59:13.660","OwnerUserId":"110933","Tags":[],"Body":"&lt;p&gt;Take a look at these:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Nosql#List_of_NoSQL_open_source_projects&quot; rel=&quot;nofollow&quot;&gt;http:\/\/en.wikipedia.org\/wiki\/Nosql#List_of_NoSQL_open_source_projects&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;and this:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Comparing+Mongo+DB+and+Couch+DB&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Comparing+Mongo+DB+and+Couch+DB&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2173158","ParentId":"2173082","CreationDate":"2010-01-31T20:23:02.700","OwnerUserId":"234815","Tags":[],"Body":"&lt;p&gt;I used something called the &lt;a href=&quot;http:\/\/www.raima.com\/products\/&quot; rel=&quot;nofollow&quot;&gt;Raima Data Manager&lt;\/a&gt; more than a dozen years ago, that qualifies as NoSQL.  It calls itself a &quot;Set Oriented Database&quot;  Its not based on tables, and there is no query &quot;language&quot;, just an C API for asking for subsets.  &lt;\/p&gt;\n\n&lt;p&gt;It's &lt;em&gt;fast&lt;\/em&gt; and easier to work with in C\/C++ and SQL, there's no building up strings to pass to a query interpreter and the data comes back as an enumerable object rather than as an array.  variable sized records are normal and don't waste space.  I never saw the source code, but there were some hints at the interface that internally, the code used pointers a lot. &lt;\/p&gt;\n\n&lt;p&gt;I'm not sure that the product I used is even sold anymore, but the company is still around.&lt;\/p&gt;\n"},{"Id":"2177226","ParentId":"2170152","CreationDate":"2010-02-01T14:13:38.047","OwnerUserId":"6844","Tags":[],"Body":"&lt;p&gt;I think that currently, the whole idea of NoSQL data stores and the concept of document databases is so new and different from the established ideas which drive relational storage that there are currently very few (if any) best practices.&lt;\/p&gt;\n\n&lt;p&gt;We know at this point that the rules for storing your data within say CouchDB (or any other document database) are rather different to those for a relational one. For example, it is pretty much a fact that normalisation and aiming for 3NF is not something one should strive for. One of the common examples would be that of a simple blog.&lt;\/p&gt;\n\n&lt;p&gt;In a relational store, you'd have a table each for &quot;Posts&quot;, &quot;Comments&quot; and &quot;Authors&quot;. Each Author would have many Posts, and each Post would have many Comments. This is a model which works well enough, and maps fine over any relational DB. However, storing the same data within a docDB would most likely be rather different. You'd probably have something like a collection of Post documents, each of which would have its own Author and collection of Comments embedded right in. Of course that's probably not the only way you could do it, and it is somewhat a compromise (now querying for a single post is fast - you only do one operation and get everything back), but you have no way of maintaining the relationship between authors and posts (since it all becomes part of the post document).&lt;\/p&gt;\n\n&lt;p&gt;I too have seen examples making use of a &quot;type&quot; attribute (in a CouchDB example). Sure, that sounds like a viable approach. Is it the best one? I haven't got a clue. Certainly in MongoDB you'd use seperate collections within a database, making the type attribute total nonsense. In CouchDB though... perhaps that &lt;em&gt;is&lt;\/em&gt; best. The other alternatives? Separate databases for each type of document? This seems a bit loopy, so I'd lean towards the &quot;type&quot; solution myself. But that's just me. Perhaps there's something better.&lt;\/p&gt;\n\n&lt;p&gt;I realise I've rambled on quite a bit here and said very little, most likely nothing you didn't already know. My point is this though - I think its up to us to experiment with the tools we've got and the data we're working with and over time the good ideas will be spread and &lt;em&gt;become&lt;\/em&gt; the best-practices. I just think you're asking a little too early in the game.&lt;\/p&gt;\n"},{"Id":"2183517","ParentId":"1974069","CreationDate":"2010-02-02T11:14:00.067","OwnerUserId":"18548","Tags":[],"Body":"&lt;p&gt;I joined the infogrid mailing list and wrote to the dev team to include a simple getting started article. The dev's have been kind enough to provide the same on their wiki here: &lt;a href=&quot;http:\/\/infogrid.org\/wiki\/Examples\/FirstStep&quot; rel=&quot;nofollow&quot;&gt;http:\/\/infogrid.org\/wiki\/Examples\/FirstStep&lt;\/a&gt; &lt;\/p&gt;\n"},{"Id":"2206058","ParentId":"1777103","CreationDate":"2010-02-05T09:06:48.993","OwnerUserId":"231183","Tags":[],"Body":"&lt;p&gt;db4o is great, but be aware that it has an open source version, but it is not free for commercial use.&lt;\/p&gt;\n"},{"Id":"2211979","ParentId":"1974069","CreationDate":"2010-02-06T04:01:29.287","OwnerUserId":"267575","Tags":[],"Body":"&lt;p&gt;I am an early user and have some experience I can share. Basically this stuff rocks. It's quite hard to get your head into at first but once once you get past the &quot;aha&quot; it makes complete sense.&lt;\/p&gt;\n"},{"Id":"2214161","ParentId":"2212230","CreationDate":"2010-02-06T18:26:26.637","OwnerUserId":"248393","Tags":[],"Body":"&lt;p&gt;Depends on your DB, but ... I would say in general, you can use &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Optimistic_concurrency_control&quot; rel=&quot;nofollow&quot;&gt;'Optimistic transactions'&lt;\/a&gt; to achieve this but I imagine one should make sure to understand the database implementation's &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Atomic_operation&quot; rel=&quot;nofollow&quot;&gt;atomicity&lt;\/a&gt; guarantees (e.g. what kind of write and read operations are atomic).&lt;\/p&gt;\n\n&lt;p&gt;There seems to be &lt;a href=&quot;http:\/\/www.google.co.uk\/search?q=hbase+transactions&amp;#38;amp;esrch=FT1&quot; rel=&quot;nofollow&quot;&gt;some discussions on the net&lt;\/a&gt; about &lt;a href=&quot;http:\/\/hadoop.apache.org\/hbase\/&quot; rel=&quot;nofollow&quot;&gt;HBase&lt;\/a&gt; transactions, if thats any help.&lt;\/p&gt;\n"},{"Id":"2214420","ParentId":"2212230","CreationDate":"2010-02-06T19:35:30.363","OwnerUserId":"164255","Tags":[],"Body":"&lt;p&gt;Generally speaking, NoSQL solutions have lighter weight transactional semantics than relational databases, but still have facilities for atomic operations at some level.&lt;\/p&gt;\n\n&lt;p&gt;Generally, the ones which do master-master replication provide less in the way of consistency, and more availability.  So one should choose the right tool for the right problem.&lt;\/p&gt;\n\n&lt;p&gt;Many offer transactions at the single document (or row etc.) level.  For example with MongoDB there is atomicity at the single document - but documents can be fairly rich so this usually works pretty well -- more info &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Atomic+Operations&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2214948","ParentId":"2124216","CreationDate":"2010-02-06T22:14:12.093","OwnerUserId":"186593","Tags":[],"Body":"&lt;p&gt;I don't have any personal experience with it, but you might check out MongoDB which has drivers in C# etc.&lt;\/p&gt;\n\n&lt;p&gt;See this blog post for some info: &lt;a href=&quot;http:\/\/odetocode.com\/Blogs\/scott\/archive\/2009\/10\/13\/experimenting-with-mongodb-from-c.aspx&quot; rel=&quot;nofollow&quot;&gt;http:\/\/odetocode.com\/Blogs\/scott\/archive\/2009\/10\/13\/experimenting-with-mongodb-from-c.aspx&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2219403","ParentId":"2041622","CreationDate":"2010-02-08T03:31:41.780","OwnerUserId":"266609","Tags":[],"Body":"&lt;p&gt;Couchdb also has a simple spatial extension &lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/vmx.cx\/cgi-bin\/blog\/index.cgi\/category\/CouchDB&quot; rel=&quot;nofollow&quot;&gt;http:\/\/vmx.cx\/cgi-bin\/blog\/index.cgi\/category\/CouchDB&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2224193","ParentId":"2212279","CreationDate":"2010-02-08T19:16:26.887","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Cassandra rows can be very large, so consider modeling it as columns in a row rather than rows in a CF; then you can use the column slice operations, which are faster than row slices.  If there are no &quot;natural&quot; keys associated with this then you can use daily or hourly keys like &quot;2010\/02\/08 13:00&quot;.&lt;\/p&gt;\n\n&lt;p&gt;Otherwise, yes, using range queries (get_key_range is deprecated in 0.5; use get_range_slice) is your best option.&lt;\/p&gt;\n"},{"Id":"2224669","ParentId":"1899843","CreationDate":"2010-02-08T20:34:30.133","OwnerUserId":"12089","Tags":[],"Body":"&lt;p&gt;MongoDB is a document database, not a key\/value store. It does provide, however, a simple form of &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/DB+Ref&quot; rel=&quot;nofollow&quot;&gt;inter-document references&lt;\/a&gt;. These work more-or-less like SQL foreign keys that are automatically nulled when the referenced object is deleted.&lt;\/p&gt;\n\n&lt;p&gt;This is adequate for the same sorts of things for which you'd use foreign keys, but it isn't optimized for serious graph traversal.&lt;\/p&gt;\n"},{"Id":"2229458","ParentId":"2229420","CreationDate":"2010-02-09T13:46:52.610","OwnerUserId":"111124","Tags":[],"Body":"&lt;p&gt;I'm not sure I would agree that the traditional SQL databases can not handle these volumes, I can query through much larger datasets within those timeframes, but it has been designed specifically to handle that kind of work and placed on suitable hardware, specifically an IO subsystem that is designed to handle large data requests. &lt;\/p&gt;\n"},{"Id":"2229461","ParentId":"2229420","CreationDate":"2010-02-09T13:47:21.533","OwnerUserId":"4918","Tags":[],"Body":"&lt;p&gt;That really depends on what clauses you have in your WHERE and what kind of projection you need on your data.&lt;\/p&gt;\n\n&lt;p&gt;It might be good enough to create the appropriate index on your table.&lt;\/p&gt;\n\n&lt;p&gt;Also, even having an optimal data structure is of no use, if you have to read 100GB per query as that will take its time too.&lt;\/p&gt;\n"},{"Id":"2229473","ParentId":"2229420","CreationDate":"2010-02-09T13:48:23.813","OwnerUserId":"55159","Tags":[],"Body":"&lt;p&gt;&lt;code&gt;NoSQL&lt;\/code&gt;, as you may have read, is not a relational database.&lt;\/p&gt;\n\n&lt;p&gt;It is a database which stores key-value pairs which you can traverse using a proprietary &lt;code&gt;API&lt;\/code&gt;.&lt;\/p&gt;\n\n&lt;p&gt;This implies you will need to define the physical layout of the data yourself, as well as do any code optimizations.&lt;\/p&gt;\n\n&lt;p&gt;I'm quite outdated on this, but several years ago I've participated in a &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Berkeley_DB&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;BerkeleyDB&lt;\/code&gt;&lt;\/a&gt; project dealing with slightly less but still high volumes of data (about &lt;code&gt;100Gb&lt;\/code&gt;).&lt;\/p&gt;\n\n&lt;p&gt;It was perfectly OK for our needs.&lt;\/p&gt;\n\n&lt;p&gt;Please also note, though it may seem obvious to you, that the queries can be optimized. Could you please post the query you use here?&lt;\/p&gt;\n"},{"Id":"2229523","ParentId":"2229420","CreationDate":"2010-02-09T13:55:14.363","OwnerUserId":"15401","Tags":[],"Body":"&lt;p&gt;If you want to do ad-hoc queries for reporting or analysis you're probably better off with something that will play nicely with off-the-shelf reporting tools.  Otherwise you are likely to find yourself getting dragged off all the time to write little report programs to query the data.  This is a strike against NoSQL type databases, but it may or may not be an issue depending on your circumstances.&lt;\/p&gt;\n\n&lt;p&gt;300GB should not be beyond the capabilities of modern RDBMS platforms, even MS SQL Server.  Some other options for large database queries of this type are:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;See if you can use a SSAS cube and aggregations to mitigate your query performance issues.  Usage-based optimiisation might get you adequate performance without having to get another database system.  SSAS can also be used in shared-nothing configurations, allowing you to stripe your queries across a cluster of relatively cheap servers with direct-attach disks.  Look at ProClarity for a front-end if you do go this way.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Sybase IQ is a RDBMS platform that uses an underlying data structure optimised for reporting queries.  It has the advantage that it plays nicely with a reasonable variety of conventional reporting tools.  Several other systems of this type exist, such as Red Brick, Teradata or Greenplum (which uses a modified version of PostgreSQL).  The principal strike against these systems is that they are not exactly mass market items and can be quite expensive.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Microsoft has a shared-nothing version of SQL Server in the pipeline, which you might be able to use.  However they've tied it to third party hardware manufacturers so you can only get it with dedicated (and therefore expensive) hardware.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Look for opportunities to build data marts with aggregated data to reduce the volumes for some of the queries.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Look at tuning your hardware.  Direct attach SAS arrays and RAID controllers can put through streaming I\/O of the sort used in table scans pretty quickly.  If you partition your tables over a large number of mirrored pairs you can get very fast streaming performance - easily capable of saturating the SAS channels.&lt;br&gt;&lt;br&gt;Practically, you're looking at getting 10-20GB\/sec from your I\/O subsystem if you want the performance targets you describe, and it is certianly possible to do this without resorting to really exotic hardware.&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2229554","ParentId":"2229420","CreationDate":"2010-02-09T14:00:08.340","OwnerUserId":"31326","Tags":[],"Body":"&lt;p&gt;From what little I understand, traditional RDBMS are row based which optimizes for insertion speed.  But retrieval speed optimization is best achieved with a column based storage system.  &lt;\/p&gt;\n\n&lt;p&gt;See &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Column-oriented_DBMS&quot; rel=&quot;nofollow&quot;&gt;Column oriented DBMS&lt;\/a&gt; for a more thorough explanation than I could give&lt;\/p&gt;\n"},{"Id":"2229880","ParentId":"2229420","CreationDate":"2010-02-09T14:46:08.847","OwnerUserId":"9034","Tags":[],"Body":"&lt;p&gt;A properly set up SQL server should be able to handle data in the terrabytes without having performance problems. I have several friends who manage SQl Server databases that size with no perfomance issues. &lt;\/p&gt;\n\n&lt;p&gt;Your problem may be one or more of these:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Inadequate server specs&lt;\/li&gt;\n&lt;li&gt;Lack of good partitioning&lt;\/li&gt;\n&lt;li&gt;Poor indexing&lt;\/li&gt;\n&lt;li&gt;Poor database design&lt;\/li&gt;\n&lt;li&gt;Poor query design including using\ntools like LINQ which may write\npoorly performing code for a database\nthat size.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;It assuredly is NOT the ability of SQL Server to handle these loads. If you have a databse that size you need to hire a professional dba with experience in optimizing large systems.&lt;\/p&gt;\n"},{"Id":"2232849","ParentId":"2231019","CreationDate":"2010-02-09T22:01:24.683","OwnerUserId":"234655","Tags":[],"Body":"&lt;p&gt;Coming from a HBase\/BigTable point of view, typically you would completely denormalize your data, and use a &quot;list&quot; field, or multidimensional map column (see this &lt;a href=&quot;http:\/\/jimbojw.com\/wiki\/index.php?title=Understanding_Hbase_and_BigTable&quot; rel=&quot;nofollow&quot;&gt;link&lt;\/a&gt; for a better description).&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;The word &quot;column&quot; is another loaded\n  word like &quot;table&quot; and &quot;base&quot; which\n  carries the emotional baggage of years\n  of RDBMS experience. &lt;\/p&gt;\n  \n  &lt;p&gt;Instead, I find it easier to think\n  about this like a multidimensional map\n  - a map of maps if you will.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;For your example for a many-to-many relationship, you can still create two tables, and use your multidimenstional map column to hold the relationship between the tables.&lt;\/p&gt;\n\n&lt;p&gt;See the FAQ question 20 in the Hadoop\/HBase &lt;a href=&quot;http:\/\/wiki.apache.org\/hadoop\/Hbase\/FAQ#A20&quot; rel=&quot;nofollow&quot;&gt;FAQ&lt;\/a&gt;:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;Q:[Michael Dagaev] How would you\n  design an Hbase table for many-to-many\n  association between two entities, for\n  example Student and Course?  &lt;\/p&gt;\n  \n  &lt;p&gt;I would\n  define two tables:  Student: student\n  id student data (name, address, ...)\n  courses (use course ids as column\n  qualifiers here) Course: course id\n  course data (name, syllabus, ...)\n  students (use student ids as column\n  qualifiers here)  Does it make sense? &lt;\/p&gt;\n  \n  &lt;p&gt;A[Jonathan Gray] : Your design does\n  make sense.  As you said, you'd\n  probably have two column-families in\n  each of the Student and Course tables.\n  One for the data, another with a\n  column per student or course. For\n  example, a student row might look\n  like: Student : id\/row\/key = 1001\n  data:name = Student Name data:address\n  = 123 ABC St courses:2001 = (If you need more information about this\n  association, for example, if they are\n  on the waiting list) courses:2002 =\n  ...  This schema gives you fast access\n  to the queries, show all classes for a\n  student (student table, courses\n  family), or all students for a class\n  (courses table, students family).&lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"2232908","ParentId":"2231019","CreationDate":"2010-02-09T22:11:21.467","OwnerUserId":"111332","Tags":[],"Body":"&lt;p&gt;In MongoDB an often used approach would be store a list of _ids of car types in each car shop. So no separate join table but still basically doing a client-side join.&lt;\/p&gt;\n\n&lt;p&gt;Embedded documents become more relevant for cases that aren't many-to-many like this.&lt;\/p&gt;\n"},{"Id":"2234395","ParentId":"2231019","CreationDate":"2010-02-10T04:22:23.313","OwnerUserId":"234031","Tags":[],"Body":"&lt;p&gt;I can only speak to CouchDB.&lt;\/p&gt;\n\n&lt;p&gt;The best way to stick your data in the db is to not normalize it at all beyond converting it to JSON. If that data is &quot;cars&quot; then stick all the data about every car in the database.&lt;\/p&gt;\n\n&lt;p&gt;You then use map\/reduce to create a normalized index of the data. So, if you want an index of every car, sorted first by shop, then by car-type you would emit each car with an index of [shop, car-type].&lt;\/p&gt;\n\n&lt;p&gt;Map reduce seems a little scary at first, but you don't need to understand all the complicated stuff or even btrees, all you need to understand is how the key sorting works.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/wiki.apache.org\/couchdb\/View_collation&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/couchdb\/View_collation&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;With that alone you can create amazing normalized indexes over differing documents with the map reduce system in CouchDB.&lt;\/p&gt;\n"},{"Id":"2235198","ParentId":"2229420","CreationDate":"2010-02-10T07:50:36.117","OwnerUserId":"13724","Tags":[],"Body":"&lt;p&gt;I expect a &quot;conventional&quot; database can do what you want, provided you structure your data appropriately for the queries you're doing.&lt;\/p&gt;\n\n&lt;p&gt;You may find that in order to generate reports respectably, you need to summarise your data as it is generated (or loaded, transformed etc) and report off the summary data.&lt;\/p&gt;\n\n&lt;p&gt;The speed of a SELECT is not related (directly, in most cases) to the number of conditions in the WHERE clause (usually), but it is to do with the explain plan and the number of rows examined. There are tools which will analyse this for you.&lt;\/p&gt;\n\n&lt;p&gt;Ultimately, at 300G (which is not THAT big) you will probably need to keep some of your data on disc (=slow) at least some of the time so you want to start reducing the number of IO operations required. Reducing IO operations may mean making covering indexes, summary tables and copies of data with differing clustered indexes. This makes your 300G bigger, but who cares.&lt;\/p&gt;\n\n&lt;p&gt;IO ops are king :)&lt;\/p&gt;\n\n&lt;p&gt;Clearly doing these things is very expensive in terms of developer time, so you should start by throwing lots of hardware at the problem, and only try to fix it with software once that becomes insufficient. Lots of RAM is a start (but it won't be able to store &gt; 10-20% of your data set at a time at current cost-effective levels) Even SSDs are not that expensive these days.&lt;\/p&gt;\n"},{"Id":"2247223","ParentId":"2247036","CreationDate":"2010-02-11T19:44:54.013","OwnerUserId":"63051","Tags":[],"Body":"&lt;p&gt;No. Right now, NoSql databases are very disparate, therefore they cannot be wrapped under a standard interface while remaining non trivial.&lt;\/p&gt;\n"},{"Id":"2247790","ParentId":"2247036","CreationDate":"2010-02-11T21:21:49.567","OwnerUserId":"200304","Tags":[],"Body":"&lt;p&gt;We have such an abstraction in &lt;a href=&quot;http:\/\/infogrid.org\/&quot; rel=&quot;nofollow&quot;&gt;InfoGrid&lt;\/a&gt; called the Store interface. It is very simplistic, but was created exactly for that purpose: a common API that allows InfoGrid to talk to different key-value store implementations without requiring changes on the higher levels.&lt;\/p&gt;\n\n&lt;p&gt;Some links:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/infogrid.org\/wiki\/Store&quot; rel=&quot;nofollow&quot;&gt;Store summary&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/infogrid.org\/browser\/trunk\/ig-stores\/modules\/org.infogrid.store\/src\/org\/infogrid\/store\/Store.java&quot; rel=&quot;nofollow&quot;&gt;Store interface&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/infogrid.org\/browser\/trunk\/ig-stores\/modules\/org.infogrid.store\/src\/org\/infogrid\/store\/Store.java&quot; rel=&quot;nofollow&quot;&gt;IterableStore interface&lt;\/a&gt; -- same, but iterable: some key-value stores can be iterated over easily, some can't&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2248697","ParentId":"2247036","CreationDate":"2010-02-11T23:58:18.863","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;Even if the NOSQL databases are very different from each other, they can be divided into meaningful groups, see &lt;a href=&quot;http:\/\/blogs.neotechnology.com\/emil\/2009\/11\/nosql-scaling-to-size-and-scaling-to-complexity.html&quot; rel=&quot;nofollow&quot;&gt;this blog post&lt;\/a&gt;. A new project aiming at defining abstractions on top of different NOSQL databases is Gremlin, see &lt;a href=&quot;http:\/\/www.infoq.com\/news\/2010\/01\/Gremlin&quot; rel=&quot;nofollow&quot;&gt;InfoQ: Gremlin, a Language for Working with Graphs&lt;\/a&gt;. Starting out from the graph database end of the NOSQL spectrum, the project has since moved on to document stores, creating an Object Document Model with implementations planned for MongoDB and CouchDB, see &lt;a href=&quot;http:\/\/github.com\/tinkerpop\/blueprints\/blob\/master\/src\/main\/java\/com\/tinkerpop\/blueprints\/odm\/Store.java&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt; and &lt;a href=&quot;http:\/\/groups.google.com\/group\/gremlin-users\/browse_thread\/thread\/d4ce1e9236d0f52f&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2249966","ParentId":"2229420","CreationDate":"2010-02-12T06:02:21.247","OwnerUserId":"190822","Tags":[],"Body":"&lt;p&gt;You should try Kickfire, which is a hardware optimised version of MySql and will breeze through your query.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.kickfire.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.kickfire.com\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;ps: I do NOT work for kickfire&lt;\/p&gt;\n"},{"Id":"2251556","ParentId":"2248789","CreationDate":"2010-02-12T11:55:33.970","OwnerUserId":"124378","Tags":[],"Body":"&lt;p&gt;Working with MongoDB in CodeIgniter wouldn't be much different than working with it anywhere else.&lt;\/p&gt;\n\n&lt;p&gt;You could knock together a MongoDB library that would connect in the constructor and store $this-&gt;conn to be used in methods later on.&lt;\/p&gt;\n\n&lt;p&gt;then either work directly with the conn property in your controllers or create a few methods in your MongoDB library to do this for you.&lt;\/p&gt;\n\n&lt;p&gt;Take a look &lt;a href=&quot;http:\/\/php.net\/manual\/en\/mongo.tutorial.php&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt; to see the plain PHP tutorial for working with MongoDB.&lt;\/p&gt;\n\n&lt;p&gt;I'd happily create you a library for this but it would come with a price. :-p&lt;\/p&gt;\n"},{"Id":"2259852","ParentId":"2259703","CreationDate":"2010-02-14T01:48:28.097","OwnerUserId":"140740","Tags":[],"Body":"&lt;p&gt;It probably doesn't matter much which Linux you install on a desktop. The user experience will be determined by Gnome or KDE, not the distro.&lt;\/p&gt;\n\n&lt;p&gt;The two aspects of linux that have the biggest user-experience impact are the desktop and the package system. Linux has, sadly, two of each.&lt;sup&gt;1.&lt;\/sup&gt;&lt;\/p&gt;\n\n&lt;p&gt;There are two desktops: Gnome and KDE. In general, you can choose Gnome or KDE with any distro and you can even install both. (You only run one at a time, though.)  Please realize that except for some configuration details, for the most part the distros redistribute the same set of Unix-model software, so you aren't getting anything wildly different or even as different as XP vs Vista. &lt;\/p&gt;\n\n&lt;p&gt;Either of the two main package systems can in some ways be used with any distro, but life will be much easier if you stay with the vanilla one for your distro. But since you aren't expecting either one I think it won't matter.&lt;\/p&gt;\n\n&lt;p&gt;Now, if you went and installed, say, &lt;a href=&quot;http:\/\/www.NetBSD.org\/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;NetBSD&lt;\/strong&gt;&lt;\/a&gt;, then you might notice some real differences, although you would &lt;em&gt;still&lt;\/em&gt; have your choice of Gnome or KDE.&lt;\/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;&lt;sup&gt;1. Technically, there are 10 or 20 window managers that provide interesting lightweight GUI's that are something a bit less than a full-blown desktop GUI, but that's in the advanced class. Also in that class: Unix servers generally run no GUI at all.&lt;\/sup&gt;&lt;\/p&gt;\n"},{"Id":"2261061","ParentId":"2259703","CreationDate":"2010-02-14T12:37:30.610","OwnerUserId":"217288","Tags":[],"Body":"&lt;p&gt;I would go with Ubuntu or OpenSuse since most of the tutorials, community support and other stuff around Mono is targeted to these distributions.&lt;\/p&gt;\n"},{"Id":"2261118","ParentId":"2259703","CreationDate":"2010-02-14T12:57:48.320","OwnerUserId":"85785","Tags":[],"Body":"&lt;p&gt;Note: you can still learn Ruby on windows, you can also use Ruby and .NET with &lt;a href=&quot;http:\/\/ironruby.net\/&quot; rel=&quot;nofollow&quot;&gt;IronRuby&lt;\/a&gt; which is nearly 100% compatible with C Ruby. That's not to stop you from learning linux though as it gives you a different perspective on OS's and will expose you to the power of the command line.&lt;\/p&gt;\n\n&lt;p&gt;Mono on linux is very complete. The best distribution to use with it would be Open Suse (as it's supported and recommended by Novell who develop Mono).&lt;\/p&gt;\n\n&lt;p&gt;If you want to go the NoSql route than I would recommend looking at &lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;redis&lt;\/a&gt; a very fast and advanced key-value data store with support for rich data structures, i.e. lists, sets and ordered sets. If you use C#\/Mono you can this &lt;a href=&quot;http:\/\/code.google.com\/p\/servicestack\/wiki\/ServiceStackRedis&quot; rel=&quot;nofollow&quot;&gt;redis client&lt;\/a&gt; which has native support for storing complex types and exposes Redis server-side lists and sets as &lt;code&gt;IList&amp;#38;lt;T&amp;#38;gt;&lt;\/code&gt; and &lt;code&gt;ICollection&amp;#38;lt;T&amp;#38;gt;&lt;\/code&gt;'s.&lt;\/p&gt;\n"},{"Id":"2264729","ParentId":"1886650","CreationDate":"2010-02-15T08:33:12.330","OwnerUserId":"6488","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;Redis&lt;\/a&gt; is worth giving a try as &lt;a href=&quot;https:\/\/github.com\/&quot; rel=&quot;nofollow&quot;&gt;Github&lt;\/a&gt; uses redis to manage a heavy queue of &lt;a href=&quot;http:\/\/github.com\/blog\/542-introducing-resque&quot; rel=&quot;nofollow&quot;&gt;background jobs&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2268101","ParentId":"2248789","CreationDate":"2010-02-15T18:32:17.377","OwnerUserId":"155862","Tags":[],"Body":"&lt;p&gt;I'm not sure if its the &quot;CodeIgniter way&quot; but I created a CodeIgniter library that extends the Mongo class with an extra property to store the current database connection.&lt;\/p&gt;\n\n&lt;p&gt;Here are the relevant code files from my project.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;config\/mongo.php&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$config['mongo_server'] = null;\n$config['mongo_dbname'] = 'mydb';\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;libraries\/Mongo.php&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class CI_Mongo extends Mongo\n{\n    var $db;\n\n    function CI_Mongo()\n    {   \n        \/\/ Fetch CodeIgniter instance\n        $ci = get_instance();\n        \/\/ Load Mongo configuration file\n        $ci-&amp;#38;gt;load-&amp;#38;gt;config('mongo');\n\n        \/\/ Fetch Mongo server and database configuration\n        $server = $ci-&amp;#38;gt;config-&amp;#38;gt;item('mongo_server');\n        $dbname = $ci-&amp;#38;gt;config-&amp;#38;gt;item('mongo_dbname');\n\n        \/\/ Initialise Mongo\n        if ($server)\n        {\n            parent::__construct($server);\n        }\n        else\n        {\n            parent::__construct();\n        }\n        $this-&amp;#38;gt;db = $this-&amp;#38;gt;$dbname;\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;And a sample controller&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;controllers\/posts.php&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class Posts extends Controller\n{\n    function Posts()\n    {\n        parent::Controller();\n    }\n\n    function index()\n    {\n        $posts = $this-&amp;#38;gt;mongo-&amp;#38;gt;db-&amp;#38;gt;posts-&amp;#38;gt;find();\n\n        foreach ($posts as $id =&amp;#38;gt; $post)\n        {\n            var_dump($id);\n            var_dump($post);\n        }\n    }\n\n    function create()\n    {\n        $post = array('title' =&amp;#38;gt; 'Test post');\n        $this-&amp;#38;gt;mongo-&amp;#38;gt;db-&amp;#38;gt;posts-&amp;#38;gt;insert($post);\n        var_dump($post);\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2278219","ParentId":"2278186","CreationDate":"2010-02-17T04:02:37.373","OwnerUserId":"241204","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;Mongo DB&lt;\/a&gt; should work well for you.  I haven't used it for blobs yet, but here is a nice FLOSS Weekly &lt;a href=&quot;http:\/\/twit.tv\/floss105&quot; rel=&quot;nofollow&quot;&gt;podcast interview with Michael Dirolf&lt;\/a&gt; from the Mongo DB team where he addresses this use case.&lt;\/p&gt;\n"},{"Id":"2278229","ParentId":"2278186","CreationDate":"2010-02-17T04:04:41.447","OwnerUserId":"201225","Tags":[],"Body":"&lt;p&gt;Whether or not to store images in a DB or the filesystem is sometime one of those &quot;holy war&quot; type of debates; each side feels their way of doing things is the one right way. In general:&lt;\/p&gt;\n\n&lt;p&gt;To store in the DB:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Easier to manage back-up\/replicate everything at once in once place.&lt;\/li&gt;\n&lt;li&gt;Helps with your data consistency and integrity.  You can set the BLOB field to disallow NULLs, but you're not going to be able to prevent an external file from being deleted. (Though this isn't applicable to NoSQL since there aren't the traditional constraints).&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;To store on the filesystem:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A filesystem is designed to serve files.  Let it do it's job.&lt;\/li&gt;\n&lt;li&gt;The DB is often your bottleneck in an application.  Whatever load you can take off it, the better.&lt;\/li&gt;\n&lt;li&gt;Easier to serve on a CDN (which you mentioned isn't applicable in your situation).&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;I tend to come down on the side of the filesystem because it scales much better.  But depending on the size of your project, either choice will likely work fine.  With NoSQL, the differences are even less apparent.&lt;\/p&gt;\n"},{"Id":"2278263","ParentId":"2278186","CreationDate":"2010-02-17T04:16:34.607","OwnerUserId":"210754","Tags":[],"Body":"&lt;p&gt;Well CDN would be the obvious choice.  Since that's out, I'd say your best bet for fault tolerance and load balancing would be your own private data center (whatever that means to you) behind 2 or more load balancers like an F5.  This will be your easiest management system and you can get as much fault tolerance as your hardware budget allows.  You won't need any new software expertise, just XCOPY.&lt;\/p&gt;\n\n&lt;p&gt;For true fault tolerance you're going to need geographic dispersion or you're subject to anyone with a backhoe.&lt;\/p&gt;\n\n&lt;p&gt;(Gravatars?)&lt;\/p&gt;\n"},{"Id":"2287361","ParentId":"2285045","CreationDate":"2010-02-18T09:02:38.290","OwnerUserId":"124894","Tags":[],"Body":"&lt;p&gt;I don't. I would like to use a simple and free key-value store that I can call in process but such thing doesn't exist afaik on the Windows platform. Now I use Sqlite but I would like to use something like Tokyo Cabinet. BerkeleyDB has license &quot;issues&quot;. &lt;\/p&gt;\n\n&lt;p&gt;However if you want to use the Windows OS your choice of NoSQL databases is limited. And there isn't always a C# provider &lt;\/p&gt;\n\n&lt;p&gt;I did try MongoDB and it was 40 times faster than Sqlite, so maybe I should use it. But I still hope for a simple in process solution. &lt;\/p&gt;\n"},{"Id":"2289084","ParentId":"2285045","CreationDate":"2010-02-18T14:01:31.133","OwnerUserId":"173227","Tags":[],"Body":"&lt;p&gt;I have no first-hand experiences., but I found &lt;a href=&quot;http:\/\/blog.boxedice.com\/2009\/07\/25\/choosing-a-non-relational-database-why-we-migrated-from-mysql-to-mongodb\/&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt; blog entry quite interesting.&lt;\/p&gt;\n"},{"Id":"2291548","ParentId":"2291433","CreationDate":"2010-02-18T19:25:21.920","OwnerUserId":"27535","Tags":[],"Body":"&lt;p&gt;An RDBMS is simply a good all rounder that has lasted for 30 years and shows no sign of flagging.\nThings like NoSQL etc are &quot;special case&quot; for certain uses.&lt;\/p&gt;\n\n&lt;p&gt;I'd use a document database when I have a document library or similar. Everything else is an RDBMS.&lt;\/p&gt;\n\n&lt;p&gt;Data aside, if you want to sell this system then you may have to target an RDBMS like SQL Server or Oracle to ensure it's supportable on your customer's infrastructure&lt;\/p&gt;\n"},{"Id":"2292619","ParentId":"2285045","CreationDate":"2010-02-18T22:12:22.783","OwnerUserId":"192001","Tags":[],"Body":"&lt;p&gt;I used redis to store logging messages across machines. It was very easy to implement, and very useful. Redis really rocks&lt;\/p&gt;\n"},{"Id":"2293862","ParentId":"2291433","CreationDate":"2010-02-19T03:22:47.900","OwnerUserId":"39094","Tags":[],"Body":"&lt;p&gt;If your data is structured the an RMDBS seems the obvious choice, if it's an unstructured data - such as a document, then a DocDBMS sounds best.&lt;\/p&gt;\n\n&lt;p&gt;RMDBS's are more of a 'backend' tool, you'd use it to provide the backend of a system you were developing.&lt;\/p&gt;\n\n&lt;p&gt;When you say DocDBMS I'm thinking you mean more of a document management system (?) - which is more of an entire solution which would include (document) data management functionality aimed at end users.&lt;\/p&gt;\n\n&lt;p&gt;To me, NoSQL is simply a variant on an RMDBS - but for more sprecific \/ niche needs.&lt;\/p&gt;\n\n&lt;p&gt;As to how to choose: draw up a list of NFR's which are relevant and do some simple analysis of the options and how they relate; scalability and performance come to mind, what about data volumes and transaction rates?  DR?  And of course important functional needs.  Are you more worried about execution qualities of whats built or longer-term evolution qualities?&lt;\/p&gt;\n"},{"Id":"2294676","ParentId":"2293674","CreationDate":"2010-02-19T07:22:35.873","OwnerUserId":"55562","Tags":[],"Body":"&lt;p&gt;Pre 1.0, django ORM underwent a major queryset re-factor. One of the reasons for this was &quot;This re-factor enables us to support non relational backends&quot;.&lt;\/p&gt;\n\n&lt;p&gt;The official support I think is definitely on the cards; but I think there were more pressing matters for 1.1 and 1.2(now in beta).&lt;\/p&gt;\n\n&lt;p&gt;However, there are of course several independent efforts to use non relational databases with django, including, but not limited to the following:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/www.allbuttonspressed.com\/projects\/django-nonrel&quot; rel=&quot;nofollow&quot;&gt;Django-nonrel&lt;\/a&gt; by Waldemar, who made django work on the appengine using the appengine patch.&lt;\/li&gt;\n&lt;li&gt;Using django with mongo db, by Kevin Fricovsky: &lt;a href=&quot;http:\/\/bitbucket.org\/gumptioncom\/django-non-relational\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/bitbucket.org\/gumptioncom\/django-non-relational\/&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;Using django with couch db, an old post, by Eric: &lt;a href=&quot;http:\/\/www.eflorenzano.com\/blog\/post\/using-couchdb-django\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.eflorenzano.com\/blog\/post\/using-couchdb-django\/&lt;\/a&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2297244","ParentId":"2291442","CreationDate":"2010-02-19T15:05:21.160","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;How much ram you needs really depends on your workload: if you are write-mostly you can get away with less, otherwise you will want ram for the read cache.&lt;\/p&gt;\n\n&lt;p&gt;You do get more ram for you money at my employer, rackspace cloud:  &lt;a href=&quot;http:\/\/www.rackspacecloud.com\/cloud_hosting_products\/servers\/pricing&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.rackspacecloud.com\/cloud_hosting_products\/servers\/pricing&lt;\/a&gt;.  (our machines also have raided disks so people typically see better i\/o performance vs EC2.  Dunno about linode.)&lt;\/p&gt;\n\n&lt;p&gt;Since with most VPSes you pay roughly 2x for the next-size instance, i.e., about the same as adding a second small instance, I would recommend going with fewer, larger instances than more, smaller ones, since in small numbers network overhead is not negligible.&lt;\/p&gt;\n\n&lt;p&gt;I do know someone using Cassandra on 256MB VMs but you're definitely in the minority if you go that small.&lt;\/p&gt;\n"},{"Id":"2313672","ParentId":"2285045","CreationDate":"2010-02-22T19:51:43.043","OwnerUserId":"92937","Tags":[],"Body":"&lt;p&gt;I apologize for going against your bold text, since I don't have any first-hand experience, but this set of blog posts is a good example of solving a problem with CouchDB.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/johnpwood.net\/2009\/06\/15\/couchdb-a-case-study\/&quot; rel=&quot;nofollow&quot;&gt;CouchDB: A Case Study&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;Essentially, the &lt;a href=&quot;http:\/\/textme.net\/&quot; rel=&quot;nofollow&quot;&gt;textme&lt;\/a&gt; application used CouchDB to deal with their exploding data problem. They found that SQL was too slow to deal with large amounts of archival data, and moved it over to CouchDB. It's an excellent read, and he discusses the entire process of figuring out what problems CouchDB could solve and how they ended up solving them.&lt;\/p&gt;\n"},{"Id":"2315450","ParentId":"2081080","CreationDate":"2010-02-23T00:59:02.373","OwnerUserId":"260555","Tags":[],"Body":"&lt;p&gt;Windows has a built-in embedded non-relational store. It is called ESENT and is used by several Windows applications, including the Active Directory and Windows Desktop Search.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/blogs.msdn.com\/windowssdk\/archive\/2008\/10\/23\/esent-extensible-storage-engine-api-in-the-windows-sdk.aspx&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blogs.msdn.com\/windowssdk\/archive\/2008\/10\/23\/esent-extensible-storage-engine-api-in-the-windows-sdk.aspx&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;If you want .NET access you can use the ManagedEsent layer on CodePlex.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/managedesent.codeplex.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/managedesent.codeplex.com\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;That project has a PersistentDictionary class that implements a key-value store that implements the IDictionary interface, but is backed by a database.&lt;\/p&gt;\n"},{"Id":"2315815","ParentId":"2081080","CreationDate":"2010-02-23T02:45:59.390","OwnerUserId":"38207","Tags":[],"Body":"&lt;p&gt;Applying the KISS principle to your problem I would recommend you use files.&lt;\/p&gt;\n\n&lt;p&gt;As in filename is the key.\nFile contents is the value.\nWindows folder is the index.&lt;\/p&gt;\n\n&lt;p&gt;Simple, quick, efficient, flexible, and foolproof (providing the fools have low intelligence).&lt;\/p&gt;\n"},{"Id":"2316921","ParentId":"2285045","CreationDate":"2010-02-23T09:16:33.997","OwnerUserId":"277084","Tags":[],"Body":"&lt;p&gt;My current project actually.&lt;\/p&gt;\n\n&lt;p&gt;Storing 18,000 objects in a normalised structure: 90,000 rows across 8 different tables. Took 1 minute to retrieve and map them to our Java object model, that's with everything correctly indexed etc.&lt;\/p&gt;\n\n&lt;p&gt;Storing them as key\/value pairs using a lightweight text representation: 1 table, 18,000 rows, 3 seconds to retrieve them all and reconstruct the Java objects.&lt;\/p&gt;\n\n&lt;p&gt;In business terms: first option was not feasible. Second option means our app works.&lt;\/p&gt;\n\n&lt;p&gt;Technology details: running on MySQL for both SQL and NoSQL!  Sticking with MySQL for good transaction support, performance, and proven track record for not corrupting data, scaling fairly well, support for clustering etc.  &lt;\/p&gt;\n\n&lt;p&gt;Our data model in MySQL is now just key fields (integers) and the big &quot;value&quot; field: just a big TEXT field basically.&lt;\/p&gt;\n\n&lt;p&gt;We did not go with any of the new players (CouchDB, Cassandra, MongoDB, etc) because although they each offer great features\/performance in their own right, there were always drawbacks for our circumstances (missing\/immature Java support).&lt;\/p&gt;\n\n&lt;p&gt;Extra benefit of (ab)using MySQL - the bits of our model that &lt;em&gt;do&lt;\/em&gt; work relationally can be easily linked to our key\/value store data.&lt;\/p&gt;\n\n&lt;p&gt;Update: here's an example of how we represented text content, not our actual business domain (we don't work with &quot;products&quot;) as my boss'd shoot me, but conveys the idea, including the recursive aspect (one entity, here a product, &quot;containing&quot; others). Hopefully it's clear how in a normalised structure this could be quite a few tables, e.g. joining a product to its range of flavours, which other products are contained, etc&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Name=An Example Product\nType=CategoryAProduct\nColour=Blue\nSize=Large\nFlavours={nice,lovely,unpleasant,foul}\nContains=[\nName=Product2\nType=CategoryBProduct\nSize=medium\nFlavours={yuck}\n------\nName=Product3\nType=CategoryCProduct\nSize=Small\nFlavours={sublime}\n]\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2317006","ParentId":"2212230","CreationDate":"2010-02-23T09:32:15.820","OwnerUserId":"277084","Tags":[],"Body":"&lt;p&gt;You can always use a NoSQL approach in a SQL DB.  NoSQL seems to generally use &quot;key\/value data stores&quot;: you can always implement this in your preferred RDBMS and hence keep the good stuff like transactions, ACID properties, support from your friendly DBA, etc, while realising the NoSQL performance and flexibility benefits, e.g. via a table such as&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE MY_KEY_VALUE_DATA\n(\n    id_content INTEGER PRIMARY KEY,\n    b_content  BLOB\n);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Bonus is you can add extra fields here to link your content into other, properly relational tables, while still keeping your bulky content in the main BLOB (or TEXT if apt) field. &lt;\/p&gt;\n\n&lt;p&gt;Personally I favour a TEXT representation so you're not tied into a language for working with the data, e.g. using serialized Java means you can access the content from Perl for reporting, say. TEXT is also easier to debug and generally work with as a developer.&lt;\/p&gt;\n"},{"Id":"2322677","ParentId":"2285045","CreationDate":"2010-02-23T23:52:04.633","OwnerUserId":"45935","Tags":[],"Body":"&lt;p&gt;Todd Hoff's &lt;a href=&quot;http:\/\/highscalability.com&quot; rel=&quot;nofollow&quot;&gt;highscalability.com&lt;\/a&gt; has a lot of great coverage of NoSQL, including some case studies.  &lt;\/p&gt;\n\n&lt;p&gt;The commercial &lt;a href=&quot;http:\/\/www.vertica.com&quot; rel=&quot;nofollow&quot;&gt;Vertica&lt;\/a&gt; columnar DBMS might suit your purposes (even though it supports SQL): it's very fast compared with traditional relational DBMSs for analytics queries.  See Stonebraker, et al.'s &lt;a href=&quot;http:\/\/database.cs.brown.edu\/papers\/stonebraker-cacm2010.pdf&quot; rel=&quot;nofollow&quot;&gt;recent CACM paper&lt;\/a&gt; contrasting Vertica with map-reduce.&lt;\/p&gt;\n\n&lt;p&gt;Update: And &lt;a href=&quot;http:\/\/developers.slashdot.org\/story\/10\/02\/23\/1826226\/How-Twitter-Is-Moving-To-the-Cassandra-Database&quot; rel=&quot;nofollow&quot;&gt;Twitter's selected Cassandra&lt;\/a&gt; over several others, including HBase, Voldemort, MongoDB, MemcacheDB, Redis, and HyperTable.&lt;\/p&gt;\n\n&lt;p&gt;Update 2: Rick Cattell has just published a comparison of several NoSQL systems in &lt;a href=&quot;http:\/\/cattell.net\/datastores\/Datastores.pdf&quot; rel=&quot;nofollow&quot;&gt;High Performance Data Stores&lt;\/a&gt;.  And highscalability.com's take on Rick's paper is &lt;a href=&quot;http:\/\/highscalability.com\/blog\/2010\/2\/25\/paper-high-performance-scalable-data-stores.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2328207","ParentId":"2328169","CreationDate":"2010-02-24T17:48:39.023","OwnerUserId":"126769","Tags":[],"Body":"&lt;p&gt;Take a look at &lt;a href=&quot;http:\/\/www.dnrtv.com\/default.aspx?showNum=163&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt; video from DNR TV, doing som hands on with &lt;a href=&quot;http:\/\/www.mongodb.org\/&quot; rel=&quot;nofollow&quot;&gt;mongodb&lt;\/a&gt;. Might be nice for a first introduction.&lt;\/p&gt;\n"},{"Id":"2328212","ParentId":"2328169","CreationDate":"2010-02-24T17:49:56.887","OwnerUserId":"103739","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/www.slideshare.net\/drumwurzel\/intro-to-mongodb&quot; rel=&quot;nofollow&quot;&gt;Here is a decent slide show&lt;\/a&gt; introducing MongoDB. I think some of the big differences is that most of the systems rely on Active Record or some similar database abstraction.&lt;\/p&gt;\n\n&lt;p&gt;Also I found a wonderful &lt;a href=&quot;http:\/\/books.couchdb.org\/relax\/&quot; rel=&quot;nofollow&quot;&gt;free orlys book on Couch DB here&lt;\/a&gt;, which is pretty awesome. &lt;\/p&gt;\n"},{"Id":"2328251","ParentId":"2328169","CreationDate":"2010-02-24T17:57:56.403","OwnerUserId":"209878","Tags":[],"Body":"&lt;p&gt;At its most basic form NoSQL is really no more than a way of storing objects using some sort of key\/value pairing system. You use this all the time already I assume. For instance. in javascript you can create an object named foo and then do foo['myobj'] = myobj to store stuff in the object.&lt;\/p&gt;\n\n&lt;p&gt;All NoSQL servers really do is give you a way to add\/delete\/query massive arrays and still allow for persistence and fault tolerance. You can create a NoSQL in memory server in about 100 lines of code. &lt;\/p&gt;\n\n&lt;p&gt;So let's do it this way...in CouchDB you use map\/reduce...so let's create a map function do to the same as a bit of SQL code:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM users WHERE age &amp;#38;gt; 10\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;In CouchDB you provide the server with a Javascript function that gets run against every item in the database...&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function (doc)\n{\n    if (doc.objType == &quot;users&quot;) {\n       if (doc.age &amp;#38;gt; 10) {\n           emit(doc._id, null)\n       }\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;That's all there really is to it.....it gets way more complex from there on the server end, as the server has to handle crashes, and multiple revisions of the same object, but this is just an example. &lt;\/p&gt;\n"},{"Id":"2328263","ParentId":"2285045","CreationDate":"2010-02-24T17:59:03.747","OwnerUserId":"82219","Tags":[],"Body":"&lt;p&gt;I've switched a small subproject from MySQL to couchdb, to be able to handle the load. The result was amazing.&lt;\/p&gt;\n\n&lt;p&gt;About 2 years ago, we've released a self written software on &lt;a href=&quot;http:\/\/www.ubuntuusers.de\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.ubuntuusers.de\/&lt;\/a&gt; (which is probably the biggest german linux community website). The site is written in Python and we've added a WSGI middleware which was able to catch all exceptions and send them to another small mysql powered website. This small website used a hash to determine different bugs and stored the number of occurrences and the last occurrence as well.&lt;\/p&gt;\n\n&lt;p&gt;Unfortunately, shortly after the release, the traceback-logger website wasn't responding anymore. We had some locking issues with the production db of our main site which was throwing exceptions nearly every request, as well as several other bugs, which we haven't explored during the testing stage. The server cluster of our main site, called the traceback-logger submit page several k times per second. And that was a way too much for the small server which hosted the traceback logger (it was already an old server, which was only used for development purposes).&lt;\/p&gt;\n\n&lt;p&gt;At this time couchdb was rather popular, and so I decided to try it out and write a small traceback-logger with it. The new logger only consisted of a single python file, which provided a bug list with sorting and filter options and a submit page. And in the background I've started a couchdb process. The new software responded extremely quickly to all requests and we were able to view the massive amount of automatic bug reports.&lt;\/p&gt;\n\n&lt;p&gt;One interesting thing is, that the solution before, was running on an old dedicated server, where the new couchdb based site on the other hand was only running on a shared xen instance with very limited resources. And I haven't even used the strength of key-values stores to scale horizontally. The ability of couchdb \/ Erlang OTP to handle concurrent requests without locking anything was already enough to serve the needs.&lt;\/p&gt;\n\n&lt;p&gt;Now, the quickly written couchdb-traceback logger is still running and is a helpful way to explore bugs on the main website. Anyway, about once a month the database becomes too big and the couchdb process gets killed. But then, the compact-db command of couchdb reduces the size from several GBs to some KBs again and the database is up and running again (maybe i should consider adding a cronjob there... 0o).&lt;\/p&gt;\n\n&lt;p&gt;In a summary, couchdb was surely the best choice (or at least a better choice than mysql) for this subproject and it does its job well.&lt;\/p&gt;\n"},{"Id":"2328462","ParentId":"2285045","CreationDate":"2010-02-24T18:28:40.397","OwnerUserId":"218059","Tags":[],"Body":"&lt;p&gt;We replaced a postgres database with a CouchDB document database because not having a fixed schema was a strong advantage to us.  Each document has a variable number of indexes used to access that document.&lt;\/p&gt;\n"},{"Id":"2329280","ParentId":"2285045","CreationDate":"2010-02-24T20:33:40.003","OwnerUserId":"84760","Tags":[],"Body":"&lt;p&gt;We moved part of our data from mysql to mongodb, not so much for scalability but more because it is a better fit for files and non-tabular data.&lt;\/p&gt;\n\n&lt;p&gt;In production we currently store:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;25 thousand files (60GB)&lt;\/li&gt;\n&lt;li&gt;130 million other &quot;documents&quot; (350GB)&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;with a daily turnover of around 10GB.&lt;\/p&gt;\n\n&lt;p&gt;The database is deployed in a &quot;paired&quot; configuration on two nodes (6x450GB sas raid10) with apache\/wsgi\/python clients using the mongodb python api (pymongo). The disk setup is probably overkill but thats what we use for mysql.&lt;\/p&gt;\n\n&lt;p&gt;Apart from some issues with pymongo threadpools and the blocking nature of the mongodb server it has been a good experience.&lt;\/p&gt;\n"},{"Id":"2334718","ParentId":"2332113","CreationDate":"2010-02-25T14:37:30.350","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Cassandra and the other distributed databases available today do not provide the kind of ad-hoc query support you are used to from sql.  This is because you can't distribute queries with joins performantly, so the emphasis is on denormalization instead.&lt;\/p&gt;\n\n&lt;p&gt;However, Cassandra 0.6 (beta officially out tomorrow, but you can build from the 0.6 branch yourself if you're impatient) supports Hadoop map\/reduce for analytics, which actually sounds like a good fit for you.&lt;\/p&gt;\n\n&lt;p&gt;Cassandra provides excellent support for adding new nodes painlessly, even to an initial group of one.&lt;\/p&gt;\n\n&lt;p&gt;That said, at a few hundred writes\/minute you're going to be fine on mysql for a long, long time.  Cassandra is much better at being a key\/value store (even better, key\/columnfamily) but MySQL is much better at being a relational database. :)&lt;\/p&gt;\n\n&lt;p&gt;There is no django support for Cassandra (or other nosql database) yet.  They are talking about doing something for the next version after 1.2, but based on talking to django devs at pycon, nobody is really sure what that will look like yet.&lt;\/p&gt;\n"},{"Id":"2335045","ParentId":"2330612","CreationDate":"2010-02-25T15:13:06.580","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/ericflo\/twissandra&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ericflo\/twissandra&lt;\/a&gt; is a twitter clone in the same vein as retwis, originally for 0.3 and now being updated for 0.6.  Catch erifclo in #cassandra on irc if you have questions.&lt;\/p&gt;\n"},{"Id":"2335086","ParentId":"2330562","CreationDate":"2010-02-25T15:18:19.113","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;If you need availability on a RF=2, clustersize=2 system, then you can't use ALL or you will not be able to write when a node goes down.&lt;\/p&gt;\n\n&lt;p&gt;That is why people recommend 3 nodes instead of 2, because then you can do quorum reads+writes and still have both strong consistency and availability if a single node goes down.&lt;\/p&gt;\n\n&lt;p&gt;With just 2 nodes you get to choose whether you want strong consistency (write with ALL) or availability in the face of a single node failure (write with ONE) but not both.  Of course if you write with ONE cassandra will do hinted handoff etc as needed to make it eventually consistent.&lt;\/p&gt;\n"},{"Id":"2338137","ParentId":"2337819","CreationDate":"2010-02-25T22:25:54.953","OwnerUserId":"95810","Tags":[],"Body":"&lt;p&gt;Python defines several special methods such as &lt;a href=&quot;http:\/\/docs.python.org\/library\/pickle.html?highlight=__getstate__#object.__getstate__&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;getstate&lt;\/strong&gt;&lt;\/a&gt; and many others to allow your classes to define exactly how best to serialize and de-serialize their instances.  They're all used internally by the &lt;code&gt;pickle&lt;\/code&gt; module (which then uses this information to produce a &quot;blob&quot;, i.e. a string of bytes, and restore objects from such blobs), but, if you want better indexing obtained by storing graphs directly rather than via opaque blobs, it's basically a question of tweaking the &lt;code&gt;pickle&lt;\/code&gt; procedures to stop just before turning the graphs into blobs.  I think you'll have to do it by copy-paste-edit of &lt;code&gt;pickle.py&lt;\/code&gt; (as it's not designed to be customized in this way by more elegant methods such as subclassing), but that should still save you lots of work wrt redoing it all from scratch.&lt;\/p&gt;\n\n&lt;p&gt;I believe this approach lies somewhere between your options 1 and 2 -- classes need to define such special methods only in response to specific needs, and most of the work needed to orchestrate the various possibility will be handled by your pickle-variant (much as it's handled by pickle itself for the &quot;normal&quot; case where the serialized form is a blob).&lt;\/p&gt;\n"},{"Id":"2345771","ParentId":"2167481","CreationDate":"2010-02-27T00:48:30.283","OwnerUserId":"260555","Tags":[],"Body":"&lt;p&gt;I've seen one here:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.woanware.co.uk\/news\/esedbviewer-v1-0-0\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.woanware.co.uk\/news\/esedbviewer-v1-0-0\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2350820","ParentId":"2285045","CreationDate":"2010-02-28T10:55:09.283","OwnerUserId":"85785","Tags":[],"Body":"&lt;p&gt;We've moved some of our data we used to store in Postgresql and Memcached into &lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;Redis&lt;\/a&gt;. Key value stores are much better suited for storing hierarchical object data. You can store blob data much faster and with much less development time and effort than using an ORM to map your blob to a RDBMS.&lt;\/p&gt;\n\n&lt;p&gt;I have an &lt;a href=&quot;http:\/\/code.google.com\/p\/servicestack\/wiki\/ServiceStackRedis&quot; rel=&quot;nofollow&quot;&gt;open source c# redis client&lt;\/a&gt; that lets you store and retrieve any POCO objects with 1 line:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var customers = redis.Lists[&quot;customers&quot;]; \/\/Implements IList&amp;#38;lt;Customer&amp;#38;gt;\ncustomers.Add(new Customer { Name = &quot;Mr Customer&quot; });\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Key value stores are also much easier to 'scale-out' as you can add a new server and then partition your load evenly to include the new server. Importantly, there is no central server that will limit your scalability. (though you will still need a strategy for consistent hashing to distribute your requests).&lt;\/p&gt;\n\n&lt;p&gt;I consider Redis to be a 'managed text file' on steroids that provides fast, concurrent and atomic access for multiple clients, so anything I used to use a text file or embedded database for I now use Redis. e.g. To get a real-time combined rolling error log for all our services (which has notoriously been a hard task for us), is now accomplished with only a couple of lines by just pre-pending the error to a Redis server side list and then trimming the list so only the last 1000 are kept, e.g:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var errors = redis.List[&quot;combined:errors&quot;];\nerrors.Insert(0, new Error { Name = ex.GetType().Name, Message = ex.Message, StackTrace = ex.StackTrace});\nredis.TrimList(errors, 1000);\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2351062","ParentId":"2351040","CreationDate":"2010-02-28T12:40:12.930","OwnerUserId":"217862","Tags":[],"Body":"&lt;p&gt;I'not a NoSQL expert, but as the name says, they don't rely necessary on SQL. You can probably do whatever you want, but will need to code map\/reduce function or other non-SQL way to query the data. &lt;\/p&gt;\n\n&lt;p&gt;Maybe this blog provide useful information to you: &lt;a href=&quot;http:\/\/horicky.blogspot.com\/2009\/11\/query-processing-for-nosql-db.html&quot; rel=&quot;nofollow&quot;&gt;Query processing for NoSQL database&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2351602","ParentId":"2351040","CreationDate":"2010-02-28T15:57:48.953","OwnerUserId":"126769","Tags":[],"Body":"&lt;p&gt;In &lt;a href=&quot;http:\/\/www.mongodb.org\/&quot; rel=&quot;nofollow&quot;&gt;mongodb&lt;\/a&gt;, you would just do something like &lt;code&gt;db.mytbl.find({&quot;vara&quot;: { $gt: 10}, &quot;varb&quot;:  2, &quot;varc&quot;: {$lt: 100 }})&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;See &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Advanced+Queries&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;, and &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Queries+and+Cursors&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt; for examples&lt;\/p&gt;\n"},{"Id":"2354279","ParentId":"2354254","CreationDate":"2010-03-01T06:52:49.750","OwnerUserId":"179328","Tags":[],"Body":"&lt;p&gt;The advantage of a relational database is the ability to relate and index information. Most key-value systems don't provide that.&lt;\/p&gt;\n\n&lt;p&gt;What you need to ask yourself is, does switching make sense for my intended use case?&lt;\/p&gt;\n\n&lt;p&gt;You have kind of missed the point. The point is, you don't have an index. You don't have a centralized list of records, or the ability to relate it together in any easy way. What makes nosql key-value stores so quick is that you store and retrieve what you need in a name-based approach. You need that blurb on someone's profile page? Just go fetch it. No need to maintain a table with everything in it. &lt;\/p&gt;\n\n&lt;p&gt;Not everything really needs to be tabular.&lt;\/p&gt;\n\n&lt;p&gt;There's advantages and disadvantages. Personally, I use a mix of both. SQL for most, and something along the lines of CouchDB for random things that have no need to be clogging up an SQL table.&lt;\/p&gt;\n\n&lt;p&gt;You can liken a key-value system to making an SQL table with two columns, a unique key and a value. This is quite fast. You have no need to do any relations or correlations or collation of data. Just find the value and return it.&lt;\/p&gt;\n\n&lt;p&gt;You'll find this is also fast in SQL databases. I've used it in place of actual key-value systems.&lt;\/p&gt;\n\n&lt;p&gt;I do not think scientific data is well suited to a nosql implementation.&lt;\/p&gt;\n"},{"Id":"2354332","ParentId":"2354254","CreationDate":"2010-03-01T07:13:31.813","OwnerUserId":"282912","Tags":[],"Body":"&lt;p&gt;The efficiency comes from three main areas:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The database has far fewer functions: there is no concept of a join and lessened or absent transactional integrity requirements. Less function means less work means faster, on the server side at least.&lt;\/li&gt;\n&lt;li&gt;Another design principle is that the data store lives in a cloud of servers so your request may have multiple respondents. These systems also claim the multi-server system improves fault tolerance through replication.&lt;\/li&gt;\n&lt;li&gt;It is fully buzzword compliant, using a bunch of ideas and descriptions that are not wholly invented yet. For example, Amazon is currently giving their services away in order to better understand how people might use them and get some experience to refine the specification.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;To my eye, someone coming to you with a requirement that &quot;our new data will be too much for our RDBMS&quot; ought either have numbers to back that assertion up or admit they just want to try the new shiny. Is noSQL meritless? Probably not. Is it going to turn the world upside-down as Java 1.0 was hyped to? Probably not.&lt;\/p&gt;\n\n&lt;p&gt;There's no harm in investigating new things, just don't bet the farm on them in favor of 50 year old, well-established, well-understood technology.&lt;\/p&gt;\n"},{"Id":"2357107","ParentId":"2357087","CreationDate":"2010-03-01T16:04:26.017","OwnerUserId":"47773","Tags":[],"Body":"&lt;p&gt;Why not start with Amazon's &lt;a href=&quot;http:\/\/aws.amazon.com\/solutions\/case-studies\/\/&quot; rel=&quot;nofollow&quot;&gt;case studies&lt;\/a&gt;?&lt;\/p&gt;\n"},{"Id":"2357402","ParentId":"2357239","CreationDate":"2010-03-01T16:45:29.210","OwnerUserId":"120163","Tags":[],"Body":"&lt;p&gt;IBM Mainframes have had &quot;non-relational&quot; databases since the 60s (hierarchial databases such as IMS + variants).  These databases are still in use because they are extremely fast and handle huge scale well.&lt;\/p&gt;\n\n&lt;p&gt;The point of relational databases was to provide a regular, relatively abstract method for storing and retrieving data in which the tuning can be done relatively independently of the data model (not true for IMS).   They were designed rather in reaction to the inability to reorganize hiearchical databases easily. The upside is nice organization; the downside is medium, not high performance.&lt;\/p&gt;\n\n&lt;p&gt;Google provides scalable storage and MapReduce to handle scale.  It isn't relational.&lt;\/p&gt;\n\n&lt;p&gt;There was a huge push early in the last decade to store data in XML, in essentially hiearchical form because XML is implicitly hierarchical.  That was a huge mistake IMHO, because it repeated the inconvenience of heirarchical databases, but had none of the performance.  I'm not very surprised this movement seems to have pretty much died.&lt;\/p&gt;\n\n&lt;p&gt;Most of the practical push to non-relational seems to me to be towards performance and scale.  I don't see how this helps &quot;small&quot; applications much.&lt;\/p&gt;\n\n&lt;p&gt;People have proposed, but not done a lot of practical data management using knowledge-based schemes.   Doug Lenat's &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Cyc&quot; rel=&quot;nofollow&quot; title=&quot;Wikipedia entry&quot;&gt;CYC&lt;\/a&gt; comes to mind here.  The ability of the database\nto help an application draw non-obvious conclusions strikes me a very interesting for &quot;small&quot; applications that are trying be &quot;smart&quot;.  But there aren't a lot of these yet. &lt;\/p&gt;\n"},{"Id":"2357700","ParentId":"2357239","CreationDate":"2010-03-01T17:28:14.610","OwnerUserId":"3211","Tags":[],"Body":"&lt;p&gt;The sweet spot of using a NoSQL database at that scale is when the database model (key-value, document, etc.) is a good match to the application's needs and the advanced relational functionality is not needed. &lt;\/p&gt;\n\n&lt;p&gt;At the small end of the spectrum, performance is a non issue because just about everything is fast. Storage engines are a non issue, if you don't need a sophisticated query engine, the lack of SQL support is a non issue. &lt;\/p&gt;\n\n&lt;p&gt;You are left with how well it fits and how easy it is to use. Honestly though, tooling does become an issue. Relational database tooling is mature, NoSQL tooling is less feature rich and less battle hardened. Too often it is roll-your-own tooling. Definitely consider what tools you'd be giving up and how much you need them.&lt;\/p&gt;\n\n&lt;p&gt;There is an additional slate of advantages for smaller projects when considering a NoSQL service (like Amazon SimpleDB and Microsoft Azure) as compared to a product. If you only have to pay for what you use and you don't use much, it can be cheaper than running a dedicated server, going all the way down to free for something like the SimpleDB free usage tier.&lt;\/p&gt;\n\n&lt;p&gt;You also avoid some of the server and database maintenance costs. This can be a big win if you don't have a DBA, or when your DBAs are already over worked. Of course you'll still have admin work to do, but it is significantly reduced, and typically simpler.&lt;\/p&gt;\n"},{"Id":"2358230","ParentId":"2357919","CreationDate":"2010-03-01T18:51:43.420","OwnerUserId":"163203","Tags":[],"Body":"&lt;p&gt;Take look at these projects:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cassandra usage &lt;a href=&quot;http:\/\/about.digg.com\/blog\/looking-future-cassandra&quot; rel=&quot;nofollow&quot;&gt;at Digg&lt;\/a&gt;.   &lt;\/li&gt;\n&lt;li&gt;Cassandra usage &lt;a href=&quot;http:\/\/nosql.mypopescu.com\/post\/407159447\/cassandra-twitter-an-interview-with-ryan-king&quot; rel=&quot;nofollow&quot;&gt;at Twitter&lt;\/a&gt;.&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/github.com\/jamesgolick\/friendly&quot; rel=&quot;nofollow&quot;&gt;Friendly&lt;\/a&gt; usage &lt;a href=&quot;http:\/\/jamesgolick.com\/2009\/12\/16\/introducing-friendly-nosql-with-mysql-in-ruby.html&quot; rel=&quot;nofollow&quot;&gt;at FetLife&lt;\/a&gt;(nsfw).&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Finally &lt;a href=&quot;http:\/\/nosql.mypopescu.com\/&quot; rel=&quot;nofollow&quot;&gt;MyNoSQL&lt;\/a&gt; is a good site for NoSQL related information.&lt;\/p&gt;\n"},{"Id":"2358572","ParentId":"2357239","CreationDate":"2010-03-01T19:47:13.280","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;When it comes to graph databases (like &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt; - a project I'm involved in) they excel at &lt;a href=&quot;http:\/\/blogs.neotechnology.com\/emil\/2009\/11\/nosql-scaling-to-size-and-scaling-to-complexity.html&quot; rel=&quot;nofollow&quot;&gt;scaling to complexity&lt;\/a&gt;. This means, they provide &lt;a href=&quot;http:\/\/www.viget.com\/extend\/nosql-misconceptions\/&quot; rel=&quot;nofollow&quot;&gt;&quot;better substrates for modeling business domains&quot;&lt;\/a&gt; (see &lt;a href=&quot;http:\/\/www.slideshare.net\/bscofield\/the-state-of-nosql&quot; rel=&quot;nofollow&quot;&gt;The State of NoSQL&lt;\/a&gt;, also by &lt;a href=&quot;http:\/\/benscofield.com\/&quot; rel=&quot;nofollow&quot;&gt;Ben Scofield&lt;\/a&gt;, too). As I see it, this is very important in small to medium sized apps.&lt;\/p&gt;\n\n&lt;p&gt;This may be better explained through examples, so here's some links to example apps\/domain modeling:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/blog.neo4j.org\/2010\/02\/access-control-lists-graph-database-way.html&quot; rel=&quot;nofollow&quot;&gt;Access control lists the graph\ndatabase way&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/blog.neo4j.org\/2009\/09\/social-networks-in-database-using-graph.html&quot; rel=&quot;nofollow&quot;&gt;Social networks in the database: using a graph database&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/Domain_Modeling_Gallery&quot; rel=&quot;nofollow&quot;&gt;Domain modeling gallery&lt;\/a&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2359212","ParentId":"2359159","CreationDate":"2010-03-01T21:31:16.307","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;8080 - JMX (remote)&lt;\/p&gt;\n\n&lt;p&gt;8888 - Remote debugger&lt;\/p&gt;\n\n&lt;p&gt;7000 - Used internal by Cassandra&lt;br&gt;\n(7001 - Obsolete, removed in 0.6.0. Used for membership communication, aka gossip)&lt;\/p&gt;\n\n&lt;p&gt;9160 - Thrift client API&lt;\/p&gt;\n\n&lt;p&gt;Cassandra FAQ &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/FAQ#ports&quot; rel=&quot;nofollow&quot;&gt;What ports does Cassandra use?&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2359282","ParentId":"2359175","CreationDate":"2010-03-01T21:43:51.987","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;A write to a Cassandra node first hits the &lt;strong&gt;CommitLog&lt;\/strong&gt; (sequential). (Then Cassandra stores values to column-family specific, in-memory data structures called Memtables. The Memtables are flushed to disk whenever one of the configurable thresholds is exceeded. (1, datasize in memtable. 2, # of objects reach certain limit, 3, lifetime of a memtable expires.))&lt;\/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;data&lt;\/strong&gt; folder contains a subfolder for each keyspace. Each subfolder contains three kind of files:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data files: An SSTable (nomenclature\nborrowed from Google) stands for\nSorted Strings Table and is a file of\nkey-value string pairs (sorted by\nkeys).&lt;\/li&gt;\n&lt;li&gt;Index file: (Key, offset) pairs (points into data file) &lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Bloom_filter&quot; rel=&quot;nofollow&quot;&gt;Bloom filter&lt;\/a&gt;: all keys in data file&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2360350","ParentId":"2357919","CreationDate":"2010-03-02T01:35:35.473","OwnerUserId":"284038","Tags":[],"Body":"&lt;p&gt;I doubt any of those guys will let us check out their code. ;)  You may want to check through here &lt;a href=&quot;http:\/\/www.opensourcerails.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.opensourcerails.com\/&lt;\/a&gt; , but I believe they all use some kind SQL.&lt;\/p&gt;\n"},{"Id":"2360378","ParentId":"2357919","CreationDate":"2010-03-02T01:43:13.483","OwnerUserId":"242493","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/harmonyapp.com\/&quot; rel=&quot;nofollow&quot;&gt;Harmony&lt;\/a&gt; is a Rails site that uses &lt;a href=&quot;http:\/\/www.mongodb.org\/&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt;. While it's not open source, it's co-creater, John Nunemaker, writes blog posts about how MongoDB is being used effectively in Harmony and often provides examples of code taken directly from the application. These posts appear on &lt;a href=&quot;http:\/\/railstips.org\/&quot; rel=&quot;nofollow&quot;&gt;RailsTips&lt;\/a&gt; and more recently on &lt;a href=&quot;http:\/\/mongotips.com\/&quot; rel=&quot;nofollow&quot;&gt;MongoTips&lt;\/a&gt;. John is also the author of &lt;a href=&quot;http:\/\/github.com\/jnunemaker\/mongomapper&quot; rel=&quot;nofollow&quot;&gt;MongoMapper&lt;\/a&gt;, an ActiveRecord-like library for MongoDB, so his writing is very informative and his code is about as close to doing things &quot;the right way&quot; as you're likely to find.&lt;\/p&gt;\n"},{"Id":"2364948","ParentId":"2357919","CreationDate":"2010-03-02T16:51:03.860","OwnerUserId":"156694","Tags":[],"Body":"&lt;p&gt;MySQL schema-less database at Friendfeed.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/bret.appspot.com\/entry\/how-friendfeed-uses-mysql&quot; rel=&quot;nofollow&quot;&gt;http:\/\/bret.appspot.com\/entry\/how-friendfeed-uses-mysql&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2365573","ParentId":"2351040","CreationDate":"2010-03-02T18:16:21.090","OwnerUserId":"196918","Tags":[],"Body":"&lt;p&gt;It depends on the data store you are using.&lt;\/p&gt;\n\n&lt;p&gt;I frequently use AppEngine and their data store only allows inequality on one column (and that column must be the first element in the sort order.  So you would not be able to run the query you posted, but you could do a similar one:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select * from DB where\nvara &amp;#38;gt; x AND\nvarb = 2 AND\nvarc in (t,u,v,w)\nvard in (x,y,z) AND\nvarf = 2 AND\nvarg = 3\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Also, you can do things like have a column that contains a list of strings and select rows that have a value in the list.&lt;\/p&gt;\n\n&lt;p&gt;So, the official answer is &quot;maybe, sorta, sometimes, but not really, except when yes&quot;&lt;\/p&gt;\n"},{"Id":"2368467","ParentId":"2360034","CreationDate":"2010-03-03T03:01:00.157","OwnerUserId":"2696","Tags":[],"Body":"&lt;p&gt;In a NoSQL or document-oriented scenario, you'd have the actual tags as part of your document, likely stored as a list.  Since you've tagged this question with &quot;couchdb&quot;, I'll use that as an example.&lt;\/p&gt;\n\n&lt;p&gt;A &quot;post&quot; document in CouchDB might look like:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n   &quot;_id&quot;: &amp;#38;lt;generated&amp;#38;gt;,\n   &quot;question&quot;: &quot;Question?&quot;,\n   &quot;answers&quot;: [... list of answers ...],\n   &quot;tags&quot;: [&quot;mysql&quot;, &quot;tagging&quot;, &quot;joins&quot;, &quot;nosql&quot;, &quot;couchdb&quot;]\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Then, to generate a view keyed by tags:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n   &quot;_id&quot;: &quot;_design\/tags&quot;,\n   &quot;language&quot;: &quot;javascript&quot;,\n   &quot;views&quot;: {\n      &quot;all&quot;: {\n         &quot;map&quot;: &quot;function(doc) {\n            emit(doc.tags, null);\n         }&quot;\n      }\n   }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;In CouchDB, you can issue an HTTP POST with multiple keys, if you wish.  An example is in &lt;a href=&quot;http:\/\/wiki.apache.org\/couchdb\/HTTP_view_API#Access.2BAC8-Query&quot; rel=&quot;nofollow&quot;&gt;the documentation&lt;\/a&gt;.  Using that technique, you would be able to search by multiple tags.&lt;\/p&gt;\n\n&lt;p&gt;Note: Setting the value to null, above, helps keep the views small.  Use &lt;code&gt;include_docs=true&lt;\/code&gt; in your query if you want to see the actual documents as well.&lt;\/p&gt;\n"},{"Id":"2369735","ParentId":"2354254","CreationDate":"2010-03-03T08:36:25.380","OwnerUserId":"260555","Tags":[],"Body":"&lt;p&gt;Here I'm assuming that you want to optimize one particular query, which is simply looking up a record by key. One example of this might be looking up a userinfo record by username. For some systems a query like that has to be incredibly fast and all other queries are unimportant.&lt;\/p&gt;\n\n&lt;p&gt;The biggest factor in database performance will be the number of I\/O operation required to read\/write data. Most database systems use similar data structures (i.e. b-trees) which can retieve uncached data in O(log(n)) I\/Os. In order to give durable updates the data will have to be written to disk: most systems do that sequentially, which is the fastest way.&lt;\/p&gt;\n\n&lt;p&gt;So, where can a Key-Value store get efficiencies?&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Non-normalized data. Putting all the data in one row means no joins.&lt;\/li&gt;\n&lt;li&gt;Low CPU overhead. A key-value store avoids the CPU cost of query processing\/optimization, security checks, constraint checks, etc.&lt;\/li&gt;\n&lt;li&gt;It is easier to have the store be in-process (as opposed to a SQL server running as a separate service) this eliminate IPC overhead.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Most RDBMS systems are built on top of something which looks like a key-value store so you could view this as cutting out the middleman.&lt;\/p&gt;\n"},{"Id":"2373364","ParentId":"2081080","CreationDate":"2010-03-03T17:11:53.857","OwnerUserId":"285504","Tags":[],"Body":"&lt;p&gt;Thanks for your kind mention of y_serial... more precisely, it is a Python module:&lt;\/p&gt;\n\n&lt;p&gt;warehouse Python objects with SQLite&lt;\/p&gt;\n\n&lt;p&gt;&quot;Serialization + persistance :: in a few lines of code, compress and annotate Python objects into SQLite; then later retrieve them chronologically by keywords without any SQL. Most useful &quot;standard&quot; module for a database to store schema-less data.&quot;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/yserial.sourceforge.net&quot; rel=&quot;nofollow&quot;&gt;http:\/\/yserial.sourceforge.net&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;In my experience, SQLite is a faster and more reliable choice than most databases (including PostgresQL and Berkeley DB) for the majority of projects -- and of course, it does not need a server daemon.&lt;\/p&gt;\n\n&lt;p&gt;yserial is very easy to implement (and far faster than the &quot;filename is the key \/ file contents is the value&quot; approach ;-)&lt;\/p&gt;\n"},{"Id":"2374517","ParentId":"2374496","CreationDate":"2010-03-03T20:02:21.853","OwnerUserId":"84651","Tags":[],"Body":"&lt;p&gt;A quick search for Berkeley DB Bindings for C# turned up - &lt;a href=&quot;http:\/\/sourceforge.net\/projects\/libdb-dotnet\/&quot; rel=&quot;nofollow&quot;&gt;Berkeley DB for .NET&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2374525","ParentId":"2374496","CreationDate":"2010-03-03T20:03:56.873","OwnerUserId":"283676","Tags":[],"Body":"&lt;p&gt;i vote for Berkley DB. it is very fast and &lt;a href=&quot;http:\/\/sourceforge.net\/projects\/libdb-dotnet\/&quot; rel=&quot;nofollow&quot;&gt;wrapper&lt;\/a&gt; is good. i used it a lot&lt;\/p&gt;\n"},{"Id":"2376631","ParentId":"2330612","CreationDate":"2010-03-04T03:09:30.193","OwnerUserId":"74235","Tags":[],"Body":"&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;you should start with &lt;a href=&quot;http:\/\/arin.me\/blog\/wtf-is-a-supercolumn-cassandra-data-model&quot; rel=&quot;nofollow&quot;&gt;WTF is a SuperColumn&lt;\/a&gt; to understand the data model.&lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http:\/\/blog.evanweaver.com\/articles\/2009\/07\/06\/up-and-running-with-cassandra\/&quot; rel=&quot;nofollow&quot;&gt;up and running with cassandra&lt;\/a&gt; is also a very good starting tutorial and it has\nexample schemas for a blog and for twitter.&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2377418","ParentId":"2348289","CreationDate":"2010-03-04T07:07:02.800","OwnerUserId":"286031","Tags":[],"Body":"&lt;p&gt;Why don't you create your own?&lt;\/p&gt;\n"},{"Id":"2381902","ParentId":"2248789","CreationDate":"2010-03-04T18:54:18.653","OwnerUserId":"166836","Tags":[],"Body":"&lt;p&gt;I'm using MongoDB w\/ CI and came up with the following. It works for me, but I'm sure it can be tweaked somewhat. I'll worry about tweaking it later but right now it does what I want.&lt;\/p&gt;\n\n&lt;p&gt;I created a model called &quot;database_conn.php&quot;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class Database_Conn extends Model {\n\n    function _connect() {\n        $m = new Mongo();\n\n        $db = $m-&amp;#38;gt;selectDB( &quot;YOUR DATABASE NAME&quot; );\n        return $db;\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Then, if I need to connect to a collection from my models.&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$collection = Database_Conn::_connect()-&amp;#38;gt;selectCollection( &quot;COLLECTION NAME&quot; );\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2397163","ParentId":"2357087","CreationDate":"2010-03-07T17:32:39.257","OwnerUserId":"25198","Tags":[],"Body":"&lt;p&gt;I'm using SimpleDB for a project now but it isn't in production yet.  We use it together with S3 to form our persistence layer.  Entities are stored as JSON encoded documents in S3 (with memcached in front as a write-through cache).  Metadata we want to query on is stored in SimpleDB.&lt;\/p&gt;\n\n&lt;p&gt;This combination is working very well.  Using a document based model allows us to have arbitrarily large entities.  The limitations of SimpleDB are less painful if you use it strictly as an index.&lt;\/p&gt;\n\n&lt;p&gt;The big lightbulb moment for me came when I stopped trying to think of SimpleDB as a drop-in replacement for a RDBMS.  The combo with S3 is quite good.&lt;\/p&gt;\n\n&lt;p&gt;Hope that's helpful.&lt;\/p&gt;\n"},{"Id":"2403246","ParentId":"2403174","CreationDate":"2010-03-08T17:26:39.873","OwnerUserId":"184977","Tags":[],"Body":"&lt;p&gt;firebird embedded version&lt;\/p&gt;\n"},{"Id":"2403258","ParentId":"2403174","CreationDate":"2010-03-08T17:28:05.630","OwnerUserId":"185349","Tags":[],"Body":"&lt;p&gt;You can check some oodb (object-oriented database)\nGemstone is nice. &lt;a href=&quot;http:\/\/www.gemstone.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.gemstone.com\/&lt;\/a&gt;\nhttp:\/\/www.versant.com\/&lt;\/p&gt;\n"},{"Id":"2403271","ParentId":"2403174","CreationDate":"2010-03-08T17:30:15.877","OwnerUserId":"285570","Tags":[],"Body":"&lt;p&gt;I think &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Berkeley_DB&quot; rel=&quot;nofollow&quot;&gt;BDB&lt;\/a&gt; is the classic choice here.&lt;\/p&gt;\n"},{"Id":"2403278","ParentId":"2403174","CreationDate":"2010-03-08T17:31:08.433","OwnerUserId":"91","Tags":[],"Body":"&lt;p&gt;Do you need something embedded in your application (if yes, which language are you using?) or a separate database server?&lt;\/p&gt;\n\n&lt;p&gt;Popular Object Databases are &lt;a href=&quot;http:\/\/www.mongodb.org\/&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; and &lt;a href=&quot;http:\/\/www.db4o.com\/&quot; rel=&quot;nofollow&quot;&gt;db4o&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2404713","ParentId":"2404155","CreationDate":"2010-03-08T21:03:31.383","OwnerUserId":"285289","Tags":[],"Body":"&lt;p&gt;In MongoDB 1.3.2+ you can add some restriction in user :&lt;\/p&gt;\n\n&lt;p&gt;&lt;code&gt;db.addUser(&quot;guest&quot;, &quot;passwordForGuest&quot;, true)&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;But it's only existing now not better. Maybe you can add some feature request&lt;\/p&gt;\n\n&lt;p&gt;see information in MongoDB documentation : &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Security+and+Authentication&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Security+and+Authentication&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2405068","ParentId":"2404155","CreationDate":"2010-03-08T21:53:58.047","OwnerUserId":"111332","Tags":[],"Body":"&lt;p&gt;I would say the best bet would be to wrap up the access to MongoDB in a service layer that enforces your specific contracts. We don't do much in the way of fine-grained access control because there are so many different cases that solving all of them correctly is tricky to get right. So for the most part it's up to the application layer to implement those kind of controls.&lt;\/p&gt;\n"},{"Id":"2409029","ParentId":"2408877","CreationDate":"2010-03-09T12:41:02.447","OwnerUserId":"249180","Tags":[],"Body":"&lt;p&gt;When I first encountered this problem, I've found the great article (&lt;a href=&quot;http:\/\/dev.mysql.com\/tech-resources\/articles\/hierarchical-data.html&quot; rel=&quot;nofollow&quot;&gt;link&lt;\/a&gt;).&lt;\/p&gt;\n\n&lt;p&gt;In tho words: in RDBMS world there are 2 main tree model storage approaches:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The Adjacency List Model&lt;\/li&gt;\n&lt;li&gt;The Nested Set Model&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2409106","ParentId":"2408877","CreationDate":"2010-03-09T12:53:54.223","OwnerUserId":"62195","Tags":[],"Body":"&lt;p&gt;Here's a &lt;a href=&quot;http:\/\/highscalability.com\/neo4j-graph-database-kicks-buttox&quot; rel=&quot;nofollow&quot;&gt;great article&lt;\/a&gt; on Neo4j.  In general, it looks like neo4j is your best option since document databases are still relatively flat and can result in some &lt;a href=&quot;http:\/\/probablyprogramming.com\/2008\/07\/04\/storing-hierarchical-data-in-couchdb\/&quot; rel=&quot;nofollow&quot;&gt;awkward setup&lt;\/a&gt; (still doable though).&lt;\/p&gt;\n\n&lt;p&gt;Neo4j, being a graph database, should be a solid fit for storing you tree.  I've never used it, but given your problem domain, it seems like the best option (at least the one to investigate first).&lt;\/p&gt;\n\n&lt;p&gt;As far as what the &quot;best way&quot; is, I think that depends on your implementation and requirements.  I think you should write a simple test against a graph database, a document database, an &lt;a href=&quot;http:\/\/www.db4o.com\/&quot; rel=&quot;nofollow&quot;&gt;object database&lt;\/a&gt; and a relational database (or not) and see which one fits the problem that you're trying to solve.&lt;\/p&gt;\n"},{"Id":"2410456","ParentId":"2410341","CreationDate":"2010-03-09T16:04:59.313","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;That depends on how you have configured storage-conf.xml on your two nodes. &lt;\/p&gt;\n\n&lt;p&gt;Hint. take a look at &lt;code&gt;&amp;#38;lt;StoragePort&amp;#38;gt;7000&amp;#38;lt;\/StoragePort&amp;#38;gt;&lt;\/code&gt; in storage-conf.xml. \n(TCP port 7000 is the standard\/default port used by Cassandra for internal communication, i.e. address to bind to and tell other nodes to connect to).&lt;\/p&gt;\n\n&lt;p&gt;UDP port (7001 default) was previous used for gossip, was removed in 0.6.0.&lt;\/p&gt;\n"},{"Id":"2411472","ParentId":"2411424","CreationDate":"2010-03-09T18:21:31.180","OwnerUserId":"536","Tags":[],"Body":"&lt;p&gt;Me too. I've been using DB4O for a few years, and have found the Object Manager clumsy, and often buggy (sometimes it won't open my large DB4O database at all).&lt;\/p&gt;\n\n&lt;p&gt;I know of no alternatives, unfortunately.&lt;\/p&gt;\n"},{"Id":"2417204","ParentId":"2411424","CreationDate":"2010-03-10T13:33:53.640","OwnerUserId":"257786","Tags":[],"Body":"&lt;p&gt;I suggest you use &lt;a href=&quot;http:\/\/www.linqpad.net\/&quot; rel=&quot;nofollow&quot;&gt;LINQPad&lt;\/a&gt; as a first start.&lt;\/p&gt;\n\n&lt;p&gt;Gamlor has &lt;a href=&quot;http:\/\/www.gamlor.info\/wordpress\/?p=949&quot; rel=&quot;nofollow&quot;&gt;a great tutorial on how to use LINQPad with db4o&lt;\/a&gt;. You'll need to modify that a bit so you use client-server access (thus not locking your db).&lt;\/p&gt;\n\n&lt;p&gt;I don't want to re-post his code here, because I think it's pointless and he's also a user here, don't wanna win his laurels. &lt;\/p&gt;\n\n&lt;p&gt;For me, LINQPad seems to be the best approach, also because I can use it on my server, where I'd be having trouble with OME for obvious reasons.&lt;\/p&gt;\n"},{"Id":"2421324","ParentId":"2413466","CreationDate":"2010-03-10T23:01:15.263","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;you have two options:&lt;\/p&gt;\n\n&lt;p&gt;(1) is sort of traditional: have one CF (columnfamily) with your foo objects, one row per foo, one column per field.  then create two index CFs, where the row key in one is the string values, and the row key in the other is lookup_id.  Columns in the index rows are foo ids.  So you do a GET on the index CF, then a MULTIGET on the ids returned.&lt;\/p&gt;\n\n&lt;p&gt;Note that if you can make id the same as lookup_id then you have one less index to maintain.&lt;\/p&gt;\n\n&lt;p&gt;High-level clients like Digg's lazyboy (http:\/\/github.com\/digg\/lazyboy) will automate maintaining the index CFs for you.  Cassandra itself does not do this automatically (yet).&lt;\/p&gt;\n\n&lt;p&gt;(2) is like (1), but you duplicate the entire foo objects into subcolumns of the index rows (that is, the index top-level columns are supercolumns).  If you're not actually querying by the foo id itself, you don't need to store it in its own CF at all.&lt;\/p&gt;\n"},{"Id":"2436098","ParentId":"2436046","CreationDate":"2010-03-12T21:43:59.217","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;Yes, row size is still limited by available memory. This is because the compaction algorithm today de-serializes the entire row in memory before writing out the compacted SSTable. &lt;\/p&gt;\n\n&lt;p&gt;This is currently aimed to be fixed in the 0.7 release. See &lt;a href=&quot;https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-16&quot; rel=&quot;nofollow&quot;&gt;CASSANDRA-16&lt;\/a&gt; for progress.&lt;\/p&gt;\n\n&lt;p&gt;Another interesting link: &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/CassandraLimitations&quot; rel=&quot;nofollow&quot;&gt;CassandraLimitations&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2438758","ParentId":"2438055","CreationDate":"2010-03-13T14:58:00.303","OwnerUserId":"155862","Tags":[],"Body":"&lt;p&gt;I think if you run it with the 'install' command line switch it installs it as a Windows Service.&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mongod --install\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;It might be worth reading &lt;a href=&quot;http:\/\/groups.google.com\/group\/mongodb-user\/browse_thread\/thread\/fde13e7ceb64ac44\/a39a7e1b711a6b00&quot; rel=&quot;nofollow&quot;&gt;this thread&lt;\/a&gt; first though. There seems to be some problems with relative\/absolute paths when the relevant registry key gets written.&lt;\/p&gt;\n"},{"Id":"2440091","ParentId":"2440079","CreationDate":"2010-03-13T21:26:17.920","OwnerUserId":"258550","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/carsonified.com\/blog\/dev\/should-you-go-beyond-relational-databases\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/carsonified.com\/blog\/dev\/should-you-go-beyond-relational-databases\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;Provides a nice breakdown of the things to consider when looking at certain types of data storage tools.&lt;\/p&gt;\n"},{"Id":"2440096","ParentId":"2440079","CreationDate":"2010-03-13T21:28:49.823","OwnerUserId":"114770","Tags":[],"Body":"&lt;p&gt;Digg have some &lt;a href=&quot;http:\/\/about.digg.com\/node\/564&quot; rel=&quot;nofollow&quot;&gt;interesting&lt;\/a&gt; &lt;a href=&quot;http:\/\/about.digg.com\/blog\/looking-future-cassandra&quot; rel=&quot;nofollow&quot;&gt;articles&lt;\/a&gt; on this question. Essentially, you're shifting the burden of processing to writes rather than reads, which may be desirable in highly scalable applications. Cassandra specifically is also highly available. &lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;Simplistically, Cassandra is a\n  distributed database with a BigTable\n  data model running on a Dynamo like\n  infrastructure. It is column-oriented\n  and allows for the storage of\n  relatively structured data. It has a\n  fully decentralized model; every node\n  is identical and there is no single\n  point of failure. It's also extremely\n  fault tolerant; data is replicated to\n  multiple nodes and across data\n  centers. Cassandra is also very\n  elastic; read and write throughput\n  increase linearly as new machines are\n  added.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"2440104","ParentId":"2440079","CreationDate":"2010-03-13T21:31:17.893","OwnerUserId":"122012","Tags":[],"Body":"&lt;p&gt;When you say, data modell is rather simple, this could speak for the NoSQL option.&lt;\/p&gt;\n\n&lt;p&gt;When you have plenty of attributes to make selections, heavy transaction load or complicated table structures, that would speak for traditional SQL tables.&lt;\/p&gt;\n\n&lt;p&gt;I would recommend to find out how difficult it would be to implement the data modell with one or two NoSQL databases. When this is rather difficult, you could also make a classical table schema to compare with.&lt;\/p&gt;\n\n&lt;p&gt;When you have difficulties with NoSQL, this could speak for the SQL option. But also it could be, that the heavy load is better handled with NoSQL -- but also it could be that a good SQL database scales sufficiently ...&lt;\/p&gt;\n\n&lt;p&gt;Buffering can also be done with a simple Proxy-Server ...&lt;\/p&gt;\n\n&lt;p&gt;On difficulties, a mix of NoSQL and SQL could be also considered.&lt;\/p&gt;\n"},{"Id":"2440141","ParentId":"2440079","CreationDate":"2010-03-13T21:41:28.973","OwnerUserId":"70604","Tags":[],"Body":"&lt;p&gt;To me, you don't have any particular problem to solve. If you need ACIDity, use a database; if you don't, then it doesn't matter. At the end just build your app. And let me quote &lt;a href=&quot;http:\/\/bjclark.me\/2009\/08\/04\/nosql-if-only-it-was-that-easy\/&quot; rel=&quot;nofollow&quot;&gt;NoSQL: If Only It Was That Easy&lt;\/a&gt;:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;The real thing to point out is that if you are being held back from making something super awesome because you can\u2019t choose a database, you are doing it wrong. If you know mysql, just used it. Optimize when you actually need to. Use it like a k\/v store, use it like a rdbms, but for god sake, build your killer app! None of this will matter to most apps. Facebook still uses MySQL, a lot. Wikipedia uses MySQL, a lot. FriendFeed uses MySQL, a lot. NoSQL is a great tool, but it\u2019s certainly not going to be your competitive edge, it\u2019s not going to make your app hot, and most of all, your users won\u2019t give a shit about any of this.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"2442760","ParentId":"2442735","CreationDate":"2010-03-14T16:13:27.147","OwnerUserId":"10661","Tags":[],"Body":"&lt;p&gt;The file system itself is faster and more stable than almost anything else.  It stores big data seamlessly and efficiently.  The API is very simple.  &lt;\/p&gt;\n\n&lt;p&gt;You can store and retrieve from the file system very, very efficiently.&lt;\/p&gt;\n\n&lt;p&gt;Since your question is a little thin on &quot;requirements&quot; it's hard to say much more.&lt;\/p&gt;\n"},{"Id":"2442778","ParentId":"2442735","CreationDate":"2010-03-14T16:17:32.993","OwnerUserId":"206367","Tags":[],"Body":"&lt;p&gt;What's wrong with &lt;a href=&quot;http:\/\/www.sqlite.org&quot; rel=&quot;nofollow&quot;&gt;SqlLite&lt;\/a&gt;? Since you did explicitly state non-sql, Berkeley DB are based on key\/value pairs which might not suffice for your needs if you wish to expand the datasets, even more so, how would you make that dataset relate to one another using key\/value pairs....&lt;\/p&gt;\n\n&lt;p&gt;On the other hand, Kdb+, looking at the &lt;a href=&quot;http:\/\/kx.com\/Products\/faq.php&quot; rel=&quot;nofollow&quot;&gt;FAQ&lt;\/a&gt; on their website is a relational database that can handle SQL via their programming language Q...be aware, if the need to migrate appears, there could be potential hitches, such as incompatible dialects or a query that uses vendor specifics, hence the potential to get locked into that database and not being able to migrate at all...something to bear in mind for later on...&lt;\/p&gt;\n\n&lt;p&gt;You need to be careful what you decide here and look at it from a long-term perspective, future upgrades, migration to another database, how easy would it be to up-scale, etc&lt;\/p&gt;\n"},{"Id":"2442784","ParentId":"2442735","CreationDate":"2010-03-14T16:19:12.750","OwnerUserId":"219155","Tags":[],"Body":"&lt;p&gt;One obvious entry in this category is Intersystems Cach\u00e9. (Well, obvious to me...) Be aware, though, it's not cheap. (But I don't think Kdb+ is either.)&lt;\/p&gt;\n"},{"Id":"2442851","ParentId":"2442735","CreationDate":"2010-03-14T16:36:39.290","OwnerUserId":"286260","Tags":[],"Body":"&lt;p&gt;What about Redis?&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/code.google.com\/p\/redis\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;Haven't try it yet did read about it and it seem to be a fast and stable enough for data storage.\nIt also provides you with a decent anti-single-point-failure solution, as far as I understand.&lt;\/p&gt;\n"},{"Id":"2442956","ParentId":"2442735","CreationDate":"2010-03-14T17:03:56.873","OwnerUserId":"257090","Tags":[],"Body":"&lt;p&gt;Berkely DB is tried and tested and hardened and is at the heart of many mega-high transaction volume systems. One example is wireless carrier infrastructure that use huge LDAP stores (OpenWave, for example) to process more than 2 BILLION transactions per day. These systems also commonly have something like Oracle in the mix too for point in time recovery, but they use Berkeley DB as replicated caches.&lt;\/p&gt;\n\n&lt;p&gt;Also, BDB is not limited to key value pairs in the simple sense of scalar values. You can store anything you want in the value, including arbitrary structures\/records.&lt;\/p&gt;\n"},{"Id":"2443045","ParentId":"2440079","CreationDate":"2010-03-14T17:31:57.507","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;I liked Ian Eure's rule of thumb: \u201cif you\u2019re deploying memcache on top of your database, you\u2019re inventing your own ad-hoc, difficult to maintain NoSQL system.\u201d&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.rackspacecloud.com\/blog\/2010\/02\/25\/should-you-switch-to-nosql-too\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.rackspacecloud.com\/blog\/2010\/02\/25\/should-you-switch-to-nosql-too\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2443392","ParentId":"2443381","CreationDate":"2010-03-14T19:15:16.920","OwnerUserId":"76337","Tags":[],"Body":"&lt;p&gt;SQL Server 2008 has new data types for storing and processing geographic information, in addition to the usual date and time types.&lt;\/p&gt;\n\n&lt;p&gt;See &quot;&lt;a href=&quot;http:\/\/technet.microsoft.com\/en-us\/library\/bb933876.aspx&quot; rel=&quot;nofollow&quot;&gt;Working with Spatial Data (Database Engine)&lt;\/a&gt;&quot;.&lt;\/p&gt;\n"},{"Id":"2443445","ParentId":"2443381","CreationDate":"2010-03-14T19:29:35.790","OwnerUserId":"178060","Tags":[],"Body":"&lt;p&gt;The &quot;best&quot; depends on how you are going to use it and what limitations you are willing to impose upon your solution.  For geographic mapping people are using a technique which transforms the lat\/long into a hash code (see &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/R-tree&quot; rel=&quot;nofollow&quot;&gt;R-Tree&lt;\/a&gt;.)  Most SQL databases also support a single column datetime.  You can look to the &lt;a href=&quot;http:\/\/www.opengeospatial.org\/&quot; rel=&quot;nofollow&quot;&gt;Open GIS&lt;\/a&gt; website for pre-made third party API's which specifically handle this type of geographic mapping (and more powerful forms of mapping.)&lt;\/p&gt;\n\n&lt;p&gt;Without knowing more about the true problem you're attempting to solve and what constraints you plan to solve it I'd refer you to one of the NoSQL types of databases, specifically &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDG&lt;\/a&gt; and then refer to &lt;a href=&quot;http:\/\/gissolved.blogspot.com\/2009\/05\/populating-mongodb-with-pois.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt; blog.&lt;\/p&gt;\n"},{"Id":"2443873","ParentId":"2443712","CreationDate":"2010-03-14T21:35:36.090","OwnerUserId":"293580","Tags":[],"Body":"&lt;p&gt;I am going through the same process. You might find SimplyStored interesting if you haven't already given it a look.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/peritor\/simply_stored&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/peritor\/simply_stored&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2445190","ParentId":"2443712","CreationDate":"2010-03-15T05:04:11.297","OwnerUserId":"242298","Tags":[],"Body":"&lt;p&gt;The basic layer of CouchRest is probably the best to get started, CouchPotato is the most active for Rails integration, SimplyStored adds some nicities on top of CouchPotato&lt;\/p&gt;\n"},{"Id":"2448065","ParentId":"2445878","CreationDate":"2010-03-15T15:02:52.630","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;you would do a multiget_slice w\/ keys of alfred, joe, and molly, and SlicePredicate column_names of id, age, phone&lt;\/p&gt;\n"},{"Id":"2448225","ParentId":"2445878","CreationDate":"2010-03-15T15:23:51.840","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;A Cassandra equivalent could be modelled as described below:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Users = { \/\/this is a ColumnFamily\n  &quot;alfred&quot;: { \/\/this is the key to this Row inside the CF\n     &quot;age&quot;:&quot;30&quot;,\n     &quot;city&quot;:&quot;london&quot;,\n     &quot;phone&quot;:&quot;3281283&quot;\n  }, \/\/ end row\n  \/\/ more rows...\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;where &quot;alfred&quot; is your (row) key and the row has three columns; age, city and phone. (Omitted the timestamp field (for simplicity) for the three columns)&lt;\/p&gt;\n"},{"Id":"2452216","ParentId":"2452169","CreationDate":"2010-03-16T05:00:25.537","OwnerUserId":"110436","Tags":[],"Body":"&lt;p&gt;What exactly do you want to implement? What do you mean saying &quot;Object Databases&quot;? Most likely you just need an ORM (Object-Relational Mapper) tool to work with RDBMS in object-oriented way. You can find list of ORMs &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/List_of_object-relational_mapping_software&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2452304","ParentId":"2452169","CreationDate":"2010-03-16T05:29:36.933","OwnerUserId":"38807","Tags":[],"Body":"&lt;p&gt;I guess you want to get started in MongoDb with asp.net MVC. In that case get the latest community supported drivers for Mongodb from &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/samus\/mongodb-csharp&lt;\/a&gt; and follow this step by step blog post &lt;a href=&quot;http:\/\/odetocode.com\/Blogs\/scott\/archive\/2009\/10\/13\/experimenting-with-mongodb-from-c.aspx&quot; rel=&quot;nofollow&quot;&gt;http:\/\/odetocode.com\/Blogs\/scott\/archive\/2009\/10\/13\/experimenting-with-mongodb-from-c.aspx&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2453646","ParentId":"2453513","CreationDate":"2010-03-16T10:46:14.947","OwnerUserId":"2289","Tags":[],"Body":"&lt;p&gt;It does support fetching by object ID.  Your id variable should be an Oid.  Is it the correct type?&lt;\/p&gt;\n\n&lt;p&gt;Here is a complete program that will&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Connect to Mongo&lt;\/li&gt;\n&lt;li&gt;Insert a document&lt;\/li&gt;\n&lt;li&gt;Fetch the document back using its ID&lt;\/li&gt;\n&lt;li&gt;Print the document's details.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;pre&gt;&lt;code&gt;\n\/\/ Connect to Mongo\nMongo db = new Mongo();\ndb.Connect();\n\n\/\/ Insert a test document\nvar insertDoc = new Document { { &quot;name&quot;, &quot;my document&quot; } };\ndb[&quot;database&quot;][&quot;collection&quot;].Insert(insertDoc);\n\n\/\/ Extract the ID from the inserted document, stripping the enclosing quotes\nstring idString = insertDoc[&quot;_id&quot;].ToString().Replace(&quot;\\&quot;&quot;, &quot;&quot;);\n\n\/\/ Get an Oid from the ID string\nOid id = new Oid(idString);\n\n\/\/ Create a document with the ID we want to find\nvar queryDoc = new Document { { &quot;_id&quot;, id } };\n\n\/\/ Query the db for a document with the required ID \nvar resultDoc = db[&quot;database&quot;][&quot;collection&quot;].FindOne(queryDoc);\ndb.Disconnect();\n\n\/\/ Print the name of the document to prove it worked\nConsole.WriteLine(resultDoc[&quot;name&quot;].ToString());\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2455442","ParentId":"2452395","CreationDate":"2010-03-16T15:01:40.137","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;You mean, random columns?&lt;\/p&gt;\n\n&lt;p&gt;I guess you'd need to read them all to the client and use random.choice or something, it won't do that for you server-side.&lt;\/p&gt;\n"},{"Id":"2457373","ParentId":"2173082","CreationDate":"2010-03-16T19:17:40.640","OwnerUserId":"62649","Tags":[],"Body":"&lt;p&gt;Jon Meredith recently gave a presentation to the Front Range PHP Users Group about NoSQL databases which you might find useful: &lt;a href=&quot;http:\/\/www.frontrangephp.org\/resources\/122-introduction-to-nosql-databases&quot; rel=&quot;nofollow&quot;&gt;link&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2457383","ParentId":"1245338","CreationDate":"2010-03-16T19:18:44.337","OwnerUserId":"62649","Tags":[],"Body":"&lt;p&gt;See &lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/2173082\/what-exactly-is-nosql\/2457373#2457373&quot;&gt;this question&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2461892","ParentId":"2411424","CreationDate":"2010-03-17T11:48:08.510","OwnerUserId":"157321","Tags":[],"Body":"&lt;p&gt;Hi,&lt;\/p&gt;\n\n&lt;p&gt;We have discussed integrating some kind of &quot;interactive LINQ query&quot; into Object Manager (.Net of course :). &lt;\/p&gt;\n\n&lt;p&gt;If you think this would be a nice feature, please vote in the issue &lt;a href=&quot;http:\/\/tracker.db4o.com\/browse\/OMN-146&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Also, any input on how to improve Object Manager is welcome.&lt;\/p&gt;\n"},{"Id":"2462653","ParentId":"2460876","CreationDate":"2010-03-17T13:43:32.797","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Migrating from MySQL to Cassandra is usually not something people describe as &quot;easy&quot; for nontrivial applications, but &lt;a href=&quot;http:\/\/github.com\/NZKoz\/cassandra_object\/tree\/master&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/NZKoz\/cassandra_object\/tree\/master&lt;\/a&gt; can help.&lt;\/p&gt;\n"},{"Id":"2463324","ParentId":"2457494","CreationDate":"2010-03-17T15:04:02.523","OwnerUserId":"3211","Tags":[],"Body":"&lt;p&gt;Currently, there are not, and there is a chance that it may not be possible.&lt;\/p&gt;\n\n&lt;p&gt;Conditional updates and deletes only allow a condition to be set on the item being updated. In addition to this, there is no built in mechanism to store multiple versions of the same data or store sequence information for multi-valued attributes. Different attribute names can be used, but this breaks querying.&lt;\/p&gt;\n\n&lt;p&gt;The recent consistency updates allow for easily implemented transactions at the item level. However, across multiple items in the same (or different) domain, there is no straight forward implementation. Also there are no isolation level options.&lt;\/p&gt;\n\n&lt;p&gt;It might be possible to do, but I fear that you would end up killing all query capability in the process. Either through inconsistent attribute names or by requiring more SELECT conditions than good performance will allow. &lt;\/p&gt;\n\n&lt;p&gt;Beyond that, it seems like it would require consistent reads for ALL access to the data. This will probably negate all of the the availability benefits of using an eventually consistent system, since consistent reads fail when even a single replica node cannot be reached. &lt;\/p&gt;\n\n&lt;p&gt;I'm not trying to be a nay-sayer, I just wonder how practical it is. You already have to give up a lot of features to get SimpleDB's high availability, which is fine if you don't need those features, but if you then give up the high availability as well, I think there are a lot fewer cases where that would be a good trade off.&lt;\/p&gt;\n"},{"Id":"2475461","ParentId":"1959818","CreationDate":"2010-03-19T06:49:55.480","OwnerUserId":"76486","Tags":[],"Body":"&lt;p&gt;It looks like there are some in the works for MongoDB:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/sbellity\/futon4mongo&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/sbellity\/futon4mongo&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Http+Interface&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Http+Interface&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongohq.com&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongohq.com&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2476507","ParentId":"2476280","CreationDate":"2010-03-19T10:37:55.400","OwnerUserId":"164197","Tags":[],"Body":"&lt;p&gt;Many of the NoSQL type systems have details of migration, there are some video examples in here&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Articles&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Articles&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;or try here&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/nosql-databases.org\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/nosql-databases.org\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/groups.google.com\/group\/nosql-discussion&quot; rel=&quot;nofollow&quot;&gt;http:\/\/groups.google.com\/group\/nosql-discussion&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2479790","ParentId":"2479589","CreationDate":"2010-03-19T18:43:57.160","OwnerUserId":"122101","Tags":[],"Body":"&lt;p&gt;Something like this:  &lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Products : { \/\/ Column Family  \n    productA : { \/\/Row key  \n        name: 'The name of the product' \/\/ column\n        price: 33.55 \/\/ column\n        tags : 'fun, toy' \/\/ column\n    }  \n}\n\nProductTag : { \/\/ Column Family\n    fun : { \/\/Row key\n        timeuuid_1 : productA \/\/ column\n        timeuuid_2 : productB \/\/ column\n    },\n    toy : { \/\/Row key\n        timeuuid_3 : productA \/\/ column\n    }\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;UPDATE&lt;\/strong&gt;&lt;br&gt;\nCheck this &lt;a href=&quot;http:\/\/www.mail-archive.com\/user@cassandra.apache.org\/msg00108.html&quot; rel=&quot;nofollow&quot;&gt;Model to store biggest score&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2483820","ParentId":"2483530","CreationDate":"2010-03-20T16:30:18.177","OwnerUserId":"131433","Tags":[],"Body":"&lt;p&gt;You seem to have a specific idea in mind of a 'document database,' but that term is not a term of art.&lt;\/p&gt;\n\n&lt;p&gt;A 'document database' could be:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a database with a big text field column and a full text index.&lt;\/li&gt;\n&lt;li&gt;an XML database such as MarkLogic or SoftwareAG&lt;\/li&gt;\n&lt;li&gt;a complex SQL schema that models a document as a lot of little pieces.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;There are more or less efficient architectures and indexing strategies for all of these. Only the last uses conventional SQL indices. Full text support uses inverted term indices such as are implemented by Lucene to supply fast search on arbitrary terms. XML databases build indexes on XPath expressions.&lt;\/p&gt;\n"},{"Id":"2484449","ParentId":"2357239","CreationDate":"2010-03-20T19:41:37.197","OwnerUserId":"285504","Tags":[],"Body":"&lt;p&gt;The question perhaps requires a bit more context... assuming a Python environment, consider the tutorial at  the y_serial project: &lt;a href=&quot;http:\/\/yserial.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/yserial.sourceforge.net\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;NoSQL is not merely adopted for reasons of scalability. Serialization (of any arbitrary Python object) and persistence are very convenient at any scale -- so consider the key-value system as one approach. &lt;\/p&gt;\n"},{"Id":"2484491","ParentId":"2278186","CreationDate":"2010-03-20T19:52:40.480","OwnerUserId":"285504","Tags":[],"Body":"&lt;p&gt;If you are in a Python environment, consider the y_serial module: &lt;a href=&quot;http:\/\/yserial.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/yserial.sourceforge.net\/&lt;\/a&gt; &lt;\/p&gt;\n\n&lt;p&gt;In under 10 minutes, you will be able to store and access your images (in fact, any arbitrary Python object including webpages) -- in compressed form; NoSQL.&lt;\/p&gt;\n"},{"Id":"2484513","ParentId":"2403174","CreationDate":"2010-03-20T19:57:59.253","OwnerUserId":"285504","Tags":[],"Body":"&lt;p&gt;Sounds like a job for y_serial ;-)&lt;\/p&gt;\n\n&lt;p&gt;Here's the description: &quot;Serialization + persistance :: in a few lines of code, compress and annotate Python objects into SQLite; then later retrieve them chronologically by keywords without any SQL. Most useful &quot;standard&quot; module for a database to store schema-less data.&quot;&lt;\/p&gt;\n\n&lt;p&gt;See &lt;a href=&quot;http:\/\/yserial.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/yserial.sourceforge.net\/&lt;\/a&gt; for more details.&lt;\/p&gt;\n"},{"Id":"2484565","ParentId":"2328169","CreationDate":"2010-03-20T20:10:21.427","OwnerUserId":"285504","Tags":[],"Body":"&lt;p&gt;y_serial is written as a single Python module which reads like a working tutorial and includes many tips and references: &lt;a href=&quot;http:\/\/yserial.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/yserial.sourceforge.net\/&lt;\/a&gt; &lt;\/p&gt;\n\n&lt;p&gt;This takes the perspective of how to persist an arbitrary Python object (e.g. a dictionary data structure) in a &quot;NoSQL&quot; (Not only SQL) manner. &lt;\/p&gt;\n"},{"Id":"2484799","ParentId":"2357087","CreationDate":"2010-03-20T21:14:13.247","OwnerUserId":"50277","Tags":[],"Body":"&lt;p&gt;I'm using SimpleDB as the main structured data store for &lt;a href=&quot;http:\/\/www.gridroom.com\/&quot; rel=&quot;nofollow&quot;&gt;GridRoom&lt;\/a&gt;, a sports video sharing and collaboration service currently in beta. &lt;\/p&gt;\n\n&lt;p&gt;I created &lt;a href=&quot;http:\/\/simplesavant.codeplex.com\/&quot; rel=&quot;nofollow&quot;&gt;Simple Savant&lt;\/a&gt;, an open-source object persistence framework written in C#, to make this easier for myself. Aside from the initial work involved in creating the Savant framework itself, using SimpleDB means I spend about 1% of my time working on and supporting the data tier vs, say, 15% with SQL Server or another RDBMS. &lt;\/p&gt;\n\n&lt;p&gt;There are still gaps in the administration and reporting tools that might make it difficult to justify moving an existing application over to SimpleDB for a couple more years, but for a new application I've found it to be a great choice.&lt;\/p&gt;\n"},{"Id":"2484928","ParentId":"2457494","CreationDate":"2010-03-20T21:54:07.940","OwnerUserId":"50277","Tags":[],"Body":"&lt;p&gt;I've thought a lot about this while working on my &lt;a href=&quot;http:\/\/simplesavant.codeplex.com\/&quot; rel=&quot;nofollow&quot;&gt;Simple Savant C# library for SimpleDB&lt;\/a&gt;, and I've come to the conclusion that attempting to layer true transactionality on a distributed system like SimpleDB is a bad idea for a whole host of reasons.&lt;\/p&gt;\n\n&lt;p&gt;The best thing I've come up with (that provides value without over-promising and over-complicating the system) is something I'm calling &quot;reliable writes&quot;. This would guarantee that all operations (puts and deletes) in a cross-domain update complete eventually. The only way for part of the write to fail permanently would be if one of your updates violated SimpleDB constraints.&lt;\/p&gt;\n\n&lt;p&gt;This feature has not yet been implemented, but you can read more details and comment on the feature &lt;a href=&quot;http:\/\/simplesavant.codeplex.com\/WorkItem\/View.aspx?WorkItemId=2181&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;. I'd be interested in hearing your thoughts and how this would meet your needs as an application developer.&lt;\/p&gt;\n"},{"Id":"2486364","ParentId":"2486346","CreationDate":"2010-03-21T08:00:57.860","OwnerUserId":"271959","Tags":[],"Body":"&lt;p&gt;First, is 0.5s a problem or not? And did you already optimize your queries, datamodel and configuration settings? If not, you can still get better performance. Performance is a choice.&lt;\/p&gt;\n\n&lt;p&gt;Besides speed, there is also functionality, that's what you will loose.&lt;\/p&gt;\n\n&lt;p&gt;===&lt;\/p&gt;\n\n&lt;p&gt;What about pushing the function to a JOIN:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;EXPLAIN ANALYZE\nSELECT \n    D.doc_id as doc_id,\n    (count(D.doc_crc32) *1.0 \/ testing.get_count_by_doc_id(D.doc_id))::real as avg_doc \nFROM \n    testing.text_attachment D\n        JOIN (SELECT testing.get_crc32_rows_by_doc_id(29758) AS r) AS crc ON D.doc_crc32 = r\nWHERE \n    D.doc_id &amp;#38;lt;&amp;#38;gt; 29758\nGROUP BY D.doc_id\nORDER BY avg_doc DESC\nLIMIT 10\n&lt;\/code&gt;&lt;\/pre&gt;\n"},{"Id":"2486373","ParentId":"2452169","CreationDate":"2010-03-21T08:05:01.730","OwnerUserId":"64105","Tags":[],"Body":"&lt;p&gt;For a good introduction to MongoDB with C#, you might look at this series:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/mookid.dk\/oncode\/archives\/1057&quot; rel=&quot;nofollow&quot;&gt;http:\/\/mookid.dk\/oncode\/archives\/1057&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/mookid.dk\/oncode\/archives\/1107&quot; rel=&quot;nofollow&quot;&gt;http:\/\/mookid.dk\/oncode\/archives\/1107&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/mookid.dk\/oncode\/archives\/1145&quot; rel=&quot;nofollow&quot;&gt;http:\/\/mookid.dk\/oncode\/archives\/1145&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/mookid.dk\/oncode\/archives\/1165&quot; rel=&quot;nofollow&quot;&gt;http:\/\/mookid.dk\/oncode\/archives\/1165&lt;\/a&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;As for using it from ASP.net MVC, I don't know of any reference-implementation yet.&lt;\/p&gt;\n"},{"Id":"2486507","ParentId":"2486346","CreationDate":"2010-03-21T09:20:31.317","OwnerUserId":"35306","Tags":[],"Body":"&lt;p&gt;1.5 GByte is nothing. Serve from ram. Build a datastructure that helps you searching.&lt;\/p&gt;\n"},{"Id":"2486640","ParentId":"2486346","CreationDate":"2010-03-21T10:05:39.503","OwnerUserId":"112499","Tags":[],"Body":"&lt;p&gt;If you're getting that bad performance out of PostgreSQL, a good start would be to tune PostgreSQL, your query and possibly your datamodel. A query like that should serve a lot faster on such a small table.&lt;\/p&gt;\n"},{"Id":"2486878","ParentId":"2486346","CreationDate":"2010-03-21T11:30:41.807","OwnerUserId":"123984","Tags":[],"Body":"&lt;p&gt;I don't think your main problem here is the kind of database you're using but the fact that you don't in fact have an &quot;index&quot; for what you're searching: similarity between documents.&lt;\/p&gt;\n\n&lt;p&gt;My proposal is to determine once which are the 10 documents similar to each of the 100.000 doc_ids and &lt;strong&gt;cache&lt;\/strong&gt; the result in a new table like this:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;doc_id(integer)-similar_doc(integer)-score(integer)\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;where you'll insert 10 rows per document each of them representing the 10 best matches for it. You'll get 400.000 rows which you can directly access by index which should take down search time to something like O(log n) (depending on index implementation).&lt;\/p&gt;\n\n&lt;p&gt;Then, on each insertion or removal of a document (or one of its values) you iterate through the documents and update the new table accordingly.&lt;\/p&gt;\n\n&lt;p&gt;e.g. when a new document is inserted:\nfor each of the documents already in the table&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;you calculate its match score and&lt;\/li&gt;\n&lt;li&gt;if the score is higher than the lowest score of the similar documents cached in the new table you swap in the similar_doc and score of the newly inserted document&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"2489708","ParentId":"2488783","CreationDate":"2010-03-22T02:27:23.573","OwnerUserId":"237955","Tags":[],"Body":"&lt;p&gt;It looks like a bug in the library:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sub readByte\n{\n    my $self  = shift;\n    my $value = shift;\n\n    my $data = $self-&amp;#38;gt;{trans}-&amp;#38;gt;readAll(1);\n    my @arr = unpack('c', $data);\n    $$value = $arr[0];    # &amp;#38;lt;~ line 376\n    return 1;\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;(from &lt;code&gt;&lt;a href=&quot;http:\/\/cpansearch.perl.org\/src\/TEODOR\/Net-Cassandra-Easy-0.05\/lib\/Net\/GenThrift\/Thrift\/BinaryProtocol.pm&quot; rel=&quot;nofollow&quot;&gt;Net::GenThrift::Thrift::BinaryProtocol&lt;\/code&gt;&lt;\/a&gt;)&lt;\/p&gt;\n\n&lt;p&gt;Apparently that sub is being called from somewhere in the library where &lt;code&gt;$value&lt;\/code&gt; is not a variable, but a constant scalar. I'd report the bug to the authors.&lt;\/p&gt;\n"},{"Id":"2493782","ParentId":"2488783","CreationDate":"2010-03-22T16:07:44.110","OwnerUserId":"58394","Tags":[],"Body":"&lt;p&gt;The code works as expected under Cassandra 0.6.x, but fails under Cassandra 0.5.x.&lt;\/p&gt;\n\n&lt;p&gt;It appears as if &lt;code&gt;Net::Cassandra::Easy&lt;\/code&gt; is targeting Cassandra 0.6.x only.&lt;\/p&gt;\n\n&lt;p&gt;Upgrading to Cassandra 0.6.x solves the problem.&lt;\/p&gt;\n"},{"Id":"2496466","ParentId":"2495141","CreationDate":"2010-03-22T23:21:57.257","OwnerUserId":"74496","Tags":[],"Body":"&lt;p&gt;How about&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$result = $cassandra-&amp;#38;gt;get(['row1', 'row2', 'row3'], family =&amp;#38;gt; 'Standard1', standard =&amp;#38;gt; 1);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;&lt;code&gt;standard =&amp;#38;gt; 1&lt;\/code&gt; will force Net::Cassandra::Easy to use a slice predicate that matches all columns in the family.&lt;\/p&gt;\n"},{"Id":"2496672","ParentId":"2452169","CreationDate":"2010-03-23T00:14:41.737","OwnerUserId":"50964","Tags":[],"Body":"&lt;p&gt;Have a look at Rob Conery's blog at &lt;a href=&quot;http:\/\/www.wekeroad.com&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.wekeroad.com&lt;\/a&gt;. He's been doing a lot of work lately with MongoDB. His main project which provides an object persistence interface, although it is still very much a work in progress, can be found on &lt;a href=&quot;http:\/\/github.com\/atheken\/NoRM&quot; rel=&quot;nofollow&quot;&gt;github as NoRM&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2496740","ParentId":"667141","CreationDate":"2010-03-23T00:33:11.360","OwnerUserId":"207036","Tags":[],"Body":"&lt;p&gt;There are three main data models (C.J.Date, E.F.Codd) and I am adding a flat file to this:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;flat file(s) (structure varies - from 'stupid' flat text to files conforming to grammars which coupled with clever tools do very clever things, think compilers and what they can do, narrow application in modelling new things)&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Hierarchical_data_model&quot; rel=&quot;nofollow&quot;&gt;hierarchical&lt;\/a&gt; (trees, nested sets - examples: xml and other markup languages, registry, organizational charts, etc; anything can be modelled, but integrity rules are not easy to express and retrieval is hard to optimize automatically, some retrieval is fast and some is very slow )&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Network_data_model&quot; rel=&quot;nofollow&quot;&gt;network&lt;\/a&gt; (networks, graphs - examples: navigational databases, hyperlinks, semantic web, again almost anything can be modelled but automatic optimizing of retrieval is a problem)&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Relational_data_model&quot; rel=&quot;nofollow&quot;&gt;relational&lt;\/a&gt; (first order predicate logic - example: relational databases, automatic optimization of retrieval)&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Both hierarchical and network can be represented in relational and relational can be expressed in the other two.&lt;\/p&gt;\n\n&lt;p&gt;The reason that relational is considered 'better' is the declarative nature and standardization on not only the data retrieval language but also on the data definition language, including the strong declarative data integrity, backed up with &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/ACID&quot; rel=&quot;nofollow&quot;&gt;stable&lt;\/a&gt;, scalable, multi-user management system.&lt;\/p&gt;\n\n&lt;p&gt;Benefits come at a cost, which most projects find to be a good ratio for systems (multi application) that store long term data in a from that will be usable in foreseeable future.&lt;\/p&gt;\n\n&lt;p&gt;If you are not building a system, but a single application, perhaps for a single user, and you are fairly certain that you will not want multiple applications using your data, nor multiple users, any time soon then you'll probably find faster approaches.&lt;\/p&gt;\n\n&lt;p&gt;Also if you don't know what kind of data you want to store and how to model it then relational model strengths are wasted on it. &lt;\/p&gt;\n\n&lt;p&gt;Or if you simply don't care about integrity of your data that much (which can be fine).&lt;\/p&gt;\n\n&lt;p&gt;All data structures are optimized for a certain kind of use, only relational if properly modelled tries to represent the 'reality' in semantically unbiased way. People who had bad experience with relational databases usually don't realize that their experience would have been much worse with other types of data models. Horrible implementations are possible, and especially with relational databases, where it is relatively easy to build complex models, you could end up with quite a monster on your hands. Still I always feel better when I try to imagine the same monster in xml.&lt;\/p&gt;\n\n&lt;p&gt;One example of how good relational model is, IMO, is ratio of complexity vs shortness of the questions that you will find that involve SQL.&lt;\/p&gt;\n"},{"Id":"2500874","ParentId":"2500595","CreationDate":"2010-03-23T14:55:48.373","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;Take a look at &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/API#get_range_slices&quot; rel=&quot;nofollow&quot;&gt;get_range_slices&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;list&amp;#38;lt;KeySlice&amp;#38;gt; get_range_slices(keyspace, column_parent, predicate, range, consistency_level) \n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;which replaces get_range_slice in 0.6.&lt;\/p&gt;\n\n&lt;p&gt;Nb. For version &amp;#38;lt; 0.6: this method is only allowed when using an order-preserving partitioner. &lt;\/p&gt;\n"},{"Id":"2502768","ParentId":"2502742","CreationDate":"2010-03-23T18:55:55.553","OwnerUserId":"84651","Tags":[],"Body":"&lt;p&gt;MongoDB powers SourceForge, The New York Times, and several other large databases...&lt;\/p&gt;\n"},{"Id":"2502789","ParentId":"2502742","CreationDate":"2010-03-23T18:58:53.600","OwnerUserId":"636","Tags":[],"Body":"&lt;p&gt;You should read the &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Use+Cases&quot; rel=&quot;nofollow&quot;&gt;MongoDB use cases&lt;\/a&gt;. People who are just playing with technology are often just looking at how does this work and are not at the point where they can understand the limitations. For the right sorts of datasets and access patterns 50GB is nothing for MongoDB running on the right hardware. &lt;\/p&gt;\n\n&lt;p&gt;These non-relational systems look at the trade-offs which RDBMs made, and changed them a bit. Consistency is not as important as other things in some situations so these solutions let you trade that off for something else. The trade-off is still relatively minor ms or maybe secs in some situations.&lt;\/p&gt;\n\n&lt;p&gt;It is worth reading about the &lt;a href=&quot;http:\/\/www.julianbrowne.com\/article\/viewer\/brewers-cap-theorem&quot; rel=&quot;nofollow&quot;&gt;CAP theorem&lt;\/a&gt; too.&lt;\/p&gt;\n"},{"Id":"2502795","ParentId":"2502742","CreationDate":"2010-03-23T18:59:10.803","OwnerUserId":"40015","Tags":[],"Body":"&lt;p&gt;Here's some benchmarks on db4o:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.db4o.com\/about\/productinformation\/benchmarks\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.db4o.com\/about\/productinformation\/benchmarks\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I think it ultimately depends on a lot of factors, including the complexity of the data, but db4o seems to certainly hang with the best of them.&lt;\/p&gt;\n"},{"Id":"2502988","ParentId":"2502742","CreationDate":"2010-03-23T19:32:06.787","OwnerUserId":"25343","Tags":[],"Body":"&lt;p&gt;I was looking at moving the API I have for sure with the stack overflow iphone app I wrote a while back to MongoDB from where it currently sits in a MySQL database. In raw form the SO CC dump is in the multi-gigabyte range and the way I constructed the documents for MongoDB resulted in a 10G+ database. It is arguable that I didn't construct the documents well but I didn't want to spend a ton of time doing this.&lt;\/p&gt;\n\n&lt;p&gt;One of the very first things you will run into if you start down this path is the lack of 32 bit support. Of course everything is moving to 64 bit now but just something to keep in mind. I don't think any of the major document databases support paging in 32 bit mode and that is understandable from a code complexity standpoint.&lt;\/p&gt;\n\n&lt;p&gt;To test what I wanted to do I used a 64 bit instance EC2 node. The second thing I ran into is that even though this machine had 7G of memory when the physical memory was exhausted things went from fast to not so fast. I'm not sure I didn't have something set up incorrectly at this point because the non-support of 32 bit system killed what I wanted to use it for but I still wanted to see what it looked like. Loading the same data dump into MySQL takes about 2 minutes on a much less powerful box but the script I used to load the two database works differently so I can't make a good comparison. Running only a subset of the data into MongoDB was much faster as long as it resulted in a database that was less than 7G.&lt;\/p&gt;\n\n&lt;p&gt;I think my take away from it was that large databases will work just fine but you may have to think about how the data is structured more than you would with a traditional database if you want to maintain the high performance. I see a lot of people using MongoDB for logging and I can imagine that a lot of those databases are massive but at the same time they may not be doing a lot of random access so that may mask what performance would look like for more traditional applications.&lt;\/p&gt;\n\n&lt;p&gt;A recent resource that might be helpful is the &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;visual guide to nosql systems&lt;\/a&gt;. There are a decent number of choices outside of MongoDB. I have used Redis as well although not with as large of a database.&lt;\/p&gt;\n"},{"Id":"2503319","ParentId":"2502742","CreationDate":"2010-03-23T20:22:19.280","OwnerUserId":"4243","Tags":[],"Body":"&lt;p&gt;Someone just went into production with a 12 terabytes of data in MongoDB.  The largest I knew of before that was 1 TB.  Lots of people are keeping really large amounts of data in Mongo.&lt;\/p&gt;\n\n&lt;p&gt;It's important to remember that Mongo works a lot like a relational database: you need the right indexes to get good performance.  You can use explain() on queries and contact &lt;a href=&quot;http:\/\/groups.google.com\/group\/mongodb-user\/&quot; rel=&quot;nofollow&quot;&gt;the user list&lt;\/a&gt; for help with this.&lt;\/p&gt;\n"},{"Id":"2504623","ParentId":"2504591","CreationDate":"2010-03-24T00:32:59.920","OwnerUserId":"28401","Tags":[],"Body":"&lt;p&gt;There is a &lt;a href=&quot;http:\/\/github.com\/samus\/mongodb-csharp\/tree\/master\/MongoDB.Linq\/&quot; rel=&quot;nofollow&quot;&gt;LINQ Provider&lt;\/a&gt; for &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; (another NoSQL DB) - haven't had any experience with it yet. The Provider is just a part of a complete .NET library for accessing MongoDB.&lt;\/p&gt;\n"},{"Id":"2504846","ParentId":"2504833","CreationDate":"2010-03-24T01:48:21.443","OwnerUserId":"3055","Tags":[],"Body":"&lt;p&gt;Well &quot;Fast, networked and language-independent&quot; + &quot;few complex queries&quot; brings to mind the various NoSQL solutions. To name a few:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/couchdb.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;\/a&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/cassandra.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;Cassandra&lt;\/a&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;And if that's not fast enough, there are always the wicked fast &lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;Redis&lt;\/a&gt; which is my personal favorite atm. :) It is not a database per se, but it's good enough for most scenarios.&lt;\/p&gt;\n\n&lt;p&gt;I am sure other people can list more NoSQL databases...&lt;br&gt;\nand there is always &lt;strong&gt;&lt;a href=&quot;http:\/\/nosql-database.org\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/nosql-database.org\/&lt;\/a&gt;&lt;\/strong&gt; .&lt;\/p&gt;\n\n&lt;p&gt;Generally speaking, databases in this category is better and faster in your scenario because they have relaxed constraints and thus is easier and faster to insert\/update\/retrieve frequently. But that requires that you think harder about your data model and it is generally not possible to do SQL-style complex queries directly -- you'll instead write more pre-computed data or use a more denormalized design to account for the lack of complex queries.&lt;\/p&gt;\n\n&lt;p&gt;But since complex queries is a minor problem in your case, I think NoSQL solutions are ideal for you.&lt;\/p&gt;\n"},{"Id":"2504851","ParentId":"2504833","CreationDate":"2010-03-24T01:49:51.317","OwnerUserId":"636","Tags":[],"Body":"&lt;p&gt;Have you done any sort of end-to-end profiling of your application and MySQL database? To provide better advice it would also be good to understand what improvements you have tried to implement, and your database structure. You haven't given a lot of information on how your MySQL database is configured either. It provides a lot of options for tuning.&lt;\/p&gt;\n\n&lt;p&gt;You should pick up a copy of &lt;a href=&quot;http:\/\/oreilly.com\/catalog\/9780596101718&quot; rel=&quot;nofollow&quot;&gt;High Performance MySQL&lt;\/a&gt; if you haven't already to learn more about the product.&lt;\/p&gt;\n\n&lt;p&gt;There is no point in doing anything until you know what your problem is. NoSQL solutions can offer performance benefits but you have provided little evidence that MySQL is incapable of servicing your needs.&lt;\/p&gt;\n"},{"Id":"2506556","ParentId":"2505900","CreationDate":"2010-03-24T09:36:02.723","OwnerUserId":"244618","Tags":[],"Body":"&lt;p&gt;Your data seems ideal for document oriented databases.&lt;br&gt;\nDocument example:&lt;br&gt;\n&lt;code&gt;{&lt;br&gt;\n&quot;type&quot;:&quot;Album&quot;,&lt;br&gt;\n&quot;artist&quot;:&quot;ArtistName&quot;,&lt;br&gt;\n&quot;album_name&quot;:&quot;AlbumName&quot;,&lt;br&gt;\n&quot;songs&quot; : [&lt;br&gt;\n {&quot;title&quot;:&quot;SongTitle&quot;,&quot;duration&quot;:4.5}&lt;br&gt;\n],&lt;br&gt;\n&quot;genres&quot;:[&quot;rock&quot;,&quot;indie&quot;]&lt;br&gt;\n}&lt;\/code&gt;  &lt;\/p&gt;\n\n&lt;p&gt;And replication is one of couchDB coolest features ( &lt;a href=&quot;http:\/\/blog.couch.io\/post\/468392274\/whats-new-in-apache-couchdb-0-11-part-three-new&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blog.couch.io\/post\/468392274\/whats-new-in-apache-couchdb-0-11-part-three-new&lt;\/a&gt; )&lt;br&gt;\nYou might also wanna take a look at Riak.&lt;\/p&gt;\n"},{"Id":"2507095","ParentId":"2505900","CreationDate":"2010-03-24T11:07:47.483","OwnerUserId":"5296","Tags":[],"Body":"&lt;p&gt;This kind of information is ideally suited to document databases.  As with much real-world data, it is not inherently relational, so shoe-horning it into a relational schema will bring headaches down the line (even using an ORM - I speak from experience).  Ubuntu already uses CouchDB for storing music metadata, as well as other things, in their &lt;a href=&quot;https:\/\/one.ubuntu.com\/&quot; rel=&quot;nofollow&quot;&gt;One product&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Taking the remainder of your questions one-by-one:&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Horizontal scaling is &lt;em&gt;WAY&lt;\/em&gt; easier than with RDBMS.  This is one of the many reasons big sites like Facebook, Digg and LinkedIn are using, or are actively investigating, schema-less databases.  For example, sharding (dividing your data across different nodes in a system) works beautifully thanks to a concept called &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Eventual_consistency&quot; rel=&quot;nofollow&quot;&gt;Eventual Consistency&lt;\/a&gt;; i.e., the data may be inconsistent across nodes for a while, but it will eventually resolve to a consistent state.  &lt;\/li&gt;\n&lt;li&gt;It depends what you mean by &quot;manage&quot;...  Installation is generally quick and easy to complete.  There are no user accounts to configure and secure (this is instead generally done in the application's business logic layer).  Working with a document DB in real time can be interesting: there's no ad hoc querying in CouchDB, for example; you have to use the Futon UI or communicate with it via HTTP requests.  MongoDB, however, does support ad hoc querying. &lt;\/li&gt;\n&lt;li&gt;I shouldn't think so.  Bastien's answer provides a good example of a JSON document serialising some data.  The beauty of schemaless DBs is that fields can be missing from one document and present in another, or the documents can be completely different from one another.  This removes many of the problems involved with RDBMS' &lt;code&gt;null&lt;\/code&gt; value, which are many and varied.  &lt;\/li&gt;\n&lt;li&gt;Yes; the associations are stored as nested documents, which are parsed in your application as object references, collections, etc.  In Bastien's answer, the &quot;songs&quot; key identifies an array of song documents.  &lt;\/li&gt;\n&lt;li&gt;This is very similar to your first question about horizontal scaling (horizontal scaling and replication are intertwined).  As the CouchIO blog post Bastien mentioned states, &quot;Replication &amp;#38;hellip; has been baked into CouchDB from the beginning.&quot;.  My understanding is that all document databases handle replication well, and do so more easily than it is to set it up in an RDBMS.  &lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Were you to decide you wanted to store the song file itself along with the metadata, you could do that too in CouchDB, by supplying the song file as an attachment to the document; further more, you wouldn't have any schema inconsistencies as a result of doing this, because there is no schema!  &lt;\/p&gt;\n\n&lt;p&gt;I hope I haven't made too many missteps here; I'm quite new to document DBs myself.  &lt;\/p&gt;\n"},{"Id":"2507634","ParentId":"2505900","CreationDate":"2010-03-24T12:38:50.580","OwnerUserId":"207036","Tags":[],"Body":"&lt;p&gt;Regarding horizontal scaling read this &lt;a href=&quot;http:\/\/spyced.blogspot.com\/2008\/12\/couchdb-not-drinking-kool-aid.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2508839","ParentId":"2501583","CreationDate":"2010-03-24T15:13:52.563","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Please submit bug reports to &lt;a href=&quot;https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA&quot; rel=&quot;nofollow&quot;&gt;https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA&lt;\/a&gt;, not SO.&lt;\/p&gt;\n"},{"Id":"2509361","ParentId":"2504833","CreationDate":"2010-03-24T16:17:38.163","OwnerUserId":"116805","Tags":[],"Body":"&lt;p&gt;With the data you've given about your application's data and workload, it is almost impossible to determine whether the problem really is MySQL itself or something else. You seem to assume that you can throw any workload to a relational engine and it should handle it. Therefore the suggestions made by other commenters about analyzing the performance more carefully are valid in my opinion. Without more data (transactions \/ second etc.) any further analysis regarding other suitable engines is also futile.&lt;\/p&gt;\n"},{"Id":"2509482","ParentId":"2504833","CreationDate":"2010-03-24T16:32:55.093","OwnerUserId":"197229","Tags":[],"Body":"&lt;p&gt;I'm not sure I agree with the advice to jump ship on traditional databases. It might not be the most efficient tool, but it is the one that is FAR more widely understood and used, and a strongly doubt you have a problem that can't be handled by an efficiently set up relational database.\nObvious answers are Oracle, SQLServer, etc, but it might just be your database structure isn't right. I don't know much about MySQL but I do know it's used in some pretty big projects (eBay being noteworthy). &lt;\/p&gt;\n"},{"Id":"2510666","ParentId":"2510627","CreationDate":"2010-03-24T19:08:27.697","OwnerUserId":"158595","Tags":[],"Body":"&lt;p&gt;It kind of depends on what sorts of analysis you are going to be doing on these stats. If you are going to be doing a lot of different operations (averaging, summing, joining...) you may find NoSQL solutions to be more of a pain then they are worth.&lt;\/p&gt;\n\n&lt;p&gt;However, if you are storing stats mostly for a display purpose, or for very specific analysis routines, NoSQL solutions start to shine.&lt;\/p&gt;\n\n&lt;p&gt;If your data is small enough, stick with a SQL solution, which will give the benefit of a full query engine to work with, but if you have lots of values (one value a day is nothing, even if you were running for a million years), and are worried about storage size and performance, NoSQL options once again may be worth it.&lt;\/p&gt;\n\n&lt;p&gt;If your data is semi-structured, take a look at CouchDB, which offers some rudimentary indexing and querying support, which could provide some basis for analysis routines. If you are storing individual values with very little structure, my best advice would be to take a look at Tokyo Cabinet and Tokyo Tyrant, which are absolutely incredible options for key-value storage.&lt;\/p&gt;\n"},{"Id":"2510680","ParentId":"2510627","CreationDate":"2010-03-24T19:09:55.340","OwnerUserId":"131070","Tags":[],"Body":"&lt;p&gt;NoSQL systems tend to optimize the case where data is stored frequently, but accessed infrequently.  In the case of statistics, you might gather lots of data from a (social) site frequently in small bits, which is optimized for. But retrieval and analysis might be slower...  It of course depends on which &quot;NoSql&quot; System you decide to use.&lt;\/p&gt;\n"},{"Id":"2516839","ParentId":"2516752","CreationDate":"2010-03-25T15:26:45.020","OwnerUserId":"110469","Tags":[],"Body":"&lt;p&gt;I'd suggest the following given that you are looking at multiple options [SQL or NoSQL]. While reading up on magento I came across &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Entity-attribute-value_model&quot; rel=&quot;nofollow&quot;&gt;http:\/\/en.wikipedia.org\/wiki\/Entity-attribute-value_model&lt;\/a&gt; which makes sense when you have a large number of attributes [columns in day to language] of which most will be null. Read up the wiki page and note the part that specifically related to lab reports.&lt;\/p&gt;\n"},{"Id":"2517105","ParentId":"2516752","CreationDate":"2010-03-25T15:57:09.990","OwnerUserId":"146325","Tags":[],"Body":"&lt;p&gt;Perhaps the original NoSQL database was MUMPS, which dates from before Codd devised his rules (i.e. the 1960s).  As the name implies  (*M*assachusetts General Hospital *U*tility *M*ulti-*P*rogramming *S*ystem),  its original purpose was the storing of medical documents.  Apparently MUMPS is still in use in some healthcare systems and other environments.  &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/MUMPS&quot; rel=&quot;nofollow&quot; title=&quot;Wikipedia article&quot;&gt;Find out more.&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;But as for the more recent rash of NoSQL databases I would be suprised if there were any implementations - yet.  Most of these products are still extremely beta and - being largely open source - lacking in support.  Medical apps are inevitably going to be extremely conservative, because people could die if the IT system fouls up.   &lt;\/p&gt;\n"},{"Id":"2520965","ParentId":"2520791","CreationDate":"2010-03-26T03:15:26.017","OwnerUserId":"108056","Tags":[],"Body":"&lt;p&gt;I would say two things.&lt;\/p&gt;\n\n&lt;p&gt;First my answer to your question. Secondly what I think you should do instead.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Answer:&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;p&gt;SQL, its easy to develop and test + production for some time. \nA table for Players, with INT or some other uniq value, not strings. (I know you said its a sample, but go for &quot;long word&quot; ints that ought to give you enough unique ID's\nSame goes for Game. Now the thing to keep the highscores together would be to have a relation between the two.&lt;\/p&gt;\n\n&lt;p&gt;&lt;em&gt;Score (Table relation):&lt;\/em&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[Player ID][Game_ID][Score]\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Where score is a numeric value... I dont know the max score of each of your games, so you figure out what type is enough.&lt;\/p&gt;\n\n&lt;p&gt;Now, this should be quite easy to implement for a start. Get that to work. But dont make every call directly to the database.&lt;\/p&gt;\n\n&lt;p&gt;Make a 3-TIER architecture. Make a datalayer and a businesslayer and then the &quot;game&quot; layer.\nSo every game calls the businesslayer with its own &quot;game ID&quot; like:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PlayerSaveScore(int gameID, int playerID, int score)\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;The Businesslayer then checks that the &quot;parameters&quot; are of the correct size and are valid ID's, perhaps validates that this player actual has been in a session the past 5 minutes etc.&lt;\/p&gt;\n\n&lt;p&gt;After validation, then the Businesslayer calles the datalayer for &quot;update table&quot; where the datalayer first looks if the record exists. IF not, then it inserts it.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tier design&lt;\/strong&gt;\nOnce you are &quot;online&quot; (in air) and the games becomes popular, then you can start to &quot;upgrade&quot;, but you are still able to get going now with a &quot;furture scaleable solution&quot;. Just remember that EVERY game MUST call to the business object\/layer, not directly - NEVER!&lt;\/p&gt;\n\n&lt;p&gt;I've been in the same &quot;thought ooh so many times&quot; but I kept getting into one simple loop called preparation, but that has almost never gotten me into a realistic solution thats up and running fast.&lt;\/p&gt;\n\n&lt;p&gt;So get 100000 players first! then start worrying when it grows beyond.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Part to... how to scale... suggestion:&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;p&gt;So here is my reason for all the trouble of building the &quot;businesslayer\/webservices&quot;... \nAnd best of all, your speed problems can be solved nicely now.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;You can implement &quot;cache&quot; quite simple.&lt;\/strong&gt;&lt;\/p&gt;\n\n&lt;p&gt;You make an extra table, if you only have 15 games, you dont need a table pr. game, but you decide. That one ONLY keeps the TOP 100 of each game. each time you post a new record from a player, you make a select on this &quot;top 100&quot; and checks if the posted value comes into the list. if it does, then handle that by updating the top 100 table and for extra speed purpose. &lt;\/p&gt;\n\n&lt;p&gt;Build the extract of Top 100 as a static datalist, eg. XML or similar static data. Depending on your platform, you pick the right &quot;static format&quot; for you.&lt;\/p&gt;\n\n&lt;p&gt;You could even improve speed further. Just keep the smallest value needed to get on top 100 of each game. That would be a record pr. game.&lt;\/p&gt;\n\n&lt;p&gt;Then match the player score against the game's &quot;lowest score in top 100&quot;... if its above, then you have some &quot;caching\/indexing&quot; to do and THEN you call the &quot;giant sort&quot; :o)&lt;\/p&gt;\n\n&lt;p&gt;Get the point? I know its a very long answer, but I wanted to post you a &quot;complete&quot; solution. &lt;\/p&gt;\n\n&lt;p&gt;Hope you will mark this as your answer :o)&lt;\/p&gt;\n"},{"Id":"2522389","ParentId":"2520791","CreationDate":"2010-03-26T10:12:33.873","OwnerUserId":"28875","Tags":[],"Body":"&lt;p&gt;I don't see why this can't be solved with one score table and simple SQL queries:&lt;\/p&gt;\n\n&lt;p&gt;(Untested pseudo-SQL)&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;create table scores {\n  player_id as integer,\n  game_id as integer,\n  score as integer\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;most played games: &lt;code&gt;SELECT count(*) AS c FROM scores GROUP BY game_id ORDER BY c DESC&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;best player: &lt;code&gt;SELECT sum(score) AS s FROM scores GROUP BY player_id ORDER BY s DESC&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;best player in a given game: &lt;code&gt;SELECT * FROM scores WHERE score=(SELECT max(score) FROM scores WHERE game_id=$given_game) LIMIT 1&lt;\/code&gt;&lt;\/p&gt;\n\n&lt;p&gt;If you need to get a list of the best players across all games simultaneously, you can extend that last query a little (which can probably be optimised with a join, but it's too early for me to think that through right now).&lt;\/p&gt;\n\n&lt;p&gt;The number of rows you're talking about is tiny in database terms. If you cache the query results as well (eg. via something like memcached, or within your RoR application) then you'll barely touch the database at all for this.&lt;\/p&gt;\n"},{"Id":"2523733","ParentId":"2476280","CreationDate":"2010-03-26T13:54:39.500","OwnerUserId":"138041","Tags":[],"Body":"&lt;p&gt;You may check a real example of a working twitter simple clone &lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/twissandra.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/twissandra.com\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;with source codes at &lt;a href=&quot;http:\/\/github.com\/ericflo\/twissandra&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ericflo\/twissandra&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;Here's another good article explaining cassandra data model:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/arin.me\/blog\/wtf-is-a-supercolumn-cassandra-data-model&quot; rel=&quot;nofollow&quot;&gt;http:\/\/arin.me\/blog\/wtf-is-a-supercolumn-cassandra-data-model&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2526222","ParentId":"2526192","CreationDate":"2010-03-26T19:40:23.507","OwnerUserId":"44861","Tags":[],"Body":"&lt;p&gt;I have used &lt;a href=&quot;http:\/\/www.mongodb.org&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt; on Windows and the install went pretty smoothly.  I haven't put a real heavy load on it yet.&lt;\/p&gt;\n\n&lt;p&gt;And you can find a &lt;a href=&quot;http:\/\/github.com\/robconery\/NoRM&quot; rel=&quot;nofollow&quot;&gt;.NET driver here&lt;\/a&gt; for Mongo if that's your platform&lt;\/p&gt;\n"},{"Id":"2528683","ParentId":"2527173","CreationDate":"2010-03-27T08:54:33.337","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;Cassandra supports map reduce since version 0.6. (Current stable release is 0.5.1, but go ahead and try the new map reduce functionality in 0.6.0-beta3) To get started I recommend to take a look at the word count map reduce example in 'contrib\/word_count'.&lt;\/p&gt;\n"},{"Id":"2528749","ParentId":"2527173","CreationDate":"2010-03-27T09:27:26.933","OwnerUserId":"295964","Tags":[],"Body":"&lt;p&gt;MongoDB has update-in-place, so MongoDB should be very good with counters. &lt;a href=&quot;http:\/\/blog.mongodb.org\/post\/171353301\/using-mongodb-for-real-time-analytics&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blog.mongodb.org\/post\/171353301\/using-mongodb-for-real-time-analytics&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2528954","ParentId":"2527682","CreationDate":"2010-03-27T10:47:26.120","OwnerUserId":"74496","Tags":[],"Body":"&lt;p&gt;Add &lt;code&gt;use Encode;&lt;\/code&gt; to the beginning of your script, and pass variables through &lt;code&gt;Encode::decode_utf8&lt;\/code&gt;. For example:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;my $get_value = $result-&amp;#38;gt;{$key}-&amp;#38;gt;{&quot;Standard1&quot;}-&amp;#38;gt;{$column};\n$get_value = Encode::decode_utf8($get_value);\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Outputs:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OK: \u2603 == \u2603\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;When you set &lt;code&gt;$set_value&lt;\/code&gt; to &quot;\\x{2603}&quot;, Perl detects the wide character and sets the string encoding to UTF-8 for you. To confirm this, print the return value of &lt;code&gt;Encode::is_utf8($set_value)&lt;\/code&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Unfortunately, once this string goes into Cassandra and back out again, the encoding information is lost. It appears that Cassandra is encoding-agnostic. Calling &lt;code&gt;Encode::decode_utf8&lt;\/code&gt; tells Perl that you have a string containing a UTF-8 byte sequence, and that it should be converted into Perl's internal representation for Unicode. As jrockway points out, you should also call &lt;code&gt;Encode::encode_utf8&lt;\/code&gt; on any strings before they are sent to Cassandra, although in most cases Perl already knows they are UTF-8, for example if you've opened a file with the &lt;code&gt;:utf8&lt;\/code&gt; encoding layer.&lt;\/p&gt;\n\n&lt;p&gt;If you use UTF-8 often, you might want to write a wrapper over Net::Cassandra::Easy to do this automatically.&lt;\/p&gt;\n\n&lt;p&gt;Finally, you don't need &lt;code&gt;use utf8;&lt;\/code&gt; unless your Perl &lt;em&gt;source code&lt;\/em&gt; (variable &lt;em&gt;names&lt;\/em&gt;, comments etc.) contains UTF-8 characters. Perl can handle UTF-8 &lt;em&gt;strings&lt;\/em&gt; whether you specify &lt;code&gt;use utf8;&lt;\/code&gt; or not.&lt;\/p&gt;\n"},{"Id":"2530300","ParentId":"2529871","CreationDate":"2010-03-27T17:56:08.307","OwnerUserId":"271959","Tags":[],"Body":"&lt;p&gt;All different products and they all have their pro's and conn's. What kind of problem do you have to solve?&lt;\/p&gt;\n\n&lt;p&gt;Huge, as in TB's?&lt;\/p&gt;\n"},{"Id":"2530354","ParentId":"2529871","CreationDate":"2010-03-27T18:09:27.683","OwnerUserId":"146325","Tags":[],"Body":"&lt;p&gt;There are many different flavours of &quot;NoSQL&quot; databases.  If your application is really like &lt;a href=&quot;http:\/\/wordnet.princeton.edu\/&quot; rel=&quot;nofollow&quot;&gt;Wordnet&lt;\/a&gt; perhaps you should look at a graph database such as &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2531507","ParentId":"2526634","CreationDate":"2010-03-28T00:50:18.183","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;I would just update the user count as a batch operation every N minutes rather than updating it in realtime.  If there's only one process updating it, you don't need to worry about contention by definition.&lt;\/p&gt;\n\n&lt;p&gt;Alternatively cassandra has a contrib\/mutex for adding lock support via ZooKeeper.&lt;\/p&gt;\n"},{"Id":"2532316","ParentId":"2526634","CreationDate":"2010-03-28T08:31:58.950","OwnerUserId":"295964","Tags":[],"Body":"&lt;p&gt;MongoDB has update-in-place and a special inc operator for counters. &lt;a href=&quot;http:\/\/blog.mongodb.org\/post\/171353301\/using-mongodb-for-real-time-analytics&quot; rel=&quot;nofollow&quot;&gt;http:\/\/blog.mongodb.org\/post\/171353301\/using-mongodb-for-real-time-analytics&lt;\/a&gt; &lt;\/p&gt;\n"},{"Id":"2533077","ParentId":"2529871","CreationDate":"2010-03-28T13:37:05.483","OwnerUserId":"297094","Tags":[],"Body":"&lt;p&gt;I would suggest to analyse your request.&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If you are going with more clusters, machines take NoSQL&lt;\/li&gt;\n&lt;li&gt;If your data model is complicated - require efficient structures take NoSQL (no limits with type of columns)&lt;\/li&gt;\n&lt;li&gt;If you fit in a few machines without scales, and you don't need super performance for multi request (as for example in social network - where lot of users send http request), and you don't think you involve saleability take RDBMS (Postgres have some good functions and structures which you can use, like array column type).&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Cassandra should work better with large scales of data, multi purpose.\nneo4j - would be better for special structures, graphs.&lt;\/p&gt;\n"},{"Id":"2534467","ParentId":"2534408","CreationDate":"2010-03-28T20:40:25.827","OwnerUserId":"81106","Tags":[],"Body":"&lt;p&gt;what raised this issue that if you have a large server farm and need to manage the distribution of your data and load balancing which is more difficult and harder to implement using RDBMS and requires high IT skills to design, plan and deploy for your solution (and still performance is less).\nbut if you have only 3 or 4 servers with small project. I don't think you have an issue about it. NoSQL database is usually considered in large server farms not small number of servers&lt;\/p&gt;\n"},{"Id":"2534505","ParentId":"2534408","CreationDate":"2010-03-28T20:50:35.377","OwnerUserId":"183402","Tags":[],"Body":"&lt;p&gt;Yes, sometimes RDBMS are not the best solution, although there are ways to accomodate user defined fields (see XML Datatype, EAV design pattern, or just have spare generic columns) sometimes a schema free database is a good choice.  &lt;\/p&gt;\n\n&lt;p&gt;However, you need to nail down your requirements before choosing to go with a document database, as you will loose a lot of the power you may be used to with the relational model &lt;\/p&gt;\n\n&lt;p&gt;eg...&lt;\/p&gt;\n\n&lt;p&gt;If you would otherwise have multiple tables in your RDBMS database, you will need to research the features MongoDB affords you to accomodate these needs.  &lt;\/p&gt;\n\n&lt;p&gt;If you will need to query the data in specific ways, again you need to research what MongoDB offers you.  &lt;\/p&gt;\n\n&lt;p&gt;I wouldnt think of NoSQL as replacement for RDBMS, rather a slightly different tool that brings its own sets of advantages and disadvantages making it more suitable for some projects than others.  &lt;\/p&gt;\n\n&lt;p&gt;(Both databases may be used in some circumstances. Also if you decide to go down the route of possibly using MongoDB, once you have researched the websites out there and have more specific questions, you can visit Freenode IRC #mongodb channel)&lt;\/p&gt;\n"},{"Id":"2535643","ParentId":"2529871","CreationDate":"2010-03-29T03:25:18.737","OwnerUserId":"125487","Tags":[],"Body":"&lt;p&gt;Cassandra and other NoSQL stores are being used for social based sites because of their need for massive write based operations. Not that MySQL and Postgres can't achieve this but NoSQL requires far less time and money, generally speaking.&lt;\/p&gt;\n\n&lt;p&gt;Sounds like you may want to look at Neo4J though, just in terms of your object model needs.&lt;\/p&gt;\n"},{"Id":"2538400","ParentId":"2529871","CreationDate":"2010-03-29T13:58:54.887","OwnerUserId":"271959","Tags":[],"Body":"&lt;p&gt;A nice article about the NoSQL-movement: &lt;a href=&quot;http:\/\/teddziuba.com\/2010\/03\/i-cant-wait-for-nosql-to-die.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/teddziuba.com\/2010\/03\/i-cant-wait-for-nosql-to-die.html&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2540140","ParentId":"2443381","CreationDate":"2010-03-29T18:00:19.757","OwnerUserId":"24279","Tags":[],"Body":"&lt;p&gt;I have used PostGres (free open source db) with PostGIS extensions for working with location data.  They are extremely good, even though I was working in an MS environment with all production databases using MSSQL 2005, I used PostGres w\/GIS to manipulate and precalculate a lot of geographic data.  &lt;\/p&gt;\n\n&lt;p&gt;PostGis has utilities for import arcview .shp files which is a huge plus since that's how most geographic data is present.  It also provides a host of location based sql functions like contains( ... ) and near( ... ); and it provides a mechanism for indexing spatial data.&lt;\/p&gt;\n\n&lt;p&gt;It's been awhile since I used it, but I remember it being rock solid and very useful.&lt;\/p&gt;\n\n&lt;p&gt;PostGIS: &lt;a href=&quot;http:\/\/postgis.refractions.net\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/postgis.refractions.net\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2540290","ParentId":"2539958","CreationDate":"2010-03-29T18:26:49.963","OwnerUserId":"18315","Tags":[],"Body":"&lt;p&gt;Marc-Andre,&lt;\/p&gt;\n\n&lt;p&gt;Your best bet is to ask on the mailing list: &lt;a href=&quot;http:\/\/lists.basho.com\/mailman\/listinfo\/riak-users_lists.basho.com&quot; rel=&quot;nofollow&quot;&gt;http:\/\/lists.basho.com\/mailman\/listinfo\/riak-users_lists.basho.com&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;There's also lots of information on the wiki: &lt;a href=&quot;http:\/\/wiki.basho.com\/display\/RIAK\/Home&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.basho.com\/display\/RIAK\/Home&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2540458","ParentId":"2539958","CreationDate":"2010-03-29T18:53:55.857","OwnerUserId":"10715","Tags":[],"Body":"&lt;p&gt;I'm not a Riak expert, but if you keep all of your EC2 instances in the same availability zone you should get more than adequate performance.  AWS has a gigabit internal network and people have been able to get the full gigabit out of it; see &lt;a href=&quot;http:\/\/blog.rightscale.com\/2007\/10\/28\/network-performance-within-amazon-ec2-and-to-amazon-s3\/&quot; rel=&quot;nofollow&quot;&gt;this blog post&lt;\/a&gt; for an example.&lt;\/p&gt;\n"},{"Id":"2541325","ParentId":"2541273","CreationDate":"2010-03-29T21:13:42.967","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;Take a look at:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;list&amp;#38;lt;KeySlice&amp;#38;gt; get_range_slices(keyspace, column_parent, predicate, range, consistency_level)\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Where your KeyRange tuple is (start_key, end_key) == (r1, r10)&lt;\/p&gt;\n"},{"Id":"2547181","ParentId":"2546948","CreationDate":"2010-03-30T17:20:12.470","OwnerUserId":"133858","Tags":[],"Body":"&lt;p&gt;Its all about using effective indexes. \nIf you have a special query, make an index for that query.&lt;\/p&gt;\n\n&lt;p&gt;Ex. make an index age_lovebutterflies_hateschoclate&lt;\/p&gt;\n\n&lt;p&gt;If you have a high-traffic site like facebook, you would need more power than only sql optimizing. E.g memcaching, implemented a search-engine like vespa or lucene\/solr  implemented as clusters. Loadbalancers, multiple servers with 64gb ram, raid disks, and lots of other server technologies... &lt;\/p&gt;\n\n&lt;p&gt;Problem with databases like mysql, postgresql, sqlite and oracle is that indexes ok for static searches, but they are not flexible. Ex if you would like to combine searches over columns that are not indexed, no indexes will be used. Ex. if you include an additional parameter like gender or maybe another like location.. you would have to create more indexes... lucene\/solr and a real search engine is much more effective in this way, as you can make as many combination you would like... All you have to think about is that a column is indexed, not what other columns its indexed together with...&lt;\/p&gt;\n\n&lt;p&gt;So, Facebook...It's a long way to go dude ;)&lt;\/p&gt;\n"},{"Id":"2547740","ParentId":"2546825","CreationDate":"2010-03-30T18:43:46.903","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;&quot;&lt;strong&gt;&lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/HintedHandoff&quot; rel=&quot;nofollow&quot;&gt;Hinted handoff&lt;\/a&gt;&lt;\/strong&gt; means that if a node that should receive a write is down, Cassandra will send that write to another node with a &quot;hint&quot; saying that when the destination node becomes available again, the write should be forwarded there.&quot;&lt;\/p&gt;\n\n&lt;p&gt;After the answer on question two I believe the third question becomes obsolete (?)&lt;\/p&gt;\n\n&lt;p&gt;And finally the first question:\nThe token, cluster name, and whether the node is bootstrapped or not will be stored in system. (thanks driftx)&lt;\/p&gt;\n"},{"Id":"2547768","ParentId":"2540040","CreationDate":"2010-03-30T18:47:55.440","OwnerUserId":"1012","Tags":[],"Body":"&lt;p&gt;check out this file in lazyboy:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/digg\/lazyboy\/blob\/master\/lazyboy\/iterators.py&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/digg\/lazyboy\/blob\/master\/lazyboy\/iterators.py&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;it has a few different range methods. &lt;\/p&gt;\n\n&lt;p&gt;E.g., line 121: def key_range(key, start=&quot;&quot;, finish=&quot;&quot;, count=100):&lt;\/p&gt;\n\n&lt;p&gt;or you could use this when you need to apply slice predicates:&lt;\/p&gt;\n\n&lt;p&gt;def slice_iterator(key, consistency, **predicate_args):&lt;\/p&gt;\n\n&lt;p&gt;HTH&lt;\/p&gt;\n"},{"Id":"2554024","ParentId":"2552985","CreationDate":"2010-03-31T15:01:56.087","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;You'd have to scan all the rows and grab the timestamp from the column(s) you're interested in.  If this is something you run every day or so, doing this in a Hadoop job should be fine.  If it's something you run every few minutes, then you'll need to come up with another approach.&lt;\/p&gt;\n"},{"Id":"2559472","ParentId":"2559411","CreationDate":"2010-04-01T09:46:42.867","OwnerUserId":"17028","Tags":[],"Body":"&lt;p&gt;Here's a quote from a recent &lt;a href=&quot;http:\/\/www.25hoursaday.com\/weblog\/2010\/03\/29\/TheNoSQLDebateAutomaticVsManualTransmission.aspx&quot; rel=&quot;nofollow&quot;&gt;blog post from Dare Obasanjo&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;SQL databases are like automatic\n  transmission and NoSQL databases are\n  like manual transmission. Once you\n  switch to NoSQL, you become\n  responsible for a lot of work that the\n  system takes care of automatically in\n  a relational database system. Similar\n  to what happens when you pick manual\n  over automatic transmission. Secondly,\n  NoSQL allows you to eke more\n  performance out of the system by\n  eliminating a lot of integrity checks\n  done by relational databases from the\n  database tier. Again, this is similar\n  to how you can get more performance\n  out of your car by driving a manual\n  transmission versus an automatic\n  transmission vehicle.&lt;\/p&gt;\n  \n  &lt;p&gt;However the most notable similarity is\n  that just like most of us can\u2019t really\n  take advantage of the benefits of a\n  manual transmission vehicle because\n  the majority of our driving is sitting\n  in traffic on the way to and from\n  work, there is a similar harsh reality\n  in that most sites aren\u2019t at Google or\n  Facebook\u2019s scale and thus have no need\n  for a Bigtable or Cassandra.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;To which I can add only that switching from MySQL, where you have at least some experience, to CouchDB, where you have no experience, means you will have to deal with a whole new set of problems and learn different concepts and best practices. While by itself this is wonderful (I am playing at home with MongoDB and like it a lot), it will be a cost that you need to calculate when estimating the work for that project, and brings unknown risks while promising unknown benefits. It will be very hard to judge if you can do the project on time and with the quality you want\/need to be successful, if it's based on a technology you don't know.&lt;\/p&gt;\n\n&lt;p&gt;Now, if you have on the team an expert in the NoSQL field, then by all means take a good look at it. But without any expertise on the team, don't jump on NoSQL for a new commercial project.&lt;\/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Update&lt;\/strong&gt;: Just to throw some gasoline in the open fire you started, here are two interesting articles from people on the SQL camp. :-)&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/teddziuba.com\/2010\/03\/i-cant-wait-for-nosql-to-die.html&quot; rel=&quot;nofollow&quot;&gt;I Can't Wait for NoSQL to Die&lt;\/a&gt;&lt;br&gt;\n&lt;a href=&quot;http:\/\/www.yafla.com\/dforbes\/The_Impact_of_SSDs_on_Database_Performance_and_the_Performance_Paradox_of_Data_Explodification&quot; rel=&quot;nofollow&quot;&gt;Fighting The NoSQL Mindset, Though This Isn't an anti-NoSQL Piece&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2559473","ParentId":"2559411","CreationDate":"2010-04-01T09:46:53.693","OwnerUserId":"274609","Tags":[],"Body":"&lt;p&gt;if you are looking for a highly scalable open source nosql solution check out cassandra or hbase&lt;\/p&gt;\n\n&lt;p&gt;edit: bad word choice, corrected, &quot;real&quot; has been replaced by &quot;highly scalable open source&quot;&lt;\/p&gt;\n"},{"Id":"2564372","ParentId":"2081080","CreationDate":"2010-04-01T23:59:41.900","OwnerUserId":"101909","Tags":[],"Body":"&lt;p&gt;Could you create a simple sqlite database with two columns:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;==documents==\nid|data\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;and data would be json data.&lt;\/p&gt;\n\n&lt;p&gt;You could also create a key table which would be:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;==keys==\nkeyname|keyvalue|id\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;that would be indexed on keyname and keyvalue for quick lookups.&lt;\/p&gt;\n\n&lt;p&gt;A single db file could be a collection, and you could create multiple db files for multiple collections.&lt;\/p&gt;\n\n&lt;p&gt;You could use folders as &quot;dbs&quot; to match mongodb's hierarchy of db-&gt;collection-&gt;document&lt;\/p&gt;\n"},{"Id":"2565451","ParentId":"2565417","CreationDate":"2010-04-02T07:03:44.050","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;I would recommend to take a look at &lt;a href=&quot;http:\/\/github.com\/tjake\/Lucandra&quot; rel=&quot;nofollow&quot;&gt;Lucandra&lt;\/a&gt; (Lucandra = &lt;a href=&quot;http:\/\/lucene.apache.org\/java\/docs\/&quot; rel=&quot;nofollow&quot;&gt;Lucene&lt;\/a&gt; + &lt;a href=&quot;http:\/\/cassandra.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;Cassandra&lt;\/a&gt; )&lt;\/p&gt;\n"},{"Id":"2566466","ParentId":"2501583","CreationDate":"2010-04-02T11:50:53.837","OwnerUserId":"58394","Tags":[],"Body":"&lt;p&gt;This bug is documented in &lt;a href=&quot;https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-934&quot; rel=&quot;nofollow&quot;&gt;CASSANDRA-934&lt;\/a&gt; and was fixed in Cassandra 0.6.1.&lt;\/p&gt;\n"},{"Id":"2567411","ParentId":"2562712","CreationDate":"2010-04-02T15:16:11.700","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;You create a CF whose keys are your categories, and whose columns are the articles in that category.  Then lookup-by-category is just another lookup-by-key.  Clients like lazyboy will automate this for you.&lt;\/p&gt;\n"},{"Id":"2567547","ParentId":"2443381","CreationDate":"2010-04-02T15:39:28.123","OwnerUserId":"108434","Tags":[],"Body":"&lt;p&gt;SQL Express is a free database + has suppport to store lat\/long &amp;#38;amp; dateTime&lt;\/p&gt;\n\n&lt;p&gt;just a tip - use UTC time for dateTime specially if your application needs to be aware of various locations...&lt;\/p&gt;\n\n&lt;p&gt;hth.&lt;\/p&gt;\n"},{"Id":"2569531","ParentId":"2568245","CreationDate":"2010-04-02T22:30:38.940","OwnerUserId":"247003","Tags":[],"Body":"&lt;p&gt;From wikipedia:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Document-oriented_database&quot; rel=&quot;nofollow&quot;&gt;Document data store&lt;\/a&gt;: As opposed to relational databases, document-based databases do not store data in tables with uniform sized fields for each record. Instead, each record is stored as a document that has certain characteristics. Any number of fields of any length can be added to a document. Fields can also contain multiple pieces of data.&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Key-value_pair&quot; rel=&quot;nofollow&quot;&gt;Key Value&lt;\/a&gt;: An associative array (also associative container, map, mapping, dictionary, finite map, and in query-processing an index or index file) is an abstract data type composed of a collection of unique keys and a collection of values, where each key is associated with one value (or set of values). The operation of finding the value associated with a key is called a lookup or indexing, and this is the most important operation supported by an associative array. The relationship between a key and its value is sometimes called a mapping or binding. For example, if the value associated with the key &quot;bob&quot; is 7, we say that our array maps &quot;bob&quot; to 7.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;More examples at &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/NoSQL&quot; rel=&quot;nofollow&quot;&gt;NoSQL&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2571210","ParentId":"2571098","CreationDate":"2010-04-03T12:13:20.140","OwnerUserId":"33890","Tags":[],"Body":"&lt;p&gt;Good question. I have heard and read a lot about NoSQL vs SQL, mostly from the NoSQL side. &lt;\/p&gt;\n\n&lt;p&gt;It looks a lot like the key-value databases that were there from a long time ago. It also looks like the Object-Oriented databases of the 1990s which were supposed to come and replace SQL. &lt;\/p&gt;\n\n&lt;p&gt;NoSQL has an advantage with respect to the &lt;a href=&quot;http:\/\/javathink.blogspot.com\/2010\/01\/characterizing-enterprise-systems-using.html&quot; rel=&quot;nofollow&quot;&gt;CAP Theorem&lt;\/a&gt;, which states that nice things about databases are consistency, availability, and partition tolerance, but you can only pick 2 of the 3. Relational databases give you consistency and availability, and a lot of popular NoSQL databases give you availability and partition tolerance. i.e. You can distribute data across many computers easily with NoSQL. So if you have an application (like a lot of Google's applications) that need to scale to a gajillion users, NoSQL is a better choice.   &lt;\/p&gt;\n\n&lt;p&gt;I think some of the advocacy of NoSQL comes from people who have gotten applications working quickly with NoSQL, and ask, &lt;em&gt;&quot;...relational databases? we don't need no stinkin' relational databases!&quot;&lt;\/em&gt;. NoSQL seems to be the way to go at Google. A friend of mine worked there for a while, and his comment was that Google folks advocate using NoSQL, but it requires a lot of code to dipsy-doodle around data in complex relationships. Problem domains with lots of joins and indices are harder to code with NoSQL plus code vs just SQL.  This works for Google because they aggressively recruit prolific coders. The other thing is that a lot of Google applications boil down to being huge lists of stuff that can be scattered across multiple machines. They achieve good query speed with their search indices, Google File System,  and &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/MapReduce&quot; rel=&quot;nofollow&quot;&gt;Map-Reduce&lt;\/a&gt;. Joins are not as much of a problem in those applications. &lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.youtube.com\/watch?v=LhnGarRsKnA&quot; rel=&quot;nofollow&quot;&gt;This video at YouTube&lt;\/a&gt; talks about NoSQL vs SQL. It's kind of funny if you are a SQL advocate, but it describes how you solve problems in NoSQL that were solved in SQL relational databases. &lt;\/p&gt;\n"},{"Id":"2571329","ParentId":"2571098","CreationDate":"2010-04-03T13:15:47.580","OwnerUserId":"50552","Tags":[],"Body":"&lt;p&gt;One of the things that's easy to overlook is that technologies are often associated with a specific development process.  For example, say you are hired at a company that has a settled team of database administrators.  They proudly guard a deliberate process of deploying and testing SQL.&lt;\/p&gt;\n\n&lt;p&gt;Now you've been given two months to implement a new procurement website, and there's just no way you could get there with the DBA team.  So you start looking for a way around the DBA team.  NoSQL can provide that.&lt;\/p&gt;\n\n&lt;p&gt;Being an alternative to SQL, while doing 60% of the things SQL does, is a powerful feature :)&lt;\/p&gt;\n"},{"Id":"2571377","ParentId":"2571098","CreationDate":"2010-04-03T13:43:07.233","OwnerUserId":"18255","Tags":[],"Body":"&lt;p&gt;It's rather like comparing bananas to mushrooms.&lt;\/p&gt;\n\n&lt;p&gt;There are very few kinds of bananas, very closely related, everyone knows what they taste like and they all peel the same way.  If you try to describe a kind of banana to someone without using the word banana, they'll probably understand and relate it to the banana they know pretty quickly.&lt;\/p&gt;\n\n&lt;p&gt;There are a wide variety of mushrooms, they all taste different, and you prepare them differently.  If you try to describe a kind of mushroom to someone without using the word mushroom, they may have no idea what you are talking about and try to make a broccoli dish with it.&lt;\/p&gt;\n\n&lt;p&gt;The relational model that almost all RDBMS conform to is fairly consistent, and the notion of referential integrity, ACID, constraints, relations (i.e. tables) and normal forms are well understood.  Modelling the data is typically important.  The idea of what a database is is well-defined and the boundary of the database's responsibility is well-defined.  Data is king.&lt;\/p&gt;\n\n&lt;p&gt;As far as NoSQL, about the only thing consistent with their various models is distributed processing, scalability and lots more code and not much of a unified query engine.  They are really databases in only the vaguest sense of an organized collection of data, like a folder of Excel spreadsheets is a database.  Rules can be in code or not or whatever.  Code is king.&lt;\/p&gt;\n\n&lt;p&gt;The problems that NoSQL systems are designed to solve are not the same problems that the relational model solves, it's horses for courses.&lt;\/p&gt;\n"},{"Id":"2571516","ParentId":"2571098","CreationDate":"2010-04-03T14:35:37.857","OwnerUserId":"38360","Tags":[],"Body":"&lt;p&gt;The original name of this technology before people started calling it &quot;NoSQL&quot; was a &lt;em&gt;distributed key\/value store&lt;\/em&gt;. This is a far more descriptive name, and I originally remember looking at it and going &quot;hey, cool, I'll bet that will end up being very useful to a lot of people.&quot;  The term has since expanded to essentially include &quot;anything that isn't a relational database&quot;, but usually, when most people talk about NoSQL, they are talking about key\/value stores.&lt;\/p&gt;\n\n&lt;p&gt;Ever since the term &lt;em&gt;NoSQL&lt;\/em&gt; was coined, it's been getting touted as a silver bullet.  I'm interested in products like Cassandra and follow their progress, but they are still immature technologies, and to claim that they are &quot;replacing&quot; SQL or RDBMSes in general (or that they will in the near future) is specious reasoning at best, if not an out-and-out lie.&lt;\/p&gt;\n\n&lt;p&gt;Products and technologies fitting under the NoSQL umbrella are geared toward the following problem domain:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You plan to deploy a large-scale, high-concurrency database (hundreds of GB, thousands of users);&lt;\/li&gt;\n&lt;li&gt;Which doesn't need ACID guarantees;&lt;\/li&gt;\n&lt;li&gt;Or relationships or constraints;&lt;\/li&gt;\n&lt;li&gt;Stores a fairly narrow set of data (the equivalent of 5-10 tables in SQL);&lt;\/li&gt;\n&lt;li&gt;Will be running on commodity hardware (i.e. Amazon EC2);&lt;\/li&gt;\n&lt;li&gt;Needs to be implemented on a very low budget and &quot;scaled out.&quot;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;This actually describes &lt;strong&gt;a lot&lt;\/strong&gt; of web sites today.  Google and Twitter fit very neatly into these requirements.  Does it really matter if a few tweets are lost or delayed?  On the other hand, these specs apply to nearly 0% of business systems, which is what a very high number of us work on developing.  Most businesses have very different requirements:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Medium-to-large-scale databases (10-100 GB) with fairly low concurrency (hundreds of users at most);&lt;\/li&gt;\n&lt;li&gt;ACID (especially the A and C - Atomicity and Consistency) is a &lt;em&gt;hard&lt;\/em&gt; requirement;&lt;\/li&gt;\n&lt;li&gt;Data is highly correlated (hierarchies, master-detail, histories);&lt;\/li&gt;\n&lt;li&gt;Has to store a wide assortment of data - hundreds or thousands of tables are not uncommon in a &lt;em&gt;normalized&lt;\/em&gt; schema (more for denormalization tables, data warehouses, etc.);&lt;\/li&gt;\n&lt;li&gt;Run on high-end hardware;&lt;\/li&gt;\n&lt;li&gt;Lots of capital available (if your business has millions of customers then you can probably find $25k or so lying &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Hyperbole&quot; rel=&quot;nofollow&quot;&gt;behind the couch&lt;\/a&gt;).&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;High-end SQL databases (SQL Server, Oracle, Teradata, Vertica, etc.) are designed for &lt;em&gt;vertical&lt;\/em&gt; scaling, they like being on machines with lots and lots of memory, fast I\/O through SANs and SSDs, and the occasional horizontal scaling through clustering (HA) and partitioning (HC).&lt;\/p&gt;\n\n&lt;p&gt;&quot;NoSQL&quot; is often compared favourably to &quot;SQL&quot; in performance terms.  But fully maxed-out, a high-end SQL database server or cluster will scale almost infinitely.  That is how they were intended to be deployed.  Beware of dubious benchmarks comparing poorly-normalized, poorly-indexed SQL databases running mysql on entry-level servers (or worse, cloud servers like Amazon EC2) to similarly-deployed NoSQL databases.  Apples and oranges.  If you work with SQL, don't be scared by that hype.&lt;\/p&gt;\n\n&lt;p&gt;SQL isn't going anywhere.  DBAs are no more likely to vanish as a result of NoSQL than PHP programmers were as a result of Java and XML.&lt;\/p&gt;\n\n&lt;p&gt;NoSQL isn't going anywhere either, because the development community has correctly recognized that RDBMSes aren't &lt;em&gt;always&lt;\/em&gt; the optimal solution to &lt;em&gt;every&lt;\/em&gt; problem.&lt;\/p&gt;\n\n&lt;p&gt;So, as a developer you owe it to yourself to at least learn what NoSQL is, what products it refers to (Cassandra, BigTable, Voldemort, db4o, etc.), and how to build and code against a simple database created with one of these.  But don't start throwing away all your SQL databases yet or thinking that your career is going to be made obsolete - that's hype, not reality.&lt;\/p&gt;\n"},{"Id":"2572635","ParentId":"2568245","CreationDate":"2010-04-03T20:51:07.247","OwnerUserId":"4243","Tags":[],"Body":"&lt;p&gt;In a document data store each record has multiple fields, similar to a relational database.  It also has secondary indexes.&lt;\/p&gt;\n\n&lt;p&gt;Example record:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&quot;id&quot; =&amp;#38;gt; 12345,\n&quot;name&quot; =&amp;#38;gt; &quot;Fred&quot;,\n&quot;age&quot; =&amp;#38;gt; 20,\n&quot;email&quot; =&amp;#38;gt; &quot;fred@example.com&quot;\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Then you could query by id, name, age, or email.&lt;\/p&gt;\n\n&lt;p&gt;A key\/value store is more like a big hash table than a traditional database: each key corresponds with a value and looking things up by that one key is the only way to access a record.  This means it's much simpler and often faster, but it's difficult to use for complex data.&lt;\/p&gt;\n\n&lt;p&gt;Example record:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;12345 =&amp;#38;gt; &quot;Fred,fred@example.com,20&quot;\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;You can only use 12345 for your query criteria.  You can't query for name, email, or age.&lt;\/p&gt;\n"},{"Id":"2573800","ParentId":"2573657","CreationDate":"2010-04-04T07:03:56.810","OwnerUserId":"69742","Tags":[],"Body":"&lt;p&gt;Ok, I've found two pages that are helpful &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Schema+Design&quot; rel=&quot;nofollow&quot;&gt;Schema Design&lt;\/a&gt; and &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/MongoDB+Data+Modeling+and+Rails&quot; rel=&quot;nofollow&quot;&gt;Data Modeling (a full application in RoR)&lt;\/a&gt; &lt;\/p&gt;\n\n&lt;p&gt;Also, the #mongodb channel on IRC is extremely helpful. The user &lt;code&gt;dacort&lt;\/code&gt; there helped me to find those very useful pages. &lt;\/p&gt;\n"},{"Id":"2574149","ParentId":"2573657","CreationDate":"2010-04-04T10:31:22.427","OwnerUserId":"43901","Tags":[],"Body":"&lt;p&gt;Here an explanation of the use of DBRefs in MongoDB: &lt;a href=&quot;http:\/\/valyagolev.net\/article\/mongo_dbref\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/valyagolev.net\/article\/mongo_dbref\/&lt;\/a&gt; &lt;\/p&gt;\n"},{"Id":"2574443","ParentId":"2571098","CreationDate":"2010-04-04T13:02:23.757","OwnerUserId":"184499","Tags":[],"Body":"&lt;p&gt;Try &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Home&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;\/a&gt;. You can find drivers and a tutorial for Java &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Java+Language+Center&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2574609","ParentId":"2571098","CreationDate":"2010-04-04T14:03:01.850","OwnerUserId":"121364","Tags":[],"Body":"&lt;p&gt;NoSQL does not refer to any single type of database system, but rather to any type of database system which is not relational. Asking for the &quot;most simple nosql engine&quot; is equivalent of asking for the &quot;most simple instrument which is not a guitar&quot;. No single definitive answer exists.&lt;\/p&gt;\n\n&lt;p&gt;First, you will need to ask yourself &lt;em&gt;why&lt;\/em&gt; a relational database is not optimal for the problem you are trying to solve. Then, use that information to decide amongst the many different kind of alternative (NoSQL) database systems available:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Document store&lt;\/li&gt;\n&lt;li&gt;Graph databases&lt;\/li&gt;\n&lt;li&gt;Key\/value stores&lt;\/li&gt;\n&lt;li&gt;Eventually consistent key\/value stores&lt;\/li&gt;\n&lt;li&gt;Object database&lt;\/li&gt;\n&lt;li&gt;etc.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;The &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/NoSQL&quot; rel=&quot;nofollow&quot;&gt;NoSQL article on Wikipedia&lt;\/a&gt; and &lt;a href=&quot;http:\/\/nosql-database.org\/&quot; rel=&quot;nofollow&quot;&gt;NoSQL-database.org&lt;\/a&gt; both seems to have comprehensive lists of popular NoSQL database implementations. If you are merely looking to investigate some of the different systems, I would suggest having a look at &lt;a href=&quot;http:\/\/hadoop.apache.org\/hbase\/&quot; rel=&quot;nofollow&quot;&gt;NHbase&lt;\/a&gt;, &lt;a href=&quot;http:\/\/cassandra.apache.org\/&quot; rel=&quot;nofollow&quot;&gt;Cassandra&lt;\/a&gt; and &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;neo4j&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2574667","ParentId":"2571098","CreationDate":"2010-04-04T14:17:40.020","OwnerUserId":"248994","Tags":[],"Body":"&lt;p&gt;Take a look at &lt;a href=&quot;http:\/\/www.db4o.com\/DownloadNow.aspx&quot; rel=&quot;nofollow&quot;&gt;Db4o&lt;\/a&gt;, an object database. I have used this briefly for a .NET project and it is easy to get started with. It is also available for Java too.&lt;\/p&gt;\n\n&lt;p&gt;This &lt;a href=&quot;http:\/\/www.buunguyen.net\/blog\/the-legend-of-data-persistence-part-1.html&quot; rel=&quot;nofollow&quot;&gt;blog post&lt;\/a&gt; does a nice job of explaining the motivation behind using an object-oriented DBMS rather than a RDBMS.&lt;\/p&gt;\n"},{"Id":"2574690","ParentId":"2574689","CreationDate":"2010-04-04T14:23:56.483","OwnerUserId":"18393","Tags":[],"Body":"&lt;p&gt;Your question could mean one of two things.&lt;\/p&gt;\n\n&lt;p&gt;If you mean a data structure for storing key-value pairs, use one of the &lt;a href=&quot;http:\/\/java.sun.com\/javase\/6\/docs\/api\/java\/util\/Map.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Map&lt;\/code&gt;&lt;\/a&gt; instances that are a standard part of the JDK.&lt;\/p&gt;\n\n&lt;p&gt;If however you are after an in-memory key-value store then I would suggest taking a look at &lt;a href=&quot;http:\/\/ehcache.org\/&quot; rel=&quot;nofollow&quot;&gt;EHCache&lt;\/a&gt; or even &lt;a href=&quot;http:\/\/memcached.org\/&quot; rel=&quot;nofollow&quot;&gt;memcached&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2574694","ParentId":"2574689","CreationDate":"2010-04-04T14:24:07.823","OwnerUserId":"87197","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/java.sun.com\/j2se\/1.4.2\/docs\/api\/java\/util\/HashMap.html&quot; rel=&quot;nofollow&quot;&gt;HashMap&lt;\/a&gt;?&lt;\/p&gt;\n"},{"Id":"2574841","ParentId":"2574689","CreationDate":"2010-04-04T15:07:26.777","OwnerUserId":"82609","Tags":[],"Body":"&lt;p&gt;There are lightweigh or embedded dbs like HSQLDB, Derby, SQLite\nBut like others don't understand why you need a db to store key\/values...&lt;\/p&gt;\n\n&lt;p&gt;Why not a Map? \nNeed to keep key\/values on app reboot? &lt;\/p&gt;\n\n&lt;p&gt;Also it's perhaps not what you need but with html5 on up to date browsers you have localStorage that permits you to store key\/values in the browser using javascript.&lt;\/p&gt;\n"},{"Id":"2576088","ParentId":"2576012","CreationDate":"2010-04-04T22:21:21.560","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;Sorted Strings Table (borrowed from google) is a file of key\/value string pairs, sorted by keys&lt;\/p&gt;\n"},{"Id":"2576857","ParentId":"2576838","CreationDate":"2010-04-05T03:59:21.303","OwnerUserId":"14955","Tags":[],"Body":"&lt;p&gt;I am looking for the exact same thing. Have not found it yet. An interesting project in this space is Bestpractical's &lt;a href=&quot;http:\/\/syncwith.us\/&quot; rel=&quot;nofollow&quot;&gt;Prophet&lt;\/a&gt; (and the bug tracker SD that is built on this). I have no idea how active it is, though, and I do not think it uses an encrypted file format (but in your scenario, whole-disk encryption would be a feasible solution).&lt;\/p&gt;\n\n&lt;p&gt;Prophet's buzzword-laden pitch reads something like this:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;A grounded, semirelational, peer to peer replicated, disconnected, versioned, property database with self-healing conflict resolution. &lt;\/p&gt;\n&lt;\/blockquote&gt;\n"},{"Id":"2576875","ParentId":"2576838","CreationDate":"2010-04-05T04:07:39.493","OwnerUserId":"289135","Tags":[],"Body":"&lt;p&gt;I have done this where a vertical-market customer wanted to use my ClipMate program to store and distribute medical-related data, and didn't want the data to escape &quot;into the wild&quot;.&lt;br&gt;\nI leveraged the encryption capability of the database (DBISAM by Elevatesoft, which has an engine that compiles directly into Delphi programs) and the &quot;hardware locking&quot; capability of my protection wrapper, Armadillo.&lt;br&gt;\nSo when someone purchases the program from the vertical market customer, they get the app (download or CD) and install it. Upon installation, they are presented with a registration dialog that shows their &quot;hardware fingerprint&quot; (as reported by Armadillo). This is derived from their ethernet address, CPU serial, hard drive serial, etc., and I could be wrong on any of those, but that's the general idea. They call it in, and a key is generated that's unique to that end-user\/machine combo.  The registration key &quot;unlocks&quot; the database, which is encrypted, and the encryption key is part of the registration code. \nThe end-user cannot make any use of the data without the key, and they cannot give away or transfer the data without receiving another unlock code.  It's pretty tight.  Would be too annoying for a low-priced app, but they're charging a lot for it, so it's worth it.  &lt;\/p&gt;\n\n&lt;p&gt;Anyway, to recap:  The database is encrypted, and the encryption code is only accessible (to the program that reads the database) when the program is properly registered to a specific user\/computer.  &lt;\/p&gt;\n\n&lt;p&gt;You COULD do this with Access, MySQL, etc.. Just encrypt the data, either at the database level (I suppose they support this, no?) or at the field level, if you want ultimate control and are ok with using a custom front-end app to read the thing.&lt;\/p&gt;\n"},{"Id":"2577012","ParentId":"2576838","CreationDate":"2010-04-05T05:06:42.910","OwnerUserId":"152253","Tags":[],"Body":"&lt;p&gt;Following is the one which I am looking for:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;From &lt;a href=&quot;http:\/\/couchdb.apache.org\/docs\/overview.html&quot; rel=&quot;nofollow&quot;&gt;official site&lt;\/a&gt;: CouchDB is a peer-based distributed\n  database system, it allows for users\n  and servers to access and update the\n  same shared data while disconnected\n  and then bi-directionally replicate\n  those changes later.&lt;\/p&gt;\n  \n  &lt;p&gt;The CouchDB document storage, view and\n  security models are designed to work\n  together to make true bi-directional\n  replication efficient and reliable.\n  Both documents and designs can\n  replicate, allowing full database\n  applications (including application\n  design, logic and data) to be\n  replicated to laptops for offline use,\n  or replicated to servers in remote\n  offices where slow or unreliable\n  connections make sharing data\n  difficult.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;Any idea how the couchDB data can be protected using user accounts? e.g. Mysql needs user to login to use the data. I am looking only for a simple authentication, not for something fool proof. I just do not want the data to be open for editing. Encrypting the file system seems to be an overkill.&lt;\/p&gt;\n"},{"Id":"2577023","ParentId":"2574689","CreationDate":"2010-04-05T05:13:08.883","OwnerUserId":"10973","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/jdbm.sourceforge.net\/&quot; rel=&quot;nofollow&quot;&gt;jdbm&lt;\/a&gt; works great for this sort of thing.  It's intended for storing on disk in a paged file, provides for basic transaction support (no guarantees on isolation, but ACD are covered).  We've used it in a production system with fairly wide deployment and have been quite pleased with the performance, stability, etc...&lt;\/p&gt;\n"},{"Id":"2577979","ParentId":"2577967","CreationDate":"2010-04-05T11:04:21.923","OwnerUserId":"44309","Tags":[],"Body":"&lt;p&gt;You probably do need a full relational DBMS, if not right now, very soon.  If you start now while your problems and data are simple and straightforward then when they become complex and difficult you will have plenty of experience with at least one DBMS to help you.  You probably don't need MySQL on all desktops, you might install it on a server for example and feed data out over your network, but you perhaps need to provide more information about your requirements, toolset and equipment to get better suggestions.&lt;\/p&gt;\n\n&lt;p&gt;And, while the other DBMSes have their strengths and weaknesses too, there's nothing wrong with MySQL for large and complex databases.  I don't know enough about SQLite to comment knowledgeably about it.&lt;\/p&gt;\n\n&lt;p&gt;EDIT: @Eric from your comments to my answer and the other answers I form even more strongly the view that it is time you moved to a database.  I'm not surprised that trying to do database operations on a 900MB Python dictionary is slow.  I think you have to first convince yourself, then your management, that you have reached the limits of what your current toolset can cope with, and that future developments are threatened unless you rethink matters.&lt;\/p&gt;\n\n&lt;p&gt;If your network really can't support a server-based database than (a) you really need to make your network robust, reliable and performant enough for such a purpose, but (b) if that is not an option, or not an early option, you should be thinking along the lines of a central database server passing out digests\/extracts\/reports to other users, rather than simultaneous, full RDBMS working in a client-server configuration.&lt;\/p&gt;\n\n&lt;p&gt;The problems you are currently experiencing are problems of not having the right tools for the job.  They are only going to get worse.  I wish I could suggest a magic way in which this is not the case, but I can't and I don't think anyone else will.&lt;\/p&gt;\n"},{"Id":"2577983","ParentId":"2577967","CreationDate":"2010-04-05T11:05:42.357","OwnerUserId":"54808","Tags":[],"Body":"&lt;p&gt;Here is a performance benchmark of different database suits -&gt;\n&lt;a href=&quot;http:\/\/www.sqlite.org\/cvstrac\/wiki?p=SpeedComparison&quot; rel=&quot;nofollow&quot;&gt;Database Speed Comparison&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I'm not sure how objective the above comparison is though, seeing as it's hosted on &lt;a href=&quot;http:\/\/www.sqlite.org&quot; rel=&quot;nofollow&quot;&gt;sqlite.org&lt;\/a&gt;. &lt;strike&gt;Sqlite only seems to be a bit slower when dropping tables, otherwise you shouldn't have any problems using it.&lt;\/strike&gt; Both sqlite and mysql seem to have their own strengths and weaknesses, in some tests the one is faster then the other, in other tests, the reverse is true.&lt;\/p&gt;\n\n&lt;p&gt;If you've been experiencing lower then expected performance, perhaps it is not sqlite that is the causing this, have you done any profiling or otherwise to make sure nothing else is causing your program to misbehave?&lt;\/p&gt;\n\n&lt;p&gt;EDIT: Updated with a link to a slightly more recent speed comparison.&lt;\/p&gt;\n"},{"Id":"2578080","ParentId":"2577967","CreationDate":"2010-04-05T11:24:54.557","OwnerUserId":"7055","Tags":[],"Body":"&lt;p&gt;Have you done any bench marking to confirm that it is the text files that are slowing you down? If you haven't, there's a good chance that tweaking some other part of the code will speed things up so that it's fast enough.&lt;\/p&gt;\n"},{"Id":"2578310","ParentId":"2577967","CreationDate":"2010-04-05T12:23:00.050","OwnerUserId":"205083","Tags":[],"Body":"&lt;p&gt;If you have that problem with a CSV file, maybe you can just pickle the dictionary and generate a pickle &quot;binary&quot; file with &lt;code&gt;pickle.HIGHEST_PROTOCOL&lt;\/code&gt; option. It can be faster to read and you get a smaller file. You can load the CSV file once and then generate the pickled file, allowing faster load in next accesses.&lt;\/p&gt;\n\n&lt;p&gt;Anyway, with 900 Mb of information, you're going to deal with some time loading it in memory. Another approach is not loading it on one step on memory, but load only the information when needed, maybe making different files by date, or any other category (company, type, etc..)&lt;\/p&gt;\n"},{"Id":"2578659","ParentId":"2577967","CreationDate":"2010-04-05T13:43:13.767","OwnerUserId":"247542","Tags":[],"Body":"&lt;p&gt;It sounds like each department has their own feudal database, and this implies a lot of unnecessary redundancy and inefficiency.&lt;\/p&gt;\n\n&lt;p&gt;Instead of transferring hundreds of megabytes to everyone across your network, why not keep your data in MySQL and have the departments upload &lt;em&gt;their&lt;\/em&gt; data to the database, where it can be normalized and accessible by everyone?&lt;\/p&gt;\n\n&lt;p&gt;As your organization grows, having completely different departmental databases that are unaware of each other, and contain potentially redundant or conflicting data, is going to become very painful.&lt;\/p&gt;\n"},{"Id":"2578751","ParentId":"2577967","CreationDate":"2010-04-05T14:03:38.757","OwnerUserId":"102022","Tags":[],"Body":"&lt;p&gt;Does the machine this process runs on have sufficient memory and bandwidth to handle this efficiently?  Putting MySQL on a slow machine and recoding the tool to use MySQL rather than text files could potentially be far more costly than simply adding memory or upgrading the machine.&lt;\/p&gt;\n"},{"Id":"2580277","ParentId":"2580244","CreationDate":"2010-04-05T18:38:26.627","OwnerUserId":"81106","Tags":[],"Body":"&lt;p&gt;you can check this thread it is similar what you asked&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/stackoverflow.com\/questions\/2534408\/nosql-vs-mysql-when-scalability-is-irrelevant\/&quot;&gt;http:\/\/stackoverflow.com\/questions\/2534408\/nosql-vs-mysql-when-scalability-is-irrelevant\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2581460","ParentId":"2577967","CreationDate":"2010-04-05T22:03:43.277","OwnerUserId":"309559","Tags":[],"Body":"&lt;p&gt;Take a look at mongodb.&lt;\/p&gt;\n"},{"Id":"2581768","ParentId":"2581738","CreationDate":"2010-04-05T23:29:15.673","OwnerUserId":"241462","Tags":[],"Body":"&lt;p&gt;I don't think security would be any different on a NoSQL database than on a relational database. In the end, security is an orthogonal question to how data is actually stored. Besides, it's not like you'd allow access to the database from anything but your business-layer servers from a networking standpoint.&lt;\/p&gt;\n\n&lt;p&gt;As for backups, most NoSQL databases that I know of allow for hot backups, just like a regular database does.&lt;\/p&gt;\n\n&lt;p&gt;The real question, IMO, is whether you can live with the restrictions that a NoSQL database puts on you - in particular, the general lack of ad-hoc queries. For example, if you ever wanted to know all of the people who ever bought product &quot;X&quot; then you'd have to build into your data access layer a counter for that from day one (or run a &lt;em&gt;very expensive&lt;\/em&gt; serial lookup of every past transaction). In a regular SQL database, you can just add an index and do a query and you're done (or even, don't add an index if it's a one-off). Or maybe you want to find out all the people who bought product &quot;Y&quot; before the latest version came out (so you can send them a reminder to upgrade or whatever): again, you have to plan that ahead with a NoSQL database, but it's trivial with a relational database.&lt;\/p&gt;\n\n&lt;p&gt;I think it makes sense when you can plan your schema and your usage pattern ahead of time, and where the occasional re-scan of records to add some new field or metric is acceptable. But for an e-commerce website, I think ad-hoc queries are just too valuable a feature to lose. Of course, that's just my opinion, and there's certainly no reason why you couldn't mix-n-match parts of the application between the two databases. I'd &lt;em&gt;personally&lt;\/em&gt; choose a relational database with memcached in between for added performance, though...&lt;\/p&gt;\n"},{"Id":"2581792","ParentId":"2581738","CreationDate":"2010-04-05T23:38:08.327","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;Handling financial information is one of the areas where SQL really is the right tool for the job. Most of the NOSQL systems were designed to improve scalability by accepting a higher risk of data loss or inconsistency. They also tend to have limited abilities to run reports over all records, since on a typical large website you only need enough data in the index to find and display a single record - the rest can be completely inaccessible until you know the record you are looking for.&lt;\/p&gt;\n\n&lt;p&gt;When dealing with money, any data inconsistency is a big problem, and if you need more scalability than a single sql server can give you, you have enough money that you can afford the higher cost of scaling sql. Also, the ad-hoc reporting available from sql is something you'd miss if you don't use sql - pretty much any information you want about sales history is trivial to get from sql, but potentially requires complex custom code from an object based store.&lt;\/p&gt;\n"},{"Id":"2581801","ParentId":"2581738","CreationDate":"2010-04-05T23:41:36.037","OwnerUserId":"164299","Tags":[],"Body":"&lt;ol&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/www.royans.net\/arch\/scalability-links-for-march-20th-2010-lots-of-datastore-related-items\/&quot; rel=&quot;nofollow&quot;&gt;Amazon&lt;\/a&gt; S3 uses NoSQL implementation.&lt;\/li&gt;\n&lt;\/ol&gt;\n"},{"Id":"2581859","ParentId":"2581738","CreationDate":"2010-04-05T23:56:30.323","OwnerUserId":"82769","Tags":[],"Body":"&lt;p&gt;The overhead that makes RDBMS's so slow, is guaranteeing atomicity, consistency, isolation, durability, also known as &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/ACID&quot; rel=&quot;nofollow&quot;&gt;ACID&lt;\/a&gt;.  Some of these properties are pretty critical for applications that deal with money.  You don't want to lose a single order when the lights go out.&lt;\/p&gt;\n\n&lt;p&gt;NoSQL databases usually sacrifice some or all of the ACID properties in return for severely reduced overhead.  For many applications, this is fine -- if a few &quot;diggs&quot; go missing when the lights go out, it's no big deal.&lt;\/p&gt;\n\n&lt;p&gt;For an ecommerce site, you need to ask yourself what you really need.  &lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you really need a level of performance that a RDBMS can't deliver?  &lt;\/li&gt;\n&lt;li&gt;Do you need the reliability that an DRMBS provides?&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Honestly, the answer to #2 is probably &quot;yes&quot;, which rules out most NoSQL solutions.  And unless you're dealing with traffic levels comparable to amazon.com's, an RDBMs, even on modest hardware will probably satisfy your performance needs just fine, especially if you limit yourself to simple queries, and index properly.  Which makes the answer to #1 &quot;no&quot;.&lt;\/p&gt;\n\n&lt;p&gt;You &lt;em&gt;could&lt;\/em&gt; however, consider using a RDBMS for transaction data, and a NoSQL database for non-critical data, like product pages, user reviews, etc.  But then you'd have twice as much  datastore software to install, and any relationships between the data in the two datastores  would have to be managed in code -- there'd be no JOINing your NoSQL database against your RDBMS.  This would likely result in an unnecessary level of complexity.&lt;\/p&gt;\n\n&lt;p&gt;In the end, if an RDBMS offers features you must have for reliability, and it performs acceptably for the sorts of load you'll be experiencing, an RDBMS is probably the best bet.&lt;\/p&gt;\n"},{"Id":"2581861","ParentId":"2580244","CreationDate":"2010-04-05T23:56:40.987","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;Basically it's about using the right tool for the job. Relational databases have been around for decades, which means they are very good at solving the problems that haven't changed in that time - things like keeping track of sales for example. Although they have become the default data store for just about everything, they are not so good at handling the problems that didn't exist twenty years ago - particularly scalability and data without a clearly defined, unchanging schema. &lt;\/p&gt;\n\n&lt;p&gt;NOSQL is a class of tools designed to solve the problems that are not perfectly suited to relational databases. Scalability is the best known, though unlikely to be a relevant to most developers. I think the other key use case that we don't see so much of yet is for small projects that don't need to worry about the data storage characteristics at all, and can just use the default - being able to skip database design, ORM and database maintenance is quite attractive. &lt;\/p&gt;\n\n&lt;p&gt;For Ecommerce specifically you're probably better off using sql at least in part - You might use NOSQL for product details or a recommendation engine, but you want your sales data in an easily queried sql table.&lt;\/p&gt;\n"},{"Id":"2582847","ParentId":"2577967","CreationDate":"2010-04-06T05:33:43.603","OwnerUserId":"11926","Tags":[],"Body":"&lt;h2&gt;Quick Summary&lt;\/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You need enough memory(RAM) to solve your problem efficiently. I think you should upgrade memory?? When reading the excellent &lt;a href=&quot;http:\/\/highscalability.com\/&quot; rel=&quot;nofollow&quot;&gt;High Scalability&lt;\/a&gt; Blog you will notice that for big sites to solve there problem efficiently they store the complete problem set in memory.&lt;\/li&gt;\n&lt;li&gt;You do need a central database solution. I don't think hand doing this with python dictionary's only will get the job done.&lt;\/li&gt;\n&lt;li&gt;How to solve &quot;your problem&quot; depends on your &quot;query's&quot;. What I would try to do first is put your data in elastic-search(see below) and query the database(see how it performs). I think this is the easiest way to tackle your problem. But as you can read below there are a lot of ways to tackle your problem.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;h2&gt;We know:&lt;\/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You used python as your program language.&lt;\/li&gt;\n&lt;li&gt;Your database is ~900MB (I think that's pretty large, but absolute manageable).&lt;\/li&gt;\n&lt;li&gt;You have loaded all the data in a python dictionary. Here I am assume the problem lays. Python tries to store the dictionary(also python dictionary's aren't the most memory friendly) in your memory, but you don't have enough memory(&lt;strong&gt;How much memory do you have????&lt;\/strong&gt;). When that happens you are going to have a lot of &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Virtual_memory&quot; rel=&quot;nofollow&quot;&gt;Virtual Memory&lt;\/a&gt;. When you attempt to read the dictionary you are constantly swapping data from you disc into memory. This swapping causes &quot;&lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Virtual_memory#Avoiding_thrashing&quot; rel=&quot;nofollow&quot;&gt;Trashing&lt;\/a&gt;&quot;. &lt;strong&gt;I am assuming that your computer does not have enough Ram. If true then I would first upgrade your memory with at least 2 Gigabytes extra RAM.&lt;\/strong&gt; When your problem set is able to fit in memory solving the problem is going to be a lot faster. I opened my computer architecture book where it(The memory hierarchy) says that main memory access time is about 40-80ns while disc memory access time is 5 ms. That is a BIG difference.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;h2&gt;Missing information&lt;\/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do you have a central server. You should use\/have a server.&lt;\/li&gt;\n&lt;li&gt;What kind of architecture does your server have? Linux\/Unix\/Windows\/Mac OSX? In my opinion your server should have linux\/Unix\/Mac OSX architecture.&lt;\/li&gt;\n&lt;li&gt;How much memory does your server have?&lt;\/li&gt;\n&lt;li&gt;Could you specify your data set(CSV) a little better.&lt;\/li&gt;\n&lt;li&gt;What kind of data mining are you doing? Do you need full-text-search capabilities? I am not assuming you are doing any complicated (SQL) query's. Performing that task with only python dictionary's will be a complicated problem. Could you formalize the query's that you would like to perform? For example:\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;&quot;get all users who work for departement x&quot;&lt;\/code&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;code&gt;&quot;get all sales from user x&quot;&lt;\/code&gt;&lt;\/li&gt;\n&lt;\/ul&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;h2&gt;Database needed&lt;\/h2&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;I am the computer person for\n  everything in a small company and I\n  have been started a new project where\n  I think it is about time to try new\n  databases.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;You are sure right that you need a database to solve your problem. Doing that yourself only using python dictionary's is difficult. Especially when your problem set can't fit in memory.&lt;\/p&gt;\n\n&lt;h2&gt;MySQL&lt;\/h2&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;I thought about using mysql, but then\n  I need installing mysql in every\n  desktop, sqlite is easier, but it is\n  very slow. I do not need a full\n  relational database, just some way of\n  play with big amounts of data in a\n  decent time.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;A centralized(Client-server architecture) database is exactly what you need to solve your problem. Let all the users access the database from 1 PC which you manage. &lt;strong&gt;You can use MySQL to solve your problem&lt;\/strong&gt;.&lt;\/p&gt;\n\n&lt;h2&gt;Tokyo Tyrant&lt;\/h2&gt;\n\n&lt;p&gt;You could also use &lt;a href=&quot;http:\/\/petewarden.typepad.com\/searchbrowser\/2009\/03\/tokyo-tyrant-tutorial.html&quot; rel=&quot;nofollow&quot;&gt;Tokyo Tyrant&lt;\/a&gt; to store all your data. Tokyo Tyrant is pretty fast and it does not have to be stored in RAM. It handles getting data a more efficient(instead of using python dictionary's). However if your problem can completely fit in Memory I think you should have look at Redis(below).&lt;\/p&gt;\n\n&lt;h2&gt;Redis:&lt;\/h2&gt;\n\n&lt;p&gt;You could for example use &lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/wiki\/QuickStart&quot; rel=&quot;nofollow&quot;&gt;Redis(quick start in 5 minutes)&lt;\/a&gt;(Redis is extremely fast) to store all sales in memory. Redis is extremely powerful and can do this kind of queries insanely fast. The only problem with Redis is that it has to fit completely in &lt;a href=&quot;http:\/\/antirez.com\/m\/p.php?i=203&quot; rel=&quot;nofollow&quot;&gt;RAM&lt;\/a&gt;, but I believe he is working on that(nightly build already supports it). Also like I already said previously solving your problem set completely from memory is how big sites solve there problem in a timely manner.&lt;\/p&gt;\n\n&lt;h2&gt;Document stores&lt;\/h2&gt;\n\n&lt;p&gt;This &lt;a href=&quot;http:\/\/bcbio.wordpress.com\/2009\/05\/10\/evaluating-key-value-and-document-stores-for-short-read-data\/&quot; rel=&quot;nofollow&quot;&gt;article&lt;\/a&gt; tries to evaluate kv-stores with document stores like couchdb\/riak\/mongodb. These stores are better capable of searching(a little slower then KV stores), but aren't good at full-text-search.&lt;\/p&gt;\n\n&lt;h2&gt;Full-text-search&lt;\/h2&gt;\n\n&lt;p&gt;If you want to do full-text-search queries you could like at: &lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/www.elasticsearch.com\/videos\/&quot; rel=&quot;nofollow&quot;&gt;elasticsearch(videos)&lt;\/a&gt;: When I saw the video demonstration of elasticsearch it looked pretty cool. You could try put(post simple json) your data in elasticsearch and see how fast it is. I am following elastissearch on github and the author is commiting a lot of new code to it.&lt;\/li&gt;\n&lt;li&gt;&lt;a href=&quot;http:\/\/lucene.apache.org\/solr\/tutorial.html&quot; rel=&quot;nofollow&quot;&gt;solr(tutorial)&lt;\/a&gt;: A lot of big companies are using solr(github, digg) to power there search. They got a big boost going from MySQL full-text search to solr.&lt;\/li&gt;\n&lt;\/ul&gt;\n"},{"Id":"2584578","ParentId":"2584567","CreationDate":"2010-04-06T11:57:54.120","OwnerUserId":"1583","Tags":[],"Body":"&lt;p&gt;You can use ESENT - an embeddable database storage engine for local storage. It comes as part of windows.&lt;\/p&gt;\n\n&lt;p&gt;Here is the managed interface &lt;a href=&quot;http:\/\/managedesent.codeplex.com\/&quot; rel=&quot;nofollow&quot;&gt;codeplex&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;As for using NoSQL databases - by all means, if they fit your needs use them. But don't do it simply because of their current popularity. If all you need to do is save documents, what is wrong with the filesystem? &lt;\/p&gt;\n"},{"Id":"2584676","ParentId":"2584567","CreationDate":"2010-04-06T12:15:01.930","OwnerUserId":"14955","Tags":[],"Body":"&lt;p&gt;There is also the &lt;a href=&quot;http:\/\/www.freedesktop.org\/wiki\/Specifications\/desktopcouch&quot; rel=&quot;nofollow&quot;&gt;Desktopcouch&lt;\/a&gt; project, which aims to provide a storage backend for (not necessarily Linux) desktop applications, for things like preferences and bookmarks. It is based on CouchDB and one of the stated goals is automatic replication and synchronization of data between computers. I believe it also plays a central role in the Ubuntu One cloud storage service.&lt;\/p&gt;\n"},{"Id":"2588981","ParentId":"2571098","CreationDate":"2010-04-06T23:16:59.890","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;I made a &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;Visual Guide to NoSQL Systems&lt;\/a&gt; to quickly see the major trade-offs involved in choosing one. The biggest choice is picking two of the following: consistency, availability, and partition tolerance.&lt;\/p&gt;\n"},{"Id":"2589004","ParentId":"2571098","CreationDate":"2010-04-06T23:20:59.647","OwnerUserId":"14316","Tags":[],"Body":"&lt;p&gt;berkeley db is pretty nice too&lt;\/p&gt;\n"},{"Id":"2589006","ParentId":"2574689","CreationDate":"2010-04-06T23:21:31.613","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;Consider using &lt;a href=&quot;http:\/\/code.google.com\/p\/jredis\/&quot; rel=&quot;nofollow&quot;&gt;jredis&lt;\/a&gt;. It's a Java client for Redis, a persistent key-value store. There's also a JDBC driver for it: code.google.com\/p\/jdbc-redis\/.&lt;\/p&gt;\n"},{"Id":"2589050","ParentId":"2568245","CreationDate":"2010-04-06T23:30:10.230","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;Here's a description of a few common data models:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Relational systems are the databases we've been using for a while now. RDBMSs and systems that support ACIDity and joins are considered relational.&lt;\/li&gt;\n&lt;li&gt;Key-value systems basically support get, put, and delete operations based on a primary key.&lt;\/li&gt;\n&lt;li&gt;Column-oriented systems still use tables but have no joins (joins must be handled within your application). Obviously, they store data by column as opposed to traditional row-oriented databases. This makes aggregations much easier.&lt;\/li&gt;\n&lt;li&gt;Document-oriented systems store structured &quot;documents&quot; such as JSON or XML but have no joins (joins must be handled within your application). It's very easy to map data from object-oriented software to these systems.&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;from &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;Visual Guide to NoSQL Systems&lt;\/a&gt;.&lt;\/p&gt;\n"},{"Id":"2589060","ParentId":"2559411","CreationDate":"2010-04-06T23:33:21.973","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;Here's a &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;Visual Guide to NoSQL Systems&lt;\/a&gt; that illustrates the primary trade-offs involved, the biggest being choosing two of the following: consistency, availability, and partition tolerance.&lt;\/p&gt;\n"},{"Id":"2593943","ParentId":"2573106","CreationDate":"2010-04-07T15:58:11.197","OwnerUserId":"48695","Tags":[],"Body":"&lt;p&gt;Cassandra by design is Key value database, so to achieve M:M there are two ways to do it.&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;1&gt; De-normalize your data so every\nrelation ship should duplicate data.&lt;\/p&gt;\n\n&lt;p&gt;ie. x-&gt;y(value) and x-&gt;z(value) and a-&gt;y(value)&lt;\/p&gt;\n\n&lt;p&gt;y should be saved for x and a&lt;\/p&gt;\n\n&lt;p&gt;This is how it should be done as it's give you strength of database &lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;2&gt;    Save reference for relational key as value.&lt;\/p&gt;\n\n&lt;p&gt;x-&gt;y(key) and x-&gt;z(Key) and a-&gt;y(Key)&lt;\/p&gt;\n\n&lt;p&gt;So if you need x with value of y it should be two operation, get\nx    which will give you value of y.\nthen    get y itself in separate    operation&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ul&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Cassandra is not RDBMS so don't wrap you mind around traditional way of doing it by dropping values and define relationship.&lt;\/p&gt;\n"},{"Id":"2605285","ParentId":"2604831","CreationDate":"2010-04-09T05:34:32.977","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;I think there is a store size setting in redis config that will stop you adding more data than can fit in the store you have set up. If you set up a store that doesn't fit in physical memory, it will simply ask the OS for more memory and some of it will be paged to disk, with obvious performance consequences.&lt;\/p&gt;\n\n&lt;p&gt;The next version of redis has its own virtual memory implementation which can store less frequently used keys on disk - &lt;a href=&quot;http:\/\/antirez.com\/post\/redis-virtual-memory-story.html&quot; rel=&quot;nofollow&quot;&gt;http:\/\/antirez.com\/post\/redis-virtual-memory-story.html&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2605897","ParentId":"2605862","CreationDate":"2010-04-09T08:07:36.200","OwnerUserId":"81179","Tags":[],"Body":"&lt;p&gt;NoSQL could be a fit when your data structure is quite simple (for example a simple key-value store) \/ predictable and you have no need for relational integrity or a need for ad-hoc and\/or advanced querying.&lt;\/p&gt;\n\n&lt;p&gt;What you win in easy scalability you might lose in flexibility and consistency though.&lt;\/p&gt;\n\n&lt;p&gt;The biggest problem would be to have an easy means for composing complex queries over your data. I would say meterological data is not the best candidate for NoSQL.&lt;\/p&gt;\n\n&lt;p&gt;I personally prefer PostgreSQL over MySQL and find it very scalable (even with millions or even billions of rows) when setup correctly.&lt;\/p&gt;\n"},{"Id":"2605912","ParentId":"2605862","CreationDate":"2010-04-09T08:12:07.373","OwnerUserId":"74305","Tags":[],"Body":"&lt;p&gt;I think you should try with a full-featured and mature DBMS, before giving up with SQL.&lt;\/p&gt;\n\n&lt;p&gt;See for instance:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.yafla.com\/dforbes\/Getting_Real_about_NoSQL_and_the_SQL_Performance_Lie\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.yafla.com\/dforbes\/Getting_Real_about_NoSQL_and_the_SQL_Performance_Lie\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.yafla.com\/dforbes\/The_Impact_of_SSDs_on_Database_Performance_and_the_Performance_Paradox_of_Data_Explodification\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.yafla.com\/dforbes\/The_Impact_of_SSDs_on_Database_Performance_and_the_Performance_Paradox_of_Data_Explodification\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2607947","ParentId":"2607923","CreationDate":"2010-04-09T13:47:46.960","OwnerUserId":"39430","Tags":[],"Body":"&lt;p&gt;One reason is that materialized views will perform poorly in an OLTP situation where there is a heavy amount of INSERTs vs. SELECTs.&lt;\/p&gt;\n\n&lt;p&gt;Everytime data is inserted the materialized views indexes must be updated, which not only slows down inserts but selects as well. The primary reason for using NoSQL is performance. By being basically a hash-key store, you get insanely fast reads\/writes, at the cost of  less control over constraints, which typically must be done at the application layer.&lt;\/p&gt;\n\n&lt;p&gt;So, while materialized views may help reads, they do nothing to speed up writes.&lt;\/p&gt;\n"},{"Id":"2608121","ParentId":"2608103","CreationDate":"2010-04-09T14:08:08.407","OwnerUserId":"30913","Tags":[],"Body":"&lt;p&gt;IMHO it is an &lt;strong&gt;axiom&lt;\/strong&gt; of NoSql that it is not ACID compliant.&lt;\/p&gt;\n\n&lt;p&gt;To elaborate: it would be possible to extend one to be ACID but in doing so you'd lose most of the reasons for using one in the first place. &lt;\/p&gt;\n\n&lt;p&gt;So like always it depends on your case.  If you're looking for ACID compliance, it would be hard to look away from the more traditional relational databases.  Of course, if you have the resources this could be implemented with a system that has a NoSql store for specifiic parts of the system.  That is, it might store the user settings or shopping cart before going to the relational database.&lt;\/p&gt;\n"},{"Id":"2608157","ParentId":"2608103","CreationDate":"2010-04-09T14:12:52.063","OwnerUserId":"16883","Tags":[],"Body":"&lt;p&gt;&quot;NoSQL&quot; is not a well-defined term. It's a very vague concept. As such, it's not even possible to say what is and what is not a &quot;NoSQL&quot; product. Not nearly all of the products typcially branded with the label are key-value stores.&lt;\/p&gt;\n"},{"Id":"2608223","ParentId":"2608103","CreationDate":"2010-04-09T14:22:40.370","OwnerUserId":"57477","Tags":[],"Body":"&lt;p&gt;Well, according to the &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/NoSQL&quot; rel=&quot;nofollow&quot;&gt;Wikipedia article on NoSQL&lt;\/a&gt;:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;NoSQL is a movement promoting a\n  loosely defined class of\n  non-relational data stores that break\n  with a long history of relational\n  databases and ACID guarantees.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;and also:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;The name was an attempt to describe\n  the emergence of a growing number of\n  non-relational, distributed data\n  stores that often did not attempt to\n  provide ACID  guarantees.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;and&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;NoSQL systems often provide weak\n  consistency guarantees such as\n  eventual consistency and transactions\n  restricted to single data items, even\n  though one can impose full ACID\n  guarantees by adding a supplementary\n  middleware layer.&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;So, in a nutshell, I'd say that one of the main benefits of a &quot;NoSQL&quot; data store is it's distinct &lt;em&gt;lack&lt;\/em&gt; of &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/ACID&quot; rel=&quot;nofollow&quot;&gt;ACID&lt;\/a&gt; properties.  Furthermore, IMHO, the more one tries to implement and enforce &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/ACID&quot; rel=&quot;nofollow&quot;&gt;ACID&lt;\/a&gt; properties, the further away from the &quot;spirit&quot; of a &quot;NoSQL&quot; data store you get, and the closer to a &quot;true&quot; &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/RDBMS&quot; rel=&quot;nofollow&quot;&gt;RDBMS&lt;\/a&gt; you get (relatively speaking, of course).&lt;\/p&gt;\n\n&lt;p&gt;However, all that said, &quot;NoSQL&quot; is a very vague term and is open to individual interpretations, and depends heavily upon just how much of a purist viewpoint you have.  For example, most modern-day RDBMS systems don't actually adhere to &lt;em&gt;all&lt;\/em&gt; of &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Edgar_F._Codd&quot; rel=&quot;nofollow&quot;&gt;Edgar F. Codd&lt;\/a&gt;'s &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Codd%27s_12_rules&quot; rel=&quot;nofollow&quot;&gt;12 rules&lt;\/a&gt; of his &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Relational_model&quot; rel=&quot;nofollow&quot;&gt;relation model&lt;\/a&gt;!&lt;\/p&gt;\n\n&lt;p&gt;Taking a pragmatic approach, it would appear that Apache's &lt;a href=&quot;http:\/\/couchdb.apache.org\/docs\/intro.html&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;\/a&gt; comes closest to embodying both ACID-compliance whilst retaining loosely-coupled, non-relational &quot;NoSQL&quot; mentality.&lt;\/p&gt;\n"},{"Id":"2608318","ParentId":"2608103","CreationDate":"2010-04-09T14:39:28.677","OwnerUserId":"252207","Tags":[],"Body":"&lt;p&gt;take a look at the CAP theorem&lt;\/p&gt;\n"},{"Id":"2608981","ParentId":"2573106","CreationDate":"2010-04-09T16:07:19.413","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Instead of using a join table the way you would with an rdbms, you would have one ColumnFamily containing a row for each X and a list of Ys associated with it, then a CF containing a row for each Y and a list of each X associated with it.&lt;\/p&gt;\n\n&lt;p&gt;If it turns out you don't really care about querying one of those directions then only keep the CF that you do care about.&lt;\/p&gt;\n"},{"Id":"2609462","ParentId":"2605862","CreationDate":"2010-04-09T17:16:18.480","OwnerUserId":"204218","Tags":[],"Body":"&lt;p&gt;I find it hard to create a coherent answer right now, but here goes.&lt;\/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Your data would fit without problem in a &quot;nosql&quot; datastore such as Cassandra (and many more probably)&lt;\/li&gt;\n&lt;li&gt;You would benefit from the schema-less design of many &quot;nosql&quot; solutions (seeing as not all columns (to use a MySQL term) are present all the time)&lt;\/li&gt;\n&lt;li&gt;The time based queries would be no problem in Cassandra (check out TimeUUID based keys)&lt;\/li&gt;\n&lt;li&gt;You don't seem to be taking advantage of the relational part of MySQL, so you wouldn't be hurt that much when losing it&lt;\/li&gt;\n&lt;li&gt;Although you might be just fine with MySQL, since you're really not describing the kind of problems, are you really having any? (Just being interested is totally cool)&lt;\/li&gt;\n&lt;li&gt;Things like indexes and search are things you would have to implement manually in many nosql datastore, if this scares you perhaps stick with sql.&lt;\/li&gt;\n&lt;\/ol&gt;\n\n&lt;p&gt;Thanks for listening ;)&lt;\/p&gt;\n"},{"Id":"2612878","ParentId":"2608103","CreationDate":"2010-04-10T09:26:47.080","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;If you are looking for an ACID compliant key\/value store, there's &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Berkeley_DB&quot; rel=&quot;nofollow&quot;&gt;Berkeley DB&lt;\/a&gt;. Among &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Graph_database&quot; rel=&quot;nofollow&quot;&gt;graph databases&lt;\/a&gt; at least &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Neo4j&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt; and &lt;a href=&quot;http:\/\/www.kobrix.com\/hgdb.jsp&quot; rel=&quot;nofollow&quot;&gt;HyperGraphDB&lt;\/a&gt; offer ACID transactions (HyperGraphDB actually uses Berkeley DB for low-level storage at the moment).&lt;\/p&gt;\n"},{"Id":"2613092","ParentId":"2607923","CreationDate":"2010-04-10T10:47:05.590","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;NoSQL is not about getting better performance out of your SQL database. It is about considering options other than the default SQL storage when there is no particular reason for the data to be in SQL at all.&lt;\/p&gt;\n\n&lt;p&gt;If you have an established SQL Database with a well designed schema and your only new requirement is improved performance, adding indexes and views is definitely the right approach. &lt;\/p&gt;\n\n&lt;p&gt;If you need to save a user profile object that you know will only ever need to be accessed by its key, SQL may not be the best option -  you gain nothing from a system with all sorts of query functionality you won't use, but being able to leave out the ORM layer while improving the performance of the queries you will be using is quite valuable.&lt;\/p&gt;\n"},{"Id":"2613918","ParentId":"2576012","CreationDate":"2010-04-10T15:48:00.207","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/MemtableSSTable&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/cassandra\/MemtableSSTable&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2614227","ParentId":"2614195","CreationDate":"2010-04-10T17:23:24.727","OwnerUserId":"28589","Tags":[],"Body":"&lt;p&gt;&lt;a href=&quot;http:\/\/svn.apache.org\/repos\/asf\/cassandra\/trunk\/src\/java\/org\/apache\/cassandra\/db\/marshal\/TimeUUIDType.java&quot; rel=&quot;nofollow&quot;&gt;TimeUUID&lt;\/a&gt; is one of six concrete implementations of the abstract class &lt;a href=&quot;http:\/\/svn.apache.org\/repos\/asf\/cassandra\/trunk\/src\/java\/org\/apache\/cassandra\/db\/marshal\/AbstractType.java&quot; rel=&quot;nofollow&quot;&gt;AbstractType&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;For ColumnFamilies you have the possiblity to specify an attribute called CompareWith. (SuperColumns have a similar CompareSubcolumnsWith attribute).  &lt;\/p&gt;\n\n&lt;p&gt;Valid values for this attribute are classes that implements the abstract class AbstractType (eg. TimeUUID). The CompareWith attribute tells Cassandra how to sort the columns for slicing operations.&lt;\/p&gt;\n\n&lt;p&gt;If you are using Java and using cassandra with TimeUUID I would recommend to read &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/FAQ#working_with_timeuuid_in_java&quot; rel=&quot;nofollow&quot;&gt;this section of the cassandra FAQ&lt;\/a&gt;.  &lt;\/p&gt;\n"},{"Id":"2614233","ParentId":"2614195","CreationDate":"2010-04-10T17:27:24.033","OwnerUserId":"99234","Tags":[],"Body":"&lt;p&gt;to indicate a unique &quot;row&quot; in a ColumnFamily&lt;\/p&gt;\n"},{"Id":"2614236","ParentId":"2614195","CreationDate":"2010-04-10T17:28:20.677","OwnerUserId":"138041","Tags":[],"Body":"&lt;p&gt;TimeUUID is a random global unique identifier. 16 bytes.&lt;\/p&gt;\n\n&lt;p&gt;Sample hex presentation: a4a70900-24e1-11df-8924-001ff3591711&lt;\/p&gt;\n\n&lt;p&gt;See &lt;a href=&quot;http:\/\/en.wikipedia.org\/wiki\/Universally_Unique_Identifier&quot; rel=&quot;nofollow&quot;&gt;http:\/\/en.wikipedia.org\/wiki\/Universally_Unique_Identifier&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;It may serve as a primary key in terms of relational database or when you need to store a list of values under some key.&lt;\/p&gt;\n\n&lt;p&gt;For example check this open source twitter example based on cassandra:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/twissandra.com\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/twissandra.com\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/ericflo\/twissandra&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ericflo\/twissandra&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;User = {\n    'a4a70900-24e1-11df-8924-001ff3591711': {\n        'id': 'a4a70900-24e1-11df-8924-001ff3591711',\n        'username': 'ericflo',\n        'password': '****',\n    },\n}\n\nUsername = {\n    'ericflo': {\n        'id': 'a4a70900-24e1-11df-8924-001ff3591711',\n    },\n}\n\nFriends = {\n    'a4a70900-24e1-11df-8924-001ff3591711': {\n        # friend id: timestamp of when the friendship was added\n        '10cf667c-24e2-11df-8924-001ff3591711': '1267413962580791',\n        '343d5db2-24e2-11df-8924-001ff3591711': '1267413990076949',\n        '3f22b5f6-24e2-11df-8924-001ff3591711': '1267414008133277',\n    },\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;Here user is assigned a unique key a4a70900-24e1-11df-8924-001ff3591711 which is used to refer to the user from other places.&lt;\/p&gt;\n"},{"Id":"2616418","ParentId":"2613357","CreationDate":"2010-04-11T08:14:26.580","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;If you have no reason to choose RDF + SPARQL (which both products support), Neo4j provides a clean Java API for manipulating a property graph (nodes + relationships + properties on both). For web applications, I wrote up an &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/IMDB_Example&quot; rel=&quot;nofollow&quot;&gt;example&lt;\/a&gt; using Spring Framework, which also exists in a simplified version as a &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/IMDB_Workshop&quot; rel=&quot;nofollow&quot;&gt;workshop&lt;\/a&gt;.&lt;\/p&gt;\n\n&lt;p&gt;Disclaimer: Obviously I'm on the Neo4j team, and I don't have any in-depth knowledge regarding AllegroGraph.&lt;\/p&gt;\n"},{"Id":"2619851","ParentId":"2619744","CreationDate":"2010-04-12T04:17:19.427","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;Being able to query properties directly is one of the features you lose when moving away from SQL, so you need a way to maintain your own index to let you find records. &lt;\/p&gt;\n\n&lt;p&gt;If your datastore does not have built in indexing or atomic list operations, you will need to deal with the locking issues you mention. However, indexing doesn't necessarily need to be synchronous - maintain a queue of updated records to be reindexed and you have a solution for 3 that can be reused to solve 2 also.&lt;\/p&gt;\n\n&lt;p&gt;If the index list for a particular value becomes too large for the system to handle in a single list, you can replace the list of users with a list of lists. However, if you have that many records with the same value it probably isn't a particularly useful search criteria anyway.&lt;\/p&gt;\n\n&lt;p&gt;Another option that is useful in some cases is to use a seperate system for the indexing - for example you could set up lucene to index the records in your main datastore.&lt;\/p&gt;\n"},{"Id":"2621849","ParentId":"2621795","CreationDate":"2010-04-12T12:03:21.930","OwnerUserId":"297484","Tags":[],"Body":"&lt;p&gt;the fuss around nosql is down to indexing, availability, and scalability. indexing is what allows the document-oriented stores to NOT open all documents if you want to get the ones where have = 1. availablity and scalability allow these systems to easily scale out and be robust in the face of unreliable hardware.&lt;\/p&gt;\n\n&lt;p&gt;erlang is designed for multi-processor systems and so is an ideal fit for distributed systems too.&lt;\/p&gt;\n"},{"Id":"2626293","ParentId":"2604745","CreationDate":"2010-04-13T00:32:40.170","OwnerUserId":"206625","Tags":[],"Body":"&lt;p&gt;Haven't followed Rob's stuff too much but just thinking out loud here. Couldn't you have a Profile provider object that the Content object could get at and that would have some way to fetch the instance of the Profile you are looking for.&lt;\/p&gt;\n\n&lt;p&gt;That would favor the composition you are looking for over the parent \/ child relationship. &lt;\/p&gt;\n\n&lt;p&gt;Again, thinking out loud here, but I would make the content object have a dependency of type IProfileProvider and I would inject that provider into the content object when needed. That would allow me to compose the Content type with the Profile type, while not explicitly having the parent \/ child relationship &lt;\/p&gt;\n"},{"Id":"2632589","ParentId":"2604745","CreationDate":"2010-04-13T19:31:29.543","OwnerUserId":"28567","Tags":[],"Body":"&lt;p&gt;I've decided that denormalizing the profile to be a &quot;smaller&quot; profile that only contains the immutable profile properties under the content would be a better solution. This minimizes the reads I'll be making while at the same time allows me to look up the actual profile object if necessary to gather deeper data on the profile.&lt;\/p&gt;\n"},{"Id":"2637815","ParentId":"2634955","CreationDate":"2010-04-14T13:42:05.013","OwnerUserId":"307430","Tags":[],"Body":"&lt;p&gt;less data!\neasy architecture!&lt;\/p&gt;\n"},{"Id":"2638093","ParentId":"2634955","CreationDate":"2010-04-14T14:17:52.067","OwnerUserId":"312262","Tags":[],"Body":"&lt;p&gt;My understanding is that you would use NoSQL when you just have a single key-value pair. Meaning, your RDMS table would just be 2 columns (key, value).&lt;\/p&gt;\n"},{"Id":"2641039","ParentId":"2640516","CreationDate":"2010-04-14T21:05:45.307","OwnerUserId":"620","Tags":[],"Body":"&lt;p&gt;Mongo uses stored Javascript in a few places including Map\/Reduce, db.eval and where clauses. Checkout this blog post for a survey: &lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/dirolf.com\/2010\/04\/05\/stored-javascript-in-mongodb-and-pymongo.html&quot; rel=&quot;nofollow&quot;&gt;Working With Stored JavaScript in MongoDB&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;The key to storing your functions on the server and making them available in these three contexts is db.system.js.save:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;db.system.js.save( { _id : &quot;foo&quot; , value : function( x , y ){ return x + y; } } );\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;More details &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Server-side+Code+Execution#Server-sideCodeExecution-Storingfunctionsserverside&quot; rel=&quot;nofollow&quot;&gt;in the Mongo docs&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2641158","ParentId":"2488783","CreationDate":"2010-04-14T21:24:26.203","OwnerUserId":"74011","Tags":[],"Body":"&lt;p&gt;hmm, it looks more like a Perl binding bug when handling exception to me.&lt;\/p&gt;\n\n&lt;p&gt;I believe that 0.6 fixes it for you because the interface has indeed changed, so 0.6 is not raising a thrift exception anymore, but the bug in thrift remains. I've opened a JIRA case, we'll see that thrift team says about it:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;https:\/\/issues.apache.org\/jira\/browse\/THRIFT-758&quot; rel=&quot;nofollow&quot;&gt;https:\/\/issues.apache.org\/jira\/browse\/THRIFT-758&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2641482","ParentId":"2634955","CreationDate":"2010-04-14T22:22:11.203","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;The general idea of NoSQL is that you should use whichever data store is the best fit for your application. If you have a table of financial data, use SQL. If you have objects that would require complex\/slow queries to map to a relational schema, use an object or key\/value store. &lt;\/p&gt;\n\n&lt;p&gt;Of course just about any real world problem you run into is somewhere in between those two extremes and neither solution will be perfect. You need to consider the capabilities of each store and the consequences of using one over the other, which will be very much specific to the problem you are trying to solve.&lt;\/p&gt;\n"},{"Id":"2642911","ParentId":"2642710","CreationDate":"2010-04-15T05:21:26.727","OwnerUserId":"113839","Tags":[],"Body":"&lt;p&gt;One possibility is to have a object and property table (which you may have already). \nThe  create the association table containing\nID  ObjectId    PropertyId   Value Date_Added\/version_number (as per your choice if you want to use timestamp or sequence column)&lt;\/p&gt;\n\n&lt;p&gt;As per your problem you would always add to association table and never update it. &lt;\/p&gt;\n\n&lt;p&gt;When you want to get a snapshot of latest object properties, you need to do a DISTINCT query for properties ordered by date_added\/version number.\nFor a given property checking history is straightforward as well.&lt;\/p&gt;\n\n&lt;p&gt;I hope this helps&lt;\/p&gt;\n"},{"Id":"2653705","ParentId":"2653074","CreationDate":"2010-04-16T14:32:54.500","OwnerUserId":"83406","Tags":[],"Body":"&lt;p&gt;Don't duplicate the properties if they'll always be the same between the &lt;code&gt;SearchResult&lt;\/code&gt; and a &lt;code&gt;Search&lt;\/code&gt;.  If a &lt;code&gt;SearchResult&lt;\/code&gt; should have a reference to a &lt;code&gt;Search&lt;\/code&gt;, keep a &lt;a href=&quot;http:\/\/code.google.com\/appengine\/docs\/python\/datastore\/typesandpropertyclasses.html#ReferenceProperty&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;ReferenceProperty&lt;\/code&gt;&lt;\/a&gt; pointing to the Search.  This basically stores the related &lt;code&gt;Search&lt;\/code&gt;'s &lt;code&gt;Key&lt;\/code&gt; in the model.&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class SearchResult(db.Model):\n    search = db.ReferenceProperty(Search, required=True)\n    # other stuff...\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;I also highly recommend you watch some of the &lt;a href=&quot;http:\/\/code.google.com\/events\/io\/2009\/sessions.html#appengine&quot; rel=&quot;nofollow&quot;&gt;App Engine videos from last year's Google I\/O&lt;\/a&gt; (and from &lt;a href=&quot;http:\/\/sites.google.com\/site\/io\/&quot; rel=&quot;nofollow&quot;&gt;2008&lt;\/a&gt;), in particular &lt;a href=&quot;http:\/\/code.google.com\/events\/io\/2009\/sessions\/BuildingScalableComplexApps.html&quot; rel=&quot;nofollow&quot;&gt;this one&lt;\/a&gt; by Brett Slatkin, and \n&lt;a href=&quot;http:\/\/www.youtube.com\/watch?v=tx5gdoNpcZM&quot; rel=&quot;nofollow&quot;&gt;this one&lt;\/a&gt; by Ryan Barrett. They're all pretty helpful videos if you have the time, but I found those two in particular to be really great.&lt;\/p&gt;\n"},{"Id":"2657215","ParentId":"2552985","CreationDate":"2010-04-17T04:21:32.167","OwnerUserId":"48695","Tags":[],"Body":"&lt;p&gt;To achieve what you described you need to have column name as time stamp and use get slice function using start time and endtime, it will give you all rows with column name with in that range. also use column name sort so you would get result in ordered by time.&lt;\/p&gt;\n"},{"Id":"2663923","ParentId":"2654423","CreationDate":"2010-04-18T20:55:22.767","OwnerUserId":"169895","Tags":[],"Body":"&lt;p&gt;First of all you have to understand the properties of each system. i can offer you to read this &lt;a href=&quot;http:\/\/www.vineetgupta.com\/2010\/01\/nosql-databases-part-1-landscape.html&quot; rel=&quot;nofollow&quot;&gt;post&lt;\/a&gt;. it's the first step to understand NOSQL or Not Only SQL.Secondly you can check  this &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;blog post&lt;\/a&gt; to understand all these stuff visually.&lt;\/p&gt;\n\n&lt;p&gt;Finally glance at open source projects such as Mongodb, Couchdb etc. to see the list you can go &lt;a href=&quot;http:\/\/java.dzone.com\/articles\/open-source-nosql-databases&quot; rel=&quot;nofollow&quot;&gt;here&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2675904","ParentId":"2675488","CreationDate":"2010-04-20T14:26:01.080","OwnerUserId":"284645","Tags":[],"Body":"&lt;p&gt;To retrieve the document(s) with &lt;code&gt;doc_id&lt;\/code&gt;=10, you need to create a view with &lt;code&gt;doc_id&lt;\/code&gt; as a key. Afair, you cannot enforce uniqueness of the &lt;code&gt;doc_id&lt;\/code&gt;. &lt;\/p&gt;\n\n&lt;p&gt;Instead of using your &lt;code&gt;doc_id&lt;\/code&gt;, you could still use CouchDB's &lt;code&gt;_id&lt;\/code&gt; field. Iirc, you do not have to leave it to CouchDB to assign a value to &lt;code&gt;_id&lt;\/code&gt;. If you do not like the UUIDs CouchDB uses for the &lt;code&gt;_id&lt;\/code&gt; field, you can create a document with an &lt;code&gt;_id&lt;\/code&gt; you specify. &lt;\/p&gt;\n\n&lt;p&gt;You need to be careful with that, esp. in a distributed setup. If you end up with different documents (on different nodes) having the same &lt;code&gt;_id&lt;\/code&gt;, CouchDB might consider them to be different versions of the same document.&lt;\/p&gt;\n"},{"Id":"2677850","ParentId":"2634955","CreationDate":"2010-04-20T19:01:38.050","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;When evaluating distributed data systems, you have to consider the CAP theorem - you can pick two of the following: consistency, availability, and partition tolerance.&lt;\/p&gt;\n\n&lt;p&gt;Cassandra is an available, partition-tolerant system that supports eventual consistency. For more information see my &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;Visual Guide to NoSQL Systems&lt;\/a&gt;. &lt;\/p&gt;\n"},{"Id":"2677881","ParentId":"2611362","CreationDate":"2010-04-20T19:05:51.970","OwnerUserId":"310465","Tags":[],"Body":"&lt;p&gt;If you're trying to quickly get a grasp on the major trade-offs involved in NoSQL systems, check out my &lt;a href=&quot;http:\/\/blog.nahurst.com\/visual-guide-to-nosql-systems&quot; rel=&quot;nofollow&quot;&gt;Visual Guide to NoSQL Systems&lt;\/a&gt;. Essentially, you must choose two of the following properties for you system to excel at: consistency, availability, or partition tolerance.&lt;\/p&gt;\n"},{"Id":"2684038","ParentId":"2682205","CreationDate":"2010-04-21T14:55:09.600","OwnerUserId":"130168","Tags":[],"Body":"&lt;p&gt;Denormalize.  See twissandra.com and the documentation at &lt;a href=&quot;http:\/\/github.com\/ericflo\/twissandra&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ericflo\/twissandra&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;More examples at &lt;a href=&quot;http:\/\/wiki.apache.org\/cassandra\/ArticlesAndPresentations&quot; rel=&quot;nofollow&quot;&gt;http:\/\/wiki.apache.org\/cassandra\/ArticlesAndPresentations&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2685181","ParentId":"2682205","CreationDate":"2010-04-21T17:33:34.660","OwnerUserId":"223992","Tags":[],"Body":"&lt;p&gt;Are you really competing with Google and Amazon in terms of traffic volumes? I'd recommend starting by looking at upgrading your current MySQL infrastructure - how many database servers do you currently run in your cluster(s)? Do you partition data?&lt;\/p&gt;\n\n&lt;p&gt;C.&lt;\/p&gt;\n"},{"Id":"2688350","ParentId":"2688305","CreationDate":"2010-04-22T05:04:20.697","OwnerUserId":"315670","Tags":[],"Body":"&lt;p&gt;in fact you are.. because searching in a single huge field for text will take much more time than indexing the database and searching the proper sql way. The database was built to be used with sql and indexes, it does not have the capability to parse and index json, so whatever way you will find to search in the json (probably just hacky string matching) will be much slower. 500k rows is not that much to handle for mysql , you don't really need hadoop, just a good normalized schema , the right indices and optimized queries   &lt;\/p&gt;\n"},{"Id":"2694073","ParentId":"2694044","CreationDate":"2010-04-22T20:03:28.620","OwnerUserId":"1220","Tags":[],"Body":"&lt;p&gt;All you really need is a serializer\/deserializer to make this work.&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/github.com\/atheken\/NoRM&quot; rel=&quot;nofollow&quot;&gt;Norm&lt;\/a&gt; has done a great job of doing just that. Makes it easier to take straight poco objects and just save them to mongo with a single line of code.&lt;\/p&gt;\n\n&lt;p&gt;They call Norm an ORM, but its really just a poco to dictionary mongo wrapper.&lt;\/p&gt;\n\n&lt;p&gt;An orm is just extra ceremony for these operations. If your data operations are abstracted into a repository, its going to be a non-issue either way, because converting to another backing store is an object per object, basis.&lt;\/p&gt;\n"},{"Id":"2694112","ParentId":"2694044","CreationDate":"2010-04-22T20:09:57.657","OwnerUserId":"38360","Tags":[],"Body":"&lt;p&gt;Well, yes, Object-&lt;strong&gt;Relational&lt;\/strong&gt; mappers are redundant with MongoDB because MongoDB isn't a &lt;strong&gt;relational&lt;\/strong&gt; database, it's a Document-Oriented database.&lt;\/p&gt;\n\n&lt;p&gt;So instead of SQL, you write queries in JSON.  Unless you really, &lt;em&gt;really&lt;\/em&gt; want to write raw JSON, as opposed to, say, Linq, then you're still going to want to use a mapper.  And if you don't want to create coupling against MongoDB itself, then you don't want to pass actual &lt;code&gt;Document&lt;\/code&gt; objects around, you want to map them to real POCOs.&lt;\/p&gt;\n\n&lt;p&gt;The mapping is much &lt;em&gt;easier&lt;\/em&gt; with a document-oriented DB like MongoDB, because you have nested documents instead of relations, but that doesn't mean it goes away completely.  It just means you've substituted one type of &quot;impedance mismatch&quot; for a different, slightly-less-dramatic mismatch.&lt;\/p&gt;\n"},{"Id":"2694961","ParentId":"2688305","CreationDate":"2010-04-22T22:25:19.227","OwnerUserId":"256376","Tags":[],"Body":"&lt;p&gt;Few pointers to consider:&lt;\/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Hadoop (HDFS specifically) distributes data around a cluster of machines. Using MapReduce to analyze\/process this data requires that the data is stored on the HDFS to make use of the parallel processing power Hadoop offers. &lt;\/p&gt;&lt;\/li&gt;\n&lt;li&gt;&lt;p&gt;Hadoop\/MapReduce is no where near real-time. Even when running on small amounts of data the time Hadoop takes to set-up a Job can be 30+ seconds. This is something that can't be stopped.&lt;\/p&gt;&lt;\/li&gt;\n&lt;\/ul&gt;\n\n&lt;p&gt;Maybe something to look into would be using Lucene to index your JSON objects as documents. You could store the index in solr and easily query on anything you want.&lt;\/p&gt;\n"},{"Id":"2699879","ParentId":"2694044","CreationDate":"2010-04-23T15:37:26.730","OwnerUserId":"295964","Tags":[],"Body":"&lt;p&gt;I think an &quot;ORM&quot; on MongoDb can be useful, not only for &quot;serializing&quot; and &quot;deserializing&quot; objects into the db (Norm seems to do a great job) but also for making it more easy to execute aggregation queries. &lt;\/p&gt;\n\n&lt;p&gt;It is nice if an &quot;ORM&quot; can generate MapReduce jobs for grouping and detecting duplicates. Some people have written code to automatically convert an sql statement into a mapreduce job: &lt;a href=&quot;http:\/\/rickosborne.org\/blog\/index.php\/2010\/02\/19\/yes-virginia-thats-automated-sql-to-mongodb-mapreduce\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/rickosborne.org\/blog\/index.php\/2010\/02\/19\/yes-virginia-thats-automated-sql-to-mongodb-mapreduce\/&lt;\/a&gt;  &lt;\/p&gt;\n"},{"Id":"2700318","ParentId":"2699932","CreationDate":"2010-04-23T16:39:43.247","OwnerUserId":"6844","Tags":[],"Body":"&lt;p&gt;I assume you are storing these contacts to form some kind of address-book style application. Going with this assumption, I would say your second example is exactly what you want to be doing. The way I look at it, each &quot;contact&quot; is a single document. All the attributes for this contact belong within the document.&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    name: &quot;John Smith&quot;,\n    number: &quot;+44 1234 567890&quot;\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;To take this a bit further, in the future you might decide you wish to store multiple numbers per person, perhaps of different types. I would embed these all inside the document for the particular contact:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    name: &quot;John Smith&quot;,\n    numbers: [\n        { number: &quot;+44 1234 567890&quot;, type: &quot;home&quot; },\n        { number: &quot;+44 7798 987654&quot;, type: &quot;mobile&quot; },\n        { number: &quot;+44 1234 987123&quot;, type: &quot;work&quot; }\n    ]\n}\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;I find a good way to approach designing a model for use in a document database is to consider what items you will wish to use independently. For those which make sense on their own, they should probably go inside their own document. For those which only make sense when viewed in the context of their &quot;container&quot; object, embed them within it.&lt;\/p&gt;\n\n&lt;p&gt;I hope this helps you.&lt;\/p&gt;\n"},{"Id":"2703959","ParentId":"2703828","CreationDate":"2010-04-24T09:36:50.623","OwnerUserId":"69083","Tags":[],"Body":"&lt;p&gt;I suppose that you actually want the items that are scheduled, but not reviewed after that scheduling?&lt;\/p&gt;\n\n&lt;p&gt;Shouldn't the reviews be connected to the scheduled items instead of difrectly to the items? Now you have to compare the dates to see which reviews comes after one scheduled item but before the next. Also, if an item is scheduled twice with a short time between, you may end up with both reviews belonging to the second scheduling.&lt;\/p&gt;\n\n&lt;p&gt;With this change you could easily pick out the unreviewed schedulings:&lt;\/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select i.id, i.name, s.execute_at\nfrom items i\ninner join scheduled_items s on s.item_id = i.id\nleft join reviewed_items r on r.scheduled_items_id = s.id\nwhere r.id is null\n&lt;\/code&gt;&lt;\/pre&gt;\n\n&lt;p&gt;As to your question:&lt;\/p&gt;\n\n&lt;blockquote&gt;\n  &lt;p&gt;I suppose that adding index to the\n  datetime fields doesn't make any sense\n  because the cardinality or uniqueness\n  on that fields are very high and index\n  won't give any(?) speed-up. Is it\n  correct?&lt;\/p&gt;\n&lt;\/blockquote&gt;\n\n&lt;p&gt;No, that is not correct. An index can be useful if the cardinality is high. An index is created by default for the unique id of a table, which of course has the highest cardinality possible.&lt;\/p&gt;\n"},{"Id":"2705842","ParentId":"2634955","CreationDate":"2010-04-24T19:30:22.863","OwnerUserId":"55150","Tags":[],"Body":"&lt;p&gt;Cassandra is the answer to a particular problem: What do you do when you have so much data that it does not fit on one server ? How do you store all your data on many servers and do not break your bank account and not make your developers insane ? Facebook gets 4 Terabyte of new compressed data EVERY DAY. And this number most likely will grow more than twice within a year.&lt;\/p&gt;\n\n&lt;p&gt;If you do not have this much data or if you have millions to pay for Enterprise Oracle\/DB2 cluster installation and specialists required to set it up and maintain it, then you are fine with SQL database.&lt;\/p&gt;\n"},{"Id":"2707570","ParentId":"2707558","CreationDate":"2010-04-25T08:07:58.417","OwnerUserId":"119280","Tags":[],"Body":"&lt;p&gt;Your approach of separating them is good.&lt;\/p&gt;\n\n&lt;p&gt;Your concern about code duplication is 100% valid.&lt;\/p&gt;\n\n&lt;p&gt;The solution is fairly straightfowrard - abstract away common functionality between the tests - e.g. &quot;RunTest&quot;, &quot;AnalyzeResult&quot;, &quot;ConnectToDB&quot; - into a common library (you did not specify which language but I assume it has a concept of a library) which can be passed configration details such as which database to connect to. &lt;\/p&gt;\n\n&lt;p&gt;Then use that library independently from the unit test driver and integrity test driver - which, if you are skilled\/lucky enough, might have very little code of its own other than configuration (e.g. which database to connect to, how to report results, and which tests to run).&lt;\/p&gt;\n\n&lt;p&gt;Similarly, if needed, common inputs\/datasets can be placed in common directory&lt;\/p&gt;\n"},{"Id":"2707582","ParentId":"2707558","CreationDate":"2010-04-25T08:12:48.023","OwnerUserId":"11361","Tags":[],"Body":"&lt;p&gt;You should separate the database related tests from the &quot;pure&quot; unit tests.&lt;br&gt;\nThe cost of having two different assemblies is very low considering the benefits - you have one suite of fast, no environment set required tests that you can run on any machine and a slower suite that tests the database integrity that can run only on specific places (e.g. build server).  &lt;\/p&gt;\n\n&lt;p&gt;Another benefit is that you can have two build processes (quick and nightly) that runs different tests suites.&lt;\/p&gt;\n\n&lt;p&gt;To avoid duplicating code you can create another assembly with the common methods\/actions that both test suites needs. Don't worry too much about duplicatimng the actual tests because you're testing different things (either logic or database) so sooner or later your tests will become quite different depending on what you're trying to test.&lt;\/p&gt;\n"},{"Id":"2712721","ParentId":"2609582","CreationDate":"2010-04-26T10:35:26.650","OwnerUserId":"255292","Tags":[],"Body":"&lt;p&gt;Very excited about Pintura too, but I'm struggling getting this setup right now. Personally one of my requirements for something that's production ready would be good documentation. It's a bit confused right now, unsure of how much of the old Persevere docs are applicable, and I had to do a lot of manual changes before I could get the \/example that came with Pintura to run. &lt;\/p&gt;\n\n&lt;p&gt;In conclusions: looks like Alpha software to me!&lt;\/p&gt;\n"},{"Id":"2714655","ParentId":"2707558","CreationDate":"2010-04-26T15:26:19.583","OwnerUserId":"47623","Tags":[],"Body":"&lt;p&gt;One more answer. You have two types of tests. What I would like to do is address the integrity tests. What you may want to do is include the &lt;strong&gt;integrity tests&lt;\/strong&gt; as a function of the &lt;strong&gt;production code&lt;\/strong&gt;. IOW, have the integrity as part of the system.&lt;\/p&gt;\n\n&lt;p&gt;You already mentioned that duplication is an issue and that you are refactoring to remove the duplication. The refactored code of course has development tests?&lt;\/p&gt;\n\n&lt;p&gt;System Monitoring can be production code. So what ever code you write becomes part of the system.&lt;\/p&gt;\n\n&lt;p&gt;The nice thing about this is that you evolve your code through your development tests.&lt;\/p&gt;\n"},{"Id":"2721292","ParentId":"2720490","CreationDate":"2010-04-27T12:54:03.913","OwnerUserId":"232760","Tags":[],"Body":"&lt;p&gt;consider also Redis DB.&lt;\/p&gt;\n\n&lt;p&gt;project page: &lt;a href=&quot;http:\/\/code.google.com\/p\/redis\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/code.google.com\/p\/redis\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;and clojure client library for it: &lt;a href=&quot;http:\/\/github.com\/ragnard\/redis-clojure\/&quot; rel=&quot;nofollow&quot;&gt;http:\/\/github.com\/ragnard\/redis-clojure\/&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2721623","ParentId":"2720490","CreationDate":"2010-04-27T13:34:28.720","OwnerUserId":"36710","Tags":[],"Body":"&lt;p&gt;I think the &lt;a href=&quot;http:\/\/wiki.github.com\/tinkerpop\/gremlin\/defining-a-property-graph&quot; rel=&quot;nofollow&quot;&gt;property graph data model&lt;\/a&gt; of &lt;a href=&quot;http:\/\/neo4j.org\/&quot; rel=&quot;nofollow&quot;&gt;Neo4j&lt;\/a&gt; is a really nice fit to Clojure, see &lt;a href=&quot;http:\/\/wiki.neo4j.org\/content\/Clojure&quot; rel=&quot;nofollow&quot;&gt;this wiki page&lt;\/a&gt; for links to more information. Regarding free, Neo4j is released under the &lt;a href=&quot;http:\/\/www.gnu.org\/licenses\/agpl.html&quot; rel=&quot;nofollow&quot;&gt;AGPL3&lt;\/a&gt; license, which means it's free to use in open source projects using a compatible license. The commercial backing company &lt;a href=&quot;http:\/\/neotechnology.com\/product-overview&quot; rel=&quot;nofollow&quot;&gt;Neo Technology&lt;\/a&gt; can provide commercial licenses.&lt;\/p&gt;\n"},{"Id":"2723620","ParentId":"2720490","CreationDate":"2010-04-27T17:49:21.060","OwnerUserId":"295964","Tags":[],"Body":"&lt;p&gt;MongoDB compared with CouchDB: &lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Comparing+Mongo+DB+and+Couch+DB&quot; rel=&quot;nofollow&quot;&gt;http:\/\/www.mongodb.org\/display\/DOCS\/Comparing+Mongo+DB+and+Couch+DB&lt;\/a&gt;&lt;\/p&gt;\n"},{"Id":"2726809","ParentId":"2634955","CreationDate":"2010-04-28T04:31:41.807","OwnerUserId":"327256","Tags":[],"Body":"&lt;p&gt;another situation that makes the choice easier is when you want to use aggregate function like sum, min, max, etcetera and complex queries (like in the financial system mentioned above) then a relational database is probably more convenient then a nosql database since both are not possible on a nosql databse unless you use really a lot of Inverted indexes. When you do use nosql you would have to do the aggregate functions in code or store them seperatly in its own columnfamily but this makes it all quite complex and reduces the performance that you gained by using nosql.&lt;\/p&gt;\n"},{"Id":"2731714","ParentId":"2684462","CreationDate":"2010-04-28T17:22:07.213","OwnerUserId":"18255","Tags":[],"Body":"&lt;p&gt;Data Warehouses have very little in common with NoSQL - the main similarity is that any two data warehouses can have very different philosopohies or conventions just like any two NoSQL systems can be nearly unrelated.&lt;\/p&gt;\n\n&lt;p&gt;The only concept they share is that they are both used to analyze large amounts of data.&lt;\/p&gt;\n\n&lt;p&gt;NoSQL solutions usually manage relatively limited schemas with large cardinality in few entities, while data warehouses typically have lots of facts and dimensions (in a dimensional model) or lots of entities in a 3NF model.  DW systems usually manage multiple lines of business and attempt to combine that data.&lt;\/p&gt;\n\n&lt;p&gt;DW systems typically have reporting abilities in SQL which allows you to access all the data in a standard way.  NoSQL systems are typically more code-based - for instance Map\/Reduce.&lt;\/p&gt;\n"},{"Id":"2731727","ParentId":"2688305","CreationDate":"2010-04-28T17:24:06.507","OwnerUserId":"99665","Tags":[],"Body":"&lt;p&gt;Sounds like you are trying to recreate CouchDB.  CouchDB is built with a map-reduce framework and is made to work specifically with JSON objects.&lt;\/p&gt;\n"},{"Id":"2733955","ParentId":"2729981","CreationDate":"2010-04-29T00:03:53.730","OwnerUserId":"101970","Tags":[],"Body":"&lt;p&gt;At one level document and key\/value are quite similar - both will return an object when you request a key. In pure key\/value that object will be a simple string, although it can be a serialized complex object. A document database extends this with functions to work with this object such as partial update functionality or search indexing.&lt;\/p&gt;\n\n&lt;p&gt;Beyond that you will need to think about your specific requirements - NOSQL covers a lot of different systems, and unlike SQL databases they all have quite different advantages\/disadvantages for a specific scenario.&lt;\/p&gt;\n"},{"Id":"2741364","ParentId":"2581738","CreationDate":"2010-04-29T23:18:25.097","OwnerUserId":"244520","Tags":[],"Body":"&lt;p&gt;You guys should check this out:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=&quot;http:\/\/www.mongodb.org\/display\/DOCS\/Replication#Replication-ReplicationAcknowledgementviagetlasterror&quot; rel=&quot;nofollow&quot;&gt;Replication Acknowledgement via getlasterror&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;MongoDB is on the verge of providing durable writes. I think that is the main issue with people discuss this topic w.r.t. money. The transactional part is less important due to the nested document features.&lt;\/p&gt;\n"},{"Id":"2743418","ParentId":"2720490","CreationDate":"2010-04-30T09:07:04.487","OwnerUserId":"222467","Tags":[],"Body":"&lt;p&gt;We are using Clojure + MongoDB, and they are works very well together.&lt;\/p&gt;\n"}]}
